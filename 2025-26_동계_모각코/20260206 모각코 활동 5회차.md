# MAE (Masked Autoencoders Are Scalable Vision Learners)

[https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)

자기지도학습 - 이미지의 일부를 가리고 복원하는 방식으로 표현을 학습하는 방법

---

## 초록 분석

_"This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision."_ _이 논문은 마스크 오토인코더(MAE)가 컴퓨터 비전에서 확장 가능한 자기지도학습 모델임을 보입니다._ `#MAE #maskedautoencoder #scalable` 핵심 키워드는 **"scalable"** — SimCLR이 "simple"을 강조했다면, MAE는 "크게 키워도 잘 된다"는 확장성을 강조함

_"Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels."_ _우리의 접근법은 단순합니다: 입력 이미지의 랜덤 패치를 마스킹하고 빠진 픽셀을 복원합니다._ NLP의 BERT와 유사한 발상 — BERT는 단어를 마스킹하고 예측, MAE는 이미지 패치를 마스킹하고 복원. 개념적으로는 단순하지만 구현에서 핵심적인 설계 선택들이 있음

_"First, we develop an asymmetric encoder-decoder architecture... Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task."_ 두 가지 핵심 설계:

- **비대칭 인코더-디코더**: 인코더는 보이는 패치만 처리 (마스크 토큰 없음), 디코더는 가볍게
- **높은 마스킹 비율(75%)**: 이미지 중복성이 높기 때문에, 쉬운 task가 되지 않으려면 많이 가려야 함

---

## 핵심 개념 분석

### 왜 단순히 BERT를 이미지에 적용하면 안 될까?

논문이 이 질문에 직접 답하는 게 인상적임. 세 가지 이유를 제시:

**아키텍처 차이** — 기존엔 CNN이 주류라 마스크 토큰 같은 개념을 삽입하기 어려웠음. ViT 등장으로 해결됨

**정보 밀도 차이** ← 이게 핵심!

- 언어: 단어 하나하나가 semantic하게 밀도 높음. 몇 개만 가려도 어려운 task
- 이미지: 공간적 중복성이 매우 높음. 옆 픽셀에서 쉽게 추측 가능
- → 해결책: **아주 많이 가리기 (75%)**

**디코더의 역할 차이**

- BERT 디코더: 의미 있는 단어를 예측 → 간단한 MLP로도 충분
- MAE 디코더: 픽셀값을 복원 → 낮은 semantic 수준, 디코더 설계가 중요해짐

---

### 아키텍처: 비대칭 인코더-디코더

**인코더 (무거움)**

- 보이는 패치(25%)만 처리 → 마스크 토큰 없음
- 표준 ViT 구조 사용
- 포지셔널 임베딩 추가

**디코더 (가벼움)**

- 인코더 출력 + 마스크 토큰을 모두 받아서 처리
- 인코더보다 훨씬 작음 (FLOPs 기준 ~9% 수준)
- **Pre-training 때만 사용** → fine-tuning 시엔 버림

왜 이 설계가 효율적인가? 인코더가 전체 패치의 25%만 처리하니 연산량이 대폭 줄어듦. 논문 실험 기준 **3× 이상 학습 속도 향상**

---

### Reconstruction Target: 픽셀을 예측한다

손실 함수는 **MSE (Mean Squared Error)** — 마스킹된 패치에 대해서만 계산 (BERT와 동일한 방식)

흥미로운 변형: **패치별 정규화(normalized pixels)**

- 각 패치의 픽셀값을 해당 패치의 평균/표준편차로 정규화해서 예측 target으로 사용
- 로컬 대비(local contrast)를 강화 → 실험 결과 성능 향상

토큰 기반 방법(BEiT 등)과 비교했을 때 픽셀 기반도 충분히 좋거나 더 나음 → 복잡한 tokenizer 없이도 잘 된다는 게 MAE의 강점

---

## 주요 실험 결과

**마스킹 비율의 영향** (Figure 5)

- fine-tuning: 40~80% 범위에서 전반적으로 잘 됨, 75%가 sweet spot
- linear probing: 비율이 높을수록 꾸준히 향상. 75%가 최적
- BERT의 15%와 대조적으로 **이미지에선 왜 75%가 필요한지** 직접 실험으로 보여줌

**주요 성능** (Table 3)

- ViT-H로 ImageNet-1K에서 **87.8% top-1** 달성 — IN1K 데이터만 사용한 방법 중 최고
- supervised pre-training보다 transfer learning 성능도 좋음

**학습 효율**

- BEiT보다 **3.5× 빠름** (인코더에서 마스크 토큰 제거 덕분)
- MoCo v3보다 적은 epoch에서 더 높은 성능

---

## SimCLR과 비교해서 흥미로운 점
data augmentation 의존도 차이가 흥미로움. SimCLR은 crop+color distortion 조합이 없으면 성능이 크게 떨어지는데, MAE는 random masking 자체가 augmentation 역할을 하기 때문에 크게 의존하지 않음.

---

## 한 줄 요약
MAE = "이미지의 75%를 가리고 복원하는 task를 통해, 라벨 없이도 지도학습을 능가하는 표현을 학습한다. 핵심은 비대칭 구조로 빠르게, 높은 마스킹 비율로 어렵게."
코드 구현 시 rotation 기반과의 가장 큰 차이는 **task의 성격** — rotation은 분류(4-class), MAE는 픽셀 복원(regression+MSE)이라는 점, 그리고 **아키텍처도 ViT 기반**이어야 패치 단위 마스킹이 자연스럽다는 점이 구현에서 핵심 고려사항이 될 것 같음!
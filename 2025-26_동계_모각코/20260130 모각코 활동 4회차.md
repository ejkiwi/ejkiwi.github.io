- `SimCLRDataset`: 동일 이미지에 augmentation 두 번 적용해 `(view1, view2)` 반환
- `SimCLRModel`: 기존 ResNet18 backbone + Projection Head (MLP)
- `NTXentLoss` + `SimCLRTrainer` 추가
- (클로드가잔뜩도와줘서어찌저찌햇다다)

## 1. `src/dataset.py` — `SimCLRDataset` 추가

SimCLR은 동일 이미지에 서로 다른 Augmentation을 두 번 적용해 Positive Pair를 만든다.  
`__getitem__`이 `(view1, view2)` 쌍을 반환하도록 해봤다.

```python
# src/dataset.py 에 추가
def get_simclr_transform(img_size: int = 32, s: float = 0.5) -> transforms.Compose:
    color_jitter = transforms.ColorJitter(
        brightness=0.8 * s, contrast=0.8 * s,
        saturation=0.8 * s, hue=0.2 * s
    )
    return transforms.Compose([
        transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomApply([color_jitter], p=0.8),
        transforms.RandomGrayscale(p=0.2),
        transforms.RandomApply(
            [transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.5
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])


class SimCLRDataset(CIFAR10):
    def __init__(
        self,
        root: str,
        train: bool = True,
        transform=None,     # get_simclr_transform() 결과를 주입
        download: bool = True,
    ):
        super().__init__(root, train=train, transform=None, download=download)
        self.aug = transform  # 두 번 각각 따로 적용

    def __getitem__(self, index):
        img = Image.fromarray(self.data[index])
        view1 = self.aug(img)
        view2 = self.aug(img)
        return view1, view2   # label 불필요 (self-supervised)

    def __len__(self):
        return len(self.data)
```

---

## 2. `src/model.py` — `SimCLRModel` 추가

기존 `Model`(ResNet18)의 backbone을 **인코더**로 재사용하고,  
그 위에 **Projection Head (MLP)** 를 추가한다.  
Downstream에서는 Projection Head를 제거하고 backbone만 사용할 것이다다.

```python
# src/model.py 에 추가

class SimCLRModel(nn.Module):
    def __init__(
        self,
        embed_dim: int = 512,
        projection_dim: int = 128,
        num_classes: int = 10,
    ):
        super().__init__()
        self.embed_dim = embed_dim

        backbone = models.resnet18()
        backbone.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        backbone.maxpool = nn.Identity()
        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  
        
        self.projection_head = nn.Sequential(  
            nn.Linear(embed_dim, embed_dim),
            nn.ReLU(inplace=True),
            nn.Linear(embed_dim, projection_dim),
        )
        self.classifier = nn.Linear(embed_dim, num_classes)
        self.use_projection = True   # Pretext: True / Downstream: False

    def forward(self, x):
        h = self.backbone(x).flatten(1)  
        if self.use_projection:
            return self.projection_head(h)  
            return self.classifier(h)     

    def freeze_backbone(self):
        for param in self.backbone.parameters():
            param.requires_grad = False

    @classmethod
    def from_checkpoint(cls, path, **kwargs):
        model = cls(**kwargs)
        model.load_state_dict(torch.load(path, map_location="cpu"))
        return model
```

---

## 3. `src/trainer.py` — `SimCLRTrainer` 추가

NT-Xent Loss(Normalized Temperature-scaled Cross Entropy)를 구현하고,  
Pretext 학습 전용 Trainer를 추가한다.

```python
# src/trainer.py 에 추가

import torch.nn.functional as F

class NTXentLoss(nn.Module):
    def __init__(self, temperature: float = 0.5):
        super().__init__()
        self.temperature = temperature

    def forward(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:
        B = z1.size(0)

        # L2 정규화
        z1 = F.normalize(z1, dim=1)
        z2 = F.normalize(z2, dim=1)

        # (2B, 2B) 유사도 행렬
        z  = torch.cat([z1, z2], dim=0)                        # (2B, dim)
        sim = torch.mm(z, z.t()) / self.temperature             # (2B, 2B)

        # 자기 자신(대각선) 마스킹
        mask = torch.eye(2 * B, dtype=torch.bool, device=z.device)
        sim.masked_fill_(mask, float("-inf"))

        # Positive index: i번째의 positive는 i+B 또는 i-B 근데 이게 맞는지 모르겠다
        labels = torch.cat([
            torch.arange(B, 2 * B),   # z1[i] → z2[i]
            torch.arange(0, B),        # z2[i] → z1[i]
        ]).to(z.device)

        loss = F.cross_entropy(sim, labels)
        return loss


class SimCLRTrainer:
    def __init__(
        self,
        model: SimCLRModel,
        device: torch.device = None,
        temperature: float = 0.5,
    ):
        self.model = model
        self.device = device or torch.device("cpu")
        self.model.to(self.device)
        self.criterion = NTXentLoss(temperature)

    def _train_one_epoch(self, loader, optimizer):
        self.model.train()
        self.model.use_projection = True
        total_loss = 0.0
        for view1, view2 in tqdm(loader, desc="SimCLR Train", leave=False):
            view1 = view1.to(self.device)
            view2 = view2.to(self.device)

            z1 = self.model(view1)
            z2 = self.model(view2)
            loss = self.criterion(z1, z2)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        return total_loss / len(loader)

    @torch.no_grad()
    def _evaluate(self, loader):
        self.model.eval()
        self.model.use_projection = True
        total_loss = 0.0
        for view1, view2 in loader:
            view1 = view1.to(self.device)
            view2 = view2.to(self.device)
            z1 = self.model(view1)
            z2 = self.model(view2)
            total_loss += self.criterion(z1, z2).item()
        return total_loss / len(loader)

    def train(
        self,
        train_loader,
        valid_loader,
        optimizer,
        epochs: int = 100,
        patience: int = 10,
        save_path: str = "./checkpoints/simclr/pretext",
        scheduler=None,
    ):
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)

        best_valid_loss = float("inf")
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            train_loss = self._train_one_epoch(train_loader, optimizer)
            valid_loss = self._evaluate(valid_loader)

            if scheduler:
                scheduler.step()

            print(f"[SimCLR] Epoch {epoch:03d} | Train Loss: {train_loss:.6f} | Valid Loss: {valid_loss:.6f}")

            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                patience_counter = 0
                ckpt_path = save_dir / f"epoch_{epoch:03d}_loss_{valid_loss:.6f}.pt"
                torch.save(self.model.state_dict(), ckpt_path)
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"[SimCLR] Early stopping at epoch {epoch}.")
                    break
```

---

## 4. `configs/config.yaml` — SimCLR 블록 추가

```yaml
simclr:
  temperature: 0.5
  projection_dim: 128
  embed_dim: 512
  pretext:
    lr: 3e-4
    weight_decay: 1e-4
    epochs: 100
    patience: 10
    save_path: "./checkpoints/simclr/pretext"
    scheduler:
      enabled: true
      type: "CosineAnnealingLR"
      T_max: 100
  downstream:
    lr: 1e-4
    weight_decay: 1e-4
    epochs: 50
    patience: 7
    freeze_backbone: true          
    save_path: "./checkpoints/simclr/downstream"
    scheduler:
      enabled: false
```

---

## 5. `experiments/train.py` — SimCLR 실험 흐름 추가

```python
# experiments/train.py 에 추가 (기존 supervised / rotation 흐름 아래에 이어서 작성)

from src.dataset import SimCLRDataset, get_simclr_transform
from src.model import SimCLRModel
from src.trainer import SimCLRTrainer, Trainer

simclr_cfg = config["simclr"]

simclr_aug = get_simclr_transform(
    img_size=config["data"]["img_size"][0],
    s=0.5
)
simclr_train_data = SimCLRDataset(
    root=config["data"]["root"],
    train=True,
    transform=simclr_aug,
)
simclr_valid_data = SimCLRDataset(
    root=config["data"]["root"],
    train=False,
    transform=simclr_aug,
)

simclr_train_loader = DataLoader(
    simclr_train_data, batch_size=config["batch_size"]["unlabeled"], shuffle=True
)
simclr_valid_loader = DataLoader(
    simclr_valid_data, batch_size=config["batch_size"]["valid"]
)

simclr_model = SimCLRModel(
    embed_dim=simclr_cfg["embed_dim"],
    projection_dim=simclr_cfg["projection_dim"],
)
simclr_trainer = SimCLRTrainer(
    simclr_model, device, temperature=simclr_cfg["temperature"]
)

simclr_optimizer = torch.optim.AdamW(
    simclr_model.parameters(),
    lr=simclr_cfg["pretext"]["lr"],
    weight_decay=simclr_cfg["pretext"]["weight_decay"],
)
simclr_scheduler = build_scheduler(simclr_optimizer, simclr_cfg["pretext"]["scheduler"]) \
    if simclr_cfg["pretext"]["scheduler"]["enabled"] else None

simclr_trainer.train(
    train_loader=simclr_train_loader,
    valid_loader=simclr_valid_loader,
    optimizer=simclr_optimizer,
    epochs=simclr_cfg["pretext"]["epochs"],
    patience=simclr_cfg["pretext"]["patience"],
    save_path=simclr_cfg["pretext"]["save_path"],
    scheduler=simclr_scheduler,
)

# 최적 체크포인트 로드
best_simclr_ckpt = min(
    Path(simclr_cfg["pretext"]["save_path"]).glob("*.pt"),
    key=lambda p: float(p.stem.split("_loss_")[-1])
)
simclr_model = SimCLRModel.from_checkpoint(
    best_simclr_ckpt,
    embed_dim=simclr_cfg["embed_dim"],
    projection_dim=simclr_cfg["projection_dim"],
)

# Linear Probe 설정 (backbone 고정, classifier만 학습)
if simclr_cfg["downstream"]["freeze_backbone"]:
    simclr_model.freeze_backbone()
simclr_model.use_projection = False   # classifier 모드로 전환

simclr_downstream_trainer = Trainer(simclr_model).to(device)

simclr_downstream_optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, simclr_model.parameters()),
    lr=simclr_cfg["downstream"]["lr"],
    weight_decay=simclr_cfg["downstream"]["weight_decay"],
)

# 기존 labeled_loader / valid_loader 재사용
simclr_downstream_trainer.train(
    train_loader=labeled_loader,
    valid_loader=valid_loader,
    optimizer=simclr_downstream_optimizer,
    epochs=simclr_cfg["downstream"]["epochs"],
    is_pretext=False,
    patience=simclr_cfg["downstream"]["patience"],
    save_path=simclr_cfg["downstream"]["save_path"],
)

print("--- SimCLR Model ---")
simclr_downstream_trainer.test(test_loader)
```

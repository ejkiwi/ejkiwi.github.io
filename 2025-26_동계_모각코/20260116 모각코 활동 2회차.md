# SSL Rotation Prediction — 코드 리팩토링
  
- **대상 원본**: `dAiv_ssl_rotation_resnet18.ipynb`
- **리팩토링 결과물**: `src/`, `configs/`, `experiments/`

---
## 1. 배경 및 목표

기존 코드는 Jupyter Notebook 단일 파일(`dAiv_ssl_rotation_resnet18.ipynb`)에 데이터 로딩, 모델 정의, 학습 루프, 실험 실행이 모두 섞여있었다. 이를 모듈화된 Python 패키지 구조로 분리해보았다!!

- 데이터 / 모델 / 학습 로직을 독립 모듈로 분리
- 모든 하이퍼파라미터를 `config.yaml` 단일 파일에서 관리
- 기존 학습 로직 보존: 체크포인트 저장, Early Stopping 동작 변경 없음

---

## 2. 구조 변경
### 2.1 리팩토링 전
```
SSL-paper-study/
└── dAiv_ssl_rotation_resnet18.ipynb   # 모든 코드가 단일 노트북에 존재
```
### 2.2 리팩토링 후
```
SSL-paper-study/
├── src/
│   ├── __init__.py          # 패키지 공개 API 정의
│   ├── dataset.py           # DataType, CIFAR10Dataset, get_transforms()
│   ├── model.py             # Model (ResNet18 기반)
│   └── trainer.py           # Trainer (scheduler 인자 추가)
├── configs/
│   └── config.yaml          # 전체 하이퍼파라미터 관리
├── experiments/
│   └── train.py             # 실험 실행 진입점
└── dAiv_ssl_rotation_resnet18.ipynb   # 원본 유지 (수정 없음)
```

---

## 3. 파일별 변경 내용
### 3.1 `src/dataset.py`
노트북의 `DataType` Enum과 `CIFAR10Dataset` 클래스를 그대로 옮기고, `get_transforms()` 함수를 새로 추가했다.

**이전된 내용 (변경 없음)**
- `DataType` Enum: `LABELED_TRAIN`, `UNLABELED_TRAIN`, `UNLABELED_VALID`, `VALID`, `TEST`
- `CIFAR10Dataset`: seed=42 고정 분할, rotation label 자동 생성, transform 외부 주입 구조

**신규 추가: `get_transforms()`**
```python
def get_transforms(
    img_size: Tuple[int, int],
    mean: List[float],
    std: List[float],
    augment: bool = False,
) -> transforms.Compose:
```

---
### 3.2 `src/model.py`

노트북의 `Model` 클래스를 변경 없이 그대로 이전했다.
- ResNet18 backbone (CIFAR-10 적합 수정: `conv1` 3×3, `maxpool → Identity`)
- `is_pretext` 플래그로 `rotation_classifier` / `classifier` 분기
- `Model.from_checkpoint()` 클래스 메서드로 가중치 로드

---
### 3.3 `src/trainer.py`
노트북의 `Trainer` 클래스를 이전하고 `train()` 메서드에 `scheduler` 인자를 추가했다.

**변경 사항**
```python
# 변경 전
def train(self, train_loader, valid_loader, optimizer, epochs, ...):

# 변경 후
def train(self, train_loader, valid_loader, optimizer, epochs, ...,
          scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None):
```

`scheduler`가 주어지면 매 epoch 종료 후 `scheduler.step()` 자동 호출:


```python
train_loss = self._train_one_epoch(...)
valid_loss, valid_acc = self._evaluate(...)

if scheduler is not None:
    scheduler.step()   # ← 추가
```


**보존된 내용 (변경 없음)**
- `_train_one_epoch()` / `_evaluate()` 학습·평가 로직
- patience 기반 Early Stopping

---
### 3.4 `src/__init__.py`
패키지 단독 임포트가 가능하도록 고쳤다.
```python
from .dataset import DataType, CIFAR10Dataset, get_transforms
from .model import Model
from .trainer import Trainer

__all__ = ["DataType", "CIFAR10Dataset", "get_transforms", "Model", "Trainer"]
```


---
### 3.5 `configs/config.yaml`
모든 하이퍼파라미터를 계층 구조로 관리한다. `experiments/train.py`에 하드코딩된 값은 없다.

```
config.yaml
├── data          : root, img_size, mean/std, split 비율
├── batch_size    : labeled / unlabeled / valid
├── device        : GPU 인덱스 (-1이면 CPU)
├── pretrained    : HuggingFace Hub repo_id / file_name
├── supervised    : lr, weight_decay, epochs, patience, save_top_k, save_path, scheduler
└── self_supervised
    ├── pretext   : (위와 동일 구조)
    └── downstream: (위와 동일 구조)
```

각 학습 단계의 `scheduler` 블록 구조:

```yaml
scheduler:
  enabled: false          # true로 변경하면 즉시 적용
  type: "CosineAnnealingLR"
  T_max: 50               # 생성자에 그대로 전달되는 파라미터
```


---

### 3.6 `experiments/train.py`
노트북의 실험 순서를 그대로 유지하며 전체 실험을 실행하는 진입점 스크립트다.

**실행 흐름**
```
1. config.yaml 로드
2. device 설정
3. Dataset / DataLoader 생성
   └─ labeled_loader만 augment=True, 나머지는 augment=False
4. Supervised 학습 (scheduler 포함)
5. SSL Pretext 학습 (HuggingFace Hub 사전학습 가중치 로드 → scheduler 포함)
6. SSL Downstream 학습 (Pretext 최적 체크포인트 로드 → scheduler 포함)
7. 두 모델 테스트 결과 출력
```

**핵심: 동적 Scheduler 생성**
```python
def build_scheduler(optimizer, sch_cfg):
    sch_cls = getattr(torch.optim.lr_scheduler, sch_cfg["type"])
    kwargs  = {k: v for k, v in sch_cfg.items() if k not in ("enabled", "type")}
    return sch_cls(optimizer, **kwargs)
```

`type` 필드만 교체하면 `StepLR`, `OneCycleLR` 등 어떤 scheduler로도 전환 가능하다.


**실행 방법**
```bash
# 프로젝트 루트에서 실행
python experiments/train.py
python experiments/train.py --config configs/config.yaml
```


---

## 변경되지 않은 것

- `dAiv_ssl_rotation_resnet18.ipynb` — 원본 노트북 무수정
- `CIFAR10Dataset`의 seed=42 데이터 분할 로직
- `Trainer`의 체크포인트 저장 / Early Stopping 동작
- `Model` 아키텍처 (backbone, classifier, rotation_classifier)
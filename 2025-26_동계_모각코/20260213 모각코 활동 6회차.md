- `MAEDataset`: 패치 마스킹 후 `(masked_img, original_img, mask)` 반환
- `MAEModel`: ViT 인코더(마스킹 안 된 패치만 처리) + 경량 디코더 구조
- `MAETrainer`: 마스킹된 패치에 대한 MSE Loss

## 1. `src/dataset.py` — `MAEDataset` 추가

Rotation과 달리 MAE는 이미지를 패치로 나눈 뒤 일부를 마스킹하고 원본 픽셀을 복원하는 것이 목표!!
`__getitem__`이 `(masked_img, original_img)` 쌍을 반환하도록 했당
```python
# src/dataset.py 에 추가

class MAEDataset(CIFAR10):
    def __init__(
        self,
        root: str,
        train: bool = True,
        mask_ratio: float = 0.75,
        patch_size: int = 4,           # CIFAR-10(32x32) 기준 → 8x8 = 64패치
        transform=None,
        download: bool = True,
    ):
        super().__init__(root, train=train, transform=None, download=download)
        self.mask_ratio = mask_ratio
        self.patch_size = patch_size
        self.base_transform = transform  # ToTensor + Normalize
        self.to_tensor = transforms.ToTensor()

    def _make_mask(self, num_patches: int) -> torch.BoolTensor:
        """True인 위치 = 마스킹(제거)할 패치임임"""
        num_mask = int(num_patches * self.mask_ratio)
        mask = torch.zeros(num_patches, dtype=torch.bool)
        mask[torch.randperm(num_patches)[:num_mask]] = True
        return mask  # shape: (num_patches,)

    def __getitem__(self, index):
        img = Image.fromarray(self.data[index])

        # 원본 텐서 (복원 target)
        original = self.base_transform(img)          # (C, H, W)

        # 마스킹된 텐서 생성
        C, H, W = original.shape
        P = self.patch_size
        num_patches = (H // P) * (W // P)

        mask = self._make_mask(num_patches)          # (num_patches,)

        # 패치 단위로 마스킹 (마스킹 위치를 0으로)
        masked = original.clone()
        idx = 0
        for row in range(H // P):
            for col in range(W // P):
                if mask[idx]:
                    masked[:, row*P:(row+1)*P, col*P:(col+1)*P] = 0.0
                idx += 1

        return masked, original, mask   # mask는 loss 계산에 사용

    def __len__(self):
        return len(self.data)
```

---
## 2. `src/model.py` — `MAEModel` 추가

기존 `Model`(ResNet18)은 Downstream에서 그대로 재사용한다.  
MAE Pretext 전용으로 **ViT 인코더 + 작은디코더** 구조를 추가했다.

```python
# src/model.py 에 추가

import math

class PatchEmbed(nn.Module):
    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):
        super().__init__()
        self.num_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        # x: (B, C, H, W) → (B, num_patches, embed_dim)
        return self.proj(x).flatten(2).transpose(1, 2)


class MAEModel(nn.Module):
    """
    - 인코더: 마스킹되지 않은 패치만 처리
    - 디코더: 전체 패치 받아 픽셀 복원
    """
    def __init__(
        self,
        img_size: int = 32,
        patch_size: int = 4,
        in_channels: int = 3,
        encoder_dim: int = 192,
        encoder_depth: int = 6,
        encoder_heads: int = 3,
        decoder_dim: int = 96,
        decoder_depth: int = 2,
        decoder_heads: int = 3,
    ):
        super().__init__()
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.encoder_dim = encoder_dim
        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, encoder_dim)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, encoder_dim))
        self.enc_pos_embed = nn.Parameter(
            torch.zeros(1, 1 + self.num_patches, encoder_dim)
        )
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=encoder_dim, nhead=encoder_heads,
            dim_feedforward=encoder_dim * 4, batch_first=True, norm_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=encoder_depth)
        self.enc_norm = nn.LayerNorm(encoder_dim)
        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim)
        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))
        self.dec_pos_embed = nn.Parameter(
            torch.zeros(1, 1 + self.num_patches, decoder_dim)
        )
        decoder_layer = nn.TransformerEncoderLayer(
            d_model=decoder_dim, nhead=decoder_heads,
            dim_feedforward=decoder_dim * 4, batch_first=True, norm_first=True
        )
        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=decoder_depth)
        self.dec_norm = nn.LayerNorm(decoder_dim)
        self.pred_head = nn.Linear(decoder_dim, patch_size ** 2 * in_channels)
        self._init_weights()

    def _init_weights(self):
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        nn.init.trunc_normal_(self.enc_pos_embed, std=0.02)
        nn.init.trunc_normal_(self.dec_pos_embed, std=0.02)
        nn.init.trunc_normal_(self.mask_token, std=0.02)

    def encode(self, x, mask):
        B = x.size(0)
        # cls token 추가
        cls = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls, x], dim=1)             # (B, 1+num_patches, dim)
        x = x + self.enc_pos_embed

        cls_x = x[:, :1, :]
        patch_x = x[:, 1:, :]                      # (B, num_patches, dim)
        visible = patch_x[~mask.unsqueeze(-1).expand_as(patch_x)].view(B, -1, self.encoder_dim)
        x_enc = torch.cat([cls_x, visible], dim=1)

        x_enc = self.encoder(x_enc)
        x_enc = self.enc_norm(x_enc)
        return x_enc

    def decode(self, x_enc, mask):
        B = x_enc.size(0)
        x_dec = self.enc_to_dec(x_enc)             # 차원 축소

        tokens = self.mask_token.expand(B, self.num_patches, -1).clone()
        num_visible = (~mask).sum(dim=1)[0].item()
        tokens[~mask] = x_dec[:, 1:1 + int(num_visible), :].reshape(-1, x_dec.size(-1))

        cls_dec = x_dec[:, :1, :]
        full = torch.cat([cls_dec, tokens], dim=1)
        full = full + self.dec_pos_embed
        full = self.decoder(full)
        full = self.dec_norm(full)

        return self.pred_head(full[:, 1:, :])       # (B, num_patches, patch_size^2*C)

    def forward(self, x, mask):
        patches = self.patch_embed(x)              # (B, num_patches, encoder_dim)
        x_enc   = self.encode(patches, mask)
        pred    = self.decode(x_enc, mask)
        return pred

    def get_encoder(self):
        return nn.Sequential(self.patch_embed, self.encoder, self.enc_norm)
```

---

## 3. `src/trainer.py` — `MAETrainer` 추가
MAE는 **마스킹 패치에 대한 MSE Loss**를 사용하므로 별도 Trainer를 추가했다...

```python
# src/trainer.py 에 추가
class MAETrainer:
    def __init__(self, model: MAEModel, device: torch.device = None):
        self.model = model
        self.device = device or torch.device("cpu")
        self.model.to(self.device)

    def _patchify(self, imgs: torch.Tensor, patch_size: int) -> torch.Tensor:
        B, C, H, W = imgs.shape
        P = patch_size
        imgs = imgs.reshape(B, C, H // P, P, W // P, P)
        imgs = imgs.permute(0, 2, 4, 1, 3, 5).reshape(B, -1, P * P * C)
        return imgs

    def _compute_loss(self, pred, target, mask):
        loss = ((pred - target) ** 2).mean(dim=-1)  # (B, num_patches)
        loss = (loss * mask.float()).sum() / mask.float().sum()
        return loss

    def _train_one_epoch(self, loader, optimizer):
        self.model.train()
        total_loss = 0.0
        for masked_imgs, original_imgs, masks in tqdm(loader, desc="MAE Train", leave=False):
            masked_imgs  = masked_imgs.to(self.device)
            original_imgs = original_imgs.to(self.device)
            masks        = masks.to(self.device)           # (B, num_patches)

            pred   = self.model(masked_imgs, masks)
            target = self._patchify(original_imgs, self.model.patch_size)

            loss = self._compute_loss(pred, target, masks)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        return total_loss / len(loader)

    @torch.no_grad()
    def _evaluate(self, loader):
        self.model.eval()
        total_loss = 0.0
        for masked_imgs, original_imgs, masks in loader:
            masked_imgs   = masked_imgs.to(self.device)
            original_imgs  = original_imgs.to(self.device)
            masks         = masks.to(self.device)

            pred   = self.model(masked_imgs, masks)
            target = self._patchify(original_imgs, self.model.patch_size)
            total_loss += self._compute_loss(pred, target, masks).item()

        return total_loss / len(loader)

    def train(
        self,
        train_loader,
        valid_loader,
        optimizer,
        epochs: int = 100,
        patience: int = 10,
        save_path: str = "./checkpoints/mae",
        scheduler=None,
    ):
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)

        best_valid_loss = float("inf")
        patience_counter = 0

        for epoch in range(1, epochs + 1):
            train_loss = self._train_one_epoch(train_loader, optimizer)
            valid_loss = self._evaluate(valid_loader)

            if scheduler:
                scheduler.step()

            print(f"[MAE] Epoch {epoch:03d} | Train Loss: {train_loss:.6f} | Valid Loss: {valid_loss:.6f}")

            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                patience_counter = 0
                ckpt_path = save_dir / f"epoch_{epoch:03d}_loss_{valid_loss:.6f}.pt"
                torch.save(self.model.state_dict(), ckpt_path)
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"[MAE] Early stopping at epoch {epoch}.")
                    break
```

---

## 4. `configs/config.yaml` — MAE 블록 추가

```yaml
# configs/config.yaml 에 추가

mae:
  patch_size: 4          # 32x32 이미지 → 8x8 = 64패치
  mask_ratio: 0.75
  encoder_dim: 192
  encoder_depth: 6
  decoder_dim: 96
  decoder_depth: 2
  pretext:
    lr: 1.5e-4
    weight_decay: 0.05
    epochs: 100
    patience: 10
    save_path: "./checkpoints/mae/pretext"
    scheduler:
      enabled: true
      type: "CosineAnnealingLR"
      T_max: 100
  downstream:
    lr: 1e-4
    weight_decay: 0.05
    epochs: 50
    patience: 7
    save_path: "./checkpoints/mae/downstream"
    scheduler:
      enabled: false
```

---

## 5. `experiments/train.py`
곧 추가할 예정이다...!

<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[ejkiwi.github.io]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://ejkiwi.github.io/</link><image><url>https://ejkiwi.github.io/lib/media/favicon.png</url><title>ejkiwi.github.io</title><link>https://ejkiwi.github.io/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 12 Aug 2025 17:54:27 GMT</lastBuildDate><atom:link href="https://ejkiwi.github.io/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 12 Aug 2025 17:54:27 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[2025_여름_모각코]]></title><description><![CDATA[ 
 <br>
<br><a data-href="뿌가각 모각코 활동" href="https://ejkiwi.github.io/2025_여름_모각코/뿌가각-모각코-활동.html" class="internal-link" target="_self" rel="noopener nofollow">뿌가각 모각코 활동</a>
<br><a data-href="20250109 모각코 활동 1회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250109-모각코-활동-1회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250109 모각코 활동 1회차</a>
<br><a data-href="20250721 모각코 활동 2회차" href="https://ejkiwi.github.io/2025_여름_모각코/20250721-모각코-활동-2회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250721 모각코 활동 2회차</a>
<br><a data-href="20250731 모각코 활동 3회차" href="https://ejkiwi.github.io/2025_여름_모각코/20250731-모각코-활동-3회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250731 모각코 활동 3회차</a>
<br><a data-href="20250808 모각코 활동 4회차" href="https://ejkiwi.github.io/2025_여름_모각코/20250808-모각코-활동-4회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250808 모각코 활동 4회차</a>
]]></description><link>https://ejkiwi.github.io/2025_여름_모각코/2025_여름_모각코.html</link><guid isPermaLink="false">2025_여름_모각코/2025_여름_모각코.md</guid><pubDate>Tue, 12 Aug 2025 17:54:14 GMT</pubDate></item><item><title><![CDATA[20250718 모각코 활동 1회차]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.컴퓨터 구조 기본 개념과 디지털 논리 이해하기<br><br><img alt="computersystem" src="https://ejkiwi.github.io/lib/media/computersystem.png" referrerpolicy="no-referrer"><br><br>중앙처리장치 (CPU)<br>
<br>명령어 실행과 연산을 담당하는 핵심 부품
<br>제어장치(Control Unit)와 산술논리장치(ALU)로 구성
<br>레지스터를 통해 임시 데이터 저장
<br>메모리 시스템<br>
<br>주기억장치(RAM): 실행 중인 프로그램과 데이터 저장
<br>보조기억장치(HDD, SSD): 영구적인 데이터 저장
<br>캐시 메모리: CPU와 주기억장치 간의 속도 차이 보완
<br>입출력 장치<br>
<br>입력장치: 키보드, 마우스, 터치스크린 등
<br>출력장치: 모니터, 프린터, 스피커 등
<br>입출력 제어기를 통해 CPU와 통신
<br><br>소프트웨어는 계층적 구조로 구성<br>시스템 소프트웨어<br>
<br>운영체제(OS): 하드웨어 자원 관리 및 사용자 인터페이스 제공
<br>컴파일러: 고급 언어를 기계어로 번역
<br>어셈블러: 어셈블리어를 기계어로 번역
<br>응용 소프트웨어<br>
<br>사용자가 직접 사용하는 프로그램
<br>워드프로세서, 웹브라우저, 게임 등
<br><br><br>
<br>0과 1로만 구성된 수 체계
<br>컴퓨터에서 모든 정보를 표현하는 기본 단위
<br>예: 1011₂ = 1×2³ + 0×2² + 1×2¹ + 1×2⁰ = 11₁₀
<br><br>
<br>0~7의 숫자로 구성
<br>이진수 3자리를 8진수 1자리로 변환 가능
<br>예: 123₈ = 1×8² + 2×8¹ + 3×8⁰ = 83₁₀
<br><br>
<br>0~9, A~F로 구성 (A=10, B=11, C=12, D=13, E=14, F=15)
<br>이진수 4자리를 16진수 1자리로 변환 가능
<br>메모리 주소 표현에 주로 사용
<br>예: A3F₁₆ = 10×16² + 3×16¹ + 15×16⁰ = 2623₁₀
<br><br>10진수 → 2진수: 2로 나누어 나머지를 역순으로 배열 2진수 → 16진수: 4자리씩 묶어서 변환<br>
<br>1010 1100₂ =  AC₁₆
<br><br><br>AND 게이트<br>
<br>모든 입력이 1일 때만 출력이 1
<br>진리표: A=0,B=0→0 | A=0,B=1→0 | A=1,B=0→0 | A=1,B=1→1
<br>기호: D형 게이트 모양
<br>OR 게이트<br>
<br>하나 이상의 입력이 1이면 출력이 1
<br>진리표: A=0,B=0→0 | A=0,B=1→1 | A=1,B=0→1 | A=1,B=1→1
<br>기호: 곡선형 게이트 모양
<br>NOT 게이트 (인버터)<br>
<br>입력의 반대 값을 출력
<br>진리표: A=0→1 | A=1→0
<br>기호: 삼각형에 작은 원
<br><br>XOR 게이트 (배타적 OR)<br>
<br>입력이 서로 다를 때만 출력이 1
<br>진리표: A=0,B=0→0 | A=0,B=1→1 | A=1,B=0→1 | A=1,B=1→0
<br>덧셈 회로에서 자리올림 없는 합 구현
<br>NAND 게이트<br>
<br>AND 게이트의 출력을 NOT한 결과
<br>범용 게이트: 모든 논리 함수 구현 가능
<br>NOR 게이트<br>
<br>OR 게이트의 출력을 NOT한 결과
<br>범용 게이트: 모든 논리 함수 구현 가능
<br><br><br>교환 법칙 (Commutative Law)<br>
<br>A + B = B + A
<br>A · B = B · A
<br>결합 법칙 (Associative Law)<br>
<br>(A + B) + C = A + (B + C)
<br>(A · B) · C = A · (B · C)
<br>분배 법칙 (Distributive Law)<br>
<br>A · (B + C) = A·B + A·C
<br>A + (B · C) = (A + B) · (A + C)
<br>항등 법칙 (Identity Law)<br>
<br>A + 0 = A
<br>A · 1 = A
<br>보수 법칙 (Complement Law)<br>
<br>A + A' = 1
<br>A · A' = 0
<br><br>논리식의 부정을 간단히 하기 때문에 중요하다!<br>(A + B)' = A' · B'<br>
<br>OR의 부정은 각각의 부정을 AND
<br>(A · B)' = A' + B'<br>
<br>AND의 부정은 각각의 부정을 OR
<br><br><br>
<br>출력이 현재 입력에만 의존
<br>메모리 기능 없음
<br>입력이 바뀌면 즉시 출력 변화
<br><br>가산기 (Adder)<br>
<br>반가산기(Half Adder): 2개의 1비트 수를 더함

<br>입력: A, B / 출력: Sum, Carry
<br>Sum = A ⊕ B, Carry = A · B


<br>전가산기(Full Adder): 이전 자리올림까지 고려

<br>입력: A, B, Cin / 출력: Sum, Cout


<br>디코더 (Decoder)<br>
<br>n비트 입력을 2ⁿ개의 출력선 중 하나를 선택
<br>메모리 주소 디코딩에 사용
<br>인코더 (Encoder)<br>
<br>2ⁿ개의 입력선 중 하나를 n비트로 인코딩
<br>키보드 입력 처리에 사용
<br>멀티플렉서 (Multiplexer, MUX)<br>
<br>여러 입력 중 하나를 선택하여 출력
<br>선택신호에 따라 입력 경로 결정
<br><br><br>
<br>출력이 현재 입력과 이전 상태에 의존
<br>메모리 기능 보유
<br>클록 신호에 동기화되어 동작

<br>컴퓨터에서 모든 동작의 타이밍을 맞춰주는 기준 신호
<br>컴퓨터 부품들이 언제 동작해야 하는지 알려주는 시간 기준


<br><br>플립플롭 (Flip-Flop)<br>
<br>1비트 정보를 저장하는 기본 메모리 소자
<br>클록 신호에 의해 상태 변화
<br><br>카운터 (Counter)<br>
<br>클록 펄스를 계수하는 회로
<br>업카운터, 다운카운터, 링카운터 등
<br>레지스터 (Register)<br>
<br>여러 비트의 정보를 저장
<br>시프트 레지스터: 데이터를 좌우로 이동
<br>메모리<br>
<br>대용량 데이터 저장을 위한 순차 회로
<br>RAM, ROM 등의 기본 구조
]]></description><link>https://ejkiwi.github.io/2025_여름_모각코/20250718-모각코-활동-1회차.html</link><guid isPermaLink="false">2025_여름_모각코/20250718 모각코 활동 1회차.md</guid><pubDate>Tue, 12 Aug 2025 17:02:26 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/computersystem.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/computersystem.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[프로세서 기초와 명령어 집합]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 프로세서 기초와 명령어 집합 공부하기<br><br>일단 컴퓨터의 구조이다<br><br><br><img alt="cpu" src="https://ejkiwi.github.io/lib/media/cpu.png" referrerpolicy="no-referrer"><br>
제어장치가 시키고, 연산장치가 일하고, 레지스터가 저장하는 느낌이다~_~<br><br>역할<br>
<br>명령어를 해석하고 실행 순서를 제어
<br>각 구성 요소에 제어 신호 전송
<br>프로그램 카운터(PC) 관리<br>
주요 기능
<br>명령어 인출 (Fetch): 메모리에서 명령어 가져오기
<br>명령어 해석 (Decode): 명령어의 의미 분석
<br>실행 제어 (Execute): 필요한 제어 신호 생성<br>
구성 요소
<br>프로그램 카운터 (PC): 다음 실행할 명령어의 주소 저장
<br>명령어 레지스터 (IR): 현재 실행 중인 명령어 저장
<br>제어 신호 생성기: 각 구성 요소 제어
<br><br>역할<br>
<br>모든 산술 연산과 논리 연산 수행
<br>비교 연산 및 시프트 연산 담당<br>
지원 연산
<br>산술 연산: 덧셈, 뺄셈, 곱셈, 나눗셈
<br>논리 연산: AND, OR, XOR, NOT
<br>비교 연산: 같음, 크기 비교
<br>시프트 연산: 좌시프트, 우시프트<br>
플래그 레지스터
<br>Zero Flag (Z): 결과가 0인지 표시
<br>Carry Flag (C): 자리올림 발생 시
<br>Overflow Flag (V): 오버플로 발생 시
<br>Negative Flag (N): 결과가 음수인지 표시
<br><br>역할<br>
<br>CPU에서 당장 사용할 데이터 보관<br>
특징
<br>CPU 내부의 고속 저장 공간
<br>메모리보다 훨씬 빠른 접근 속도
<br>제한된 개수 (보통 16~32개)<br>
종류
<br>범용 레지스터: 일반적인 데이터 저장
<br>특수 목적 레지스터: 특정 용도로 사용

<br>PC (Program Counter): 다음 명령어 주소
<br>SP (Stack Pointer): 스택의 최상위 주소
<br>LR (Link Register): 함수 호출 시 복귀 주소


<br><br><br>정의<br>
<br>소프트웨어와 하드웨어 간의 인터페이스
<br>프로그래머가 볼 수 있는 컴퓨터의 추상적 모델
<br>CPU가 이해할 수 있는 명령어들의 집합<br>
ISA가 정의하는 요소
<br>명령어 집합: 사용 가능한 모든 명령어
<br>레지스터: 개수, 크기, 용도
<br>메모리 모델: 주소 지정 방식, 데이터 타입
<br>예외 처리: 인터럽트, 예외 상황 처리
<br><br><br>특징<br>
<br>오픈소스 ISA
<br>모듈러 설계 (기본 + 확장)
<br>학술 및 상업적 목적으로 자유 사용
<br><br>32개의 범용 레지스터 (x0 ~ x31)<br><br>특수 레지스터<br>
<br>x0 (zero): 항상 0 값, 쓰기 무시
<br>pc: 프로그램 카운터 (명시적 접근 불가)
<br><br><br>용도: 레지스터 간 연산 형식: op rd, rs1, rs2<br><br>용도: 즉시값과 레지스터 연산, 로드 명령어 형식: op rd, rs1, imm 또는 op rd, imm(rs1)<br><br>용도: 스토어 명령어 형식: op rs2, imm(rs1)<br><br>용도: 조건부 분기 형식: op rs1, rs2, label<br><br>용도: 상위 즉시값 로드 형식: op rd, imm<br><br>용도: 무조건 점프 형식: op rd, label<br><br><br>형식: addi x1, x2, 100<br>
의미: 즉시값을 직접 사용<br>
장점: 빠름<br>
단점: 값의 크기 제한<br><br>형식: add x1, x2, x3<br>
의미: 레지스터 값 직접 사용<br>
장점: 가장 빠름<br>
특징: RISC의 기본 방식<br><br>형식: lw x1, 8(x2)<br>
의미: 베이스 레지스터 + 오프셋<br>
용도: 배열, 구조체 접근<br>
계산: 주소 = x2 + 8<br><br>형식: beq x1, x2, label<br>
의미: 현재 PC + 오프셋<br>
용도: 분기 명령어<br>
장점: 위치 독립적 코드<br><br><br>1. 인출 (Fetch)<br>IR ← Memory[PC]
PC ← PC + 4
<br>2. 해석 (Decode)<br>
<br>명령어 형식 판별
<br>필요한 레지스터 식별
<br>제어 신호 생성
<br>3. 실행 (Execute)<br>
<br>ALU 연산 수행
<br>메모리 접근 (필요시)
<br>결과 저장
<br>4. 쓰기 (Write-back)<br>
<br>결과를 목적지 레지스터에 저장
<br>플래그 업데이트
<br><br>1. Fetch: IR ← Memory[PC], PC ← PC + 4
2. Decode: R-type, rs1=x2, rs2=x3, rd=x1, funct=ADD
3. Execute: ALU_result ← Register[x2] + Register[x3]
4. Write-back: Register[x1] ← ALU_result
]]></description><link>https://ejkiwi.github.io/2025_여름_모각코/20250731-모각코-활동-3회차.html</link><guid isPermaLink="false">2025_여름_모각코/20250731 모각코 활동 3회차.md</guid><pubDate>Tue, 12 Aug 2025 17:24:13 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/cpu.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/cpu.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[메모리 시스템과 캐시]]></title><description><![CDATA[ 
 <br><br><br><br><img alt="memory" src="https://ejkiwi.github.io/lib/media/memory.png" referrerpolicy="no-referrer"><br>(위 사진의 input sources는 안 다룰것이댜)<br><br><br>
<br>용량: 32~64개 × 32/64비트 = 수백 바이트
<br>속도: 1 클록 사이클
<br>위치: CPU 내부
<br>용도: 현재 작업 중인 데이터
<br><br>
<br>용량: 16KB ~ 64KB
<br>속도: 1-2 클록 사이클
<br>위치: CPU 코어 내부
<br>특징: 명령어 캐시(I-Cache)와 데이터 캐시(D-Cache)로 분리
<br><br>
<br>용량: 256KB ~ 1MB
<br>속도: 3-10 클록 사이클
<br>위치: CPU 코어별 또는 공유
<br>특징: 통합 캐시 (명령어 + 데이터)
<br><br>
<br>용량: 8MB ~ 32MB
<br>속도: 10-20 클록 사이클
<br>위치: 여러 코어가 공유
<br>특징: 코어 간 데이터 공유 역할
<br><br>
<br>용량: 4GB ~ 128GB
<br>속도: 100-300 클록 사이클
<br>기술: DDR4, DDR5
<br>특징: 휘발성, 프로그램 실행 공간
<br><br>
<br>SSD: 빠르지만 비쌈 (마이크로초 단위)
<br>HDD: 느리지만 저렴 (밀리초 단위)
<br>특징: 비휘발성, 영구 저장
<br><br><br><br>
<br>캐시 히트 (Cache Hit)
<br>CPU → 캐시 확인 → 데이터 있음! → 빠른 반환
      (1-10 사이클)
<br>
<br>캐시 미스 (Cache Miss)
<br>CPU → 캐시 확인 → 데이터 없음 → 메인 메모리 접근 → 캐시 업데이트 → 반환
      (100-300 사이클)
<br><br>
<br>캐시 라인 (Cache Line/Block)

<br>캐시와 메모리 간 데이터 전송의 최소 단위
<br>일반적으로 32바이트 ~ 128바이트
<br>공간적 지역성 활용


<br>태그 (Tag)

<br>캐시 라인이 메모리의 어느 부분을 저장하고 있는지 식별
<br>메모리 주소의 상위 비트들


<br>유효 비트 (Valid Bit)

<br>캐시 라인에 유효한 데이터가 있는지 표시
<br>0: 무효, 1: 유효


<br>캐시 교체 방식(정책)

<br>LRU (Least Recently Used)

<br>가장 오래 사용되지 않은 블록을 교체


<br>FIFO (First In First Out)

<br>가장 먼저 들어온 블록을 교체


<br>Random

<br>무작위로 블록 선택




<br><br>
<br>직접매핑 : 메모리의 각 블록이 캐시의 정확히 한 위치에만 매핑
<br>완전 연관 매핑 : 메모리의 어떤 블록이라도 캐시의 어느 위치에나 저장 가능
<br>집합 연관 매핑 : 캐시를 여러 집합(set)으로 나누고, 각 집합 내에서는 완전 연관
]]></description><link>https://ejkiwi.github.io/2025_여름_모각코/20250808-모각코-활동-4회차.html</link><guid isPermaLink="false">2025_여름_모각코/20250808 모각코 활동 4회차.md</guid><pubDate>Tue, 12 Aug 2025 17:43:11 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/memory.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/memory.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[데이터 표현과 산술 연산]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.데이터 표현(정수, 실수, 문자) 이해하기<br>
2.산술 연산을 위한 회로 설계 공부<br><br><br><br>특징<br>
<br>0 이상의 양수만 표현
<br>n비트로 0부터 2ⁿ-1까지 표현 가능
<br>모든 비트가 크기를 나타냄
<br>예시

<br>0000₂ = 0₁₀
<br>0101₂ = 5₁₀
<br>1111₂ = 15₁₀


<br><br><br>
<br>최상위 비트(MSB)가 부호 비트 (0=양수, 1=음수)
<br>나머지 비트가 크기를 나타냄
<br>문제점 : 0이 두 개 (+0, -0), 연산이 복잡
<br>예시 (4비트)

<br>0011₂ = +3₁₀
<br>1011₂ = -3₁₀


<br><br>
<br>양수의 모든 비트를 반전시켜 음수 표현
<br>문제점 : 여전히 0이 두 개 존재
<br>예시

<br>+3: 0011₂
<br>-3: 1100₂ (0011의 모든 비트 반전)


<br><br>현재 가장 널리 사용되는 방식<br>계산 방법<br>
<br>양수의 1의 보수를 구함 (모든 비트 반전)
<br>1을 더함
<br>
<br>예시

<br>+3: 0011₂
<br>-3: 1100₂ + 1₂ = 1101₂


<br>0이 하나만 존재
<br>덧셈/뺄셈 회로가 동일
<br>범위: -2ⁿ⁻¹ ~ 2ⁿ⁻¹-1
<br>8비트 2의 보수 범위: -128 ~ +127
<br><br>표현 가능한 범위를 벗어나는 경우<br>
<br>부호 없는 정수: 자리올림이 발생하면 오버플로
<br>부호 있는 정수: 부호가 바뀌면 오버플로
<br><br><br>구성 요소<br>
<br>부호(Sign): 1비트 (0=양수, 1=음수)
<br>지수(Exponent): 8비트 (단정도), 11비트 (배정도)
<br>가수(Mantissa/Significand): 23비트 (단정도), 52비트 (배정도)
<br><br>형식: S EEEEEEEE MMMMMMMMMMMMMMMMMMMMMMM<br>
<br>S: Sign bit (부호 비트) - 1비트
<br>EEEEEEEE: Exponent (지수) - 8비트
<br>MMMMMMMMMMMMMMMMMMMMMMM: Mantissa/Significand (가수) - 23비트
<br>표현 방법<br>
<br>실수를 이진 소수로 변환
<br>정규화 (1.xxxxx × 2ᵏ 형태)
<br>지수에 바이어스(127) 더함
<br>가수에서 정수부 1 제거 (암묵적 1)
<br>
<br>예시: 12.75₁₀ 표현

<br>12.75₁₀ = 1100.11₂
<br>정규화: 1.10011 × 2³
<br>지수: 3 + 127 = 130₁₀ = 10000010₂
<br>가수: 10011000000000000000000₂
<br>결과: 0 10000010 10011000000000000000000


<br><br>
<br>0: 지수와 가수가 모두 0
<br>무한대(∞): 지수가 모두 1, 가수가 0
<br>NaN: 지수가 모두 1, 가수가 0이 아님
<br><br>
<br>모든 실수를 정확히 표현할 수 없음
<br>반올림 오차 발생 가능
<br>예: 0.1₁₀을 이진수로 정확히 표현 불가
<br><br><br>특징<br>
<br>7비트 코드 (0~127)
<br>128개 문자 표현
<br>영어 알파벳, 숫자, 특수문자, 제어문자
<br>주요 코드<br>
<br>'0' ~ '9': 48 ~ 57 (30H ~ 39H)
<br>'A' ~ 'Z': 65 ~ 90 (41H ~ 5AH)
<br>'a' ~ 'z': 97 ~ 122 (61H ~ 7AH)
<br>공백: 32 (20H)
<br>줄바꿈(LF): 10 (0AH)
<br>확장 ASCII<br>
<br>8비트 사용 (0~255)
<br>추가 128개 문자 (특수문자, 외국어 문자)
<br><br>배경<br>
<br>ASCII로는 전 세계 모든 문자 표현 불가
<br>다국어 지원 필요
<br>특징<br>
<br>전 세계 모든 문자를 하나의 코드 체계로 통합
<br>현재 약 150,000개 문자 수록
<br>코드 포인트: U+0000 ~ U+10FFFF
<br>인코딩 방식<br>
<br>UTF-8: 가변 길이 (1~4바이트), ASCII 호환
<br>UTF-16: 가변 길이 (2~4바이트)
<br>UTF-32: 고정 길이 (4바이트)
<br>UTF-8의 장점<br>
<br>ASCII와 완전 호환
<br>인터넷에서 가장 널리 사용
<br>영어는 1바이트, 한글은 3바이트로 효율적
<br><br><br>
<br>CPU의 핵심 구성 요소
<br>산술 연산(+, -, ×, ÷)과 논리 연산(AND, OR, NOT) 수행
<br>비교 연산과 시프트 연산도 담당
<br><br>입력<br>
<br>A, B: 피연산자 (보통 32비트 또는 64비트)
<br>제어 신호: 수행할 연산 지정
<br>캐리 입력(Cin): 이전 연산의 자리올림
<br>출력<br>
<br>결과(Result): 연산 결과
<br>플래그(Flags): 연산 상태 정보

<br>Zero Flag (Z): 결과가 0일 때
<br>Carry Flag (C): 자리올림 발생 시
<br>Overflow Flag (V): 오버플로 발생 시
<br>Negative Flag (N): 결과가 음수일 때


<br><br>제어 신호 예시 (4비트)<br>
<br>0000: A AND B
<br>0001: A OR B
<br>0010: A + B
<br>0110: A - B
<br>0111: A &lt; B인지 비교
<br><br><br>기능: 2개의 1비트 수를 더함 입력: A, B 출력: Sum, Carry<br>논리식<br>
<br>Sum = A ⊕ B (XOR)
<br>Carry = A · B (AND)
<br>진리표<br><br><br>기능: 2개의 1비트 수와 이전 자리올림을 더함 입력: A, B, Cin 출력: Sum, Cout<br>논리식<br>
<br>Sum = A ⊕ B ⊕ Cin
<br>Cout = A·B + Cin·(A ⊕ B)
<br>구현 방법<br>
<br>반가산기 2개를 연결하여 구성 가능
<br><br><br>원리: A - B = A + (-B) = A + B의 2의 보수<br>구현 방법<br>
<br>B의 모든 비트를 반전 (1의 보수)
<br>가산기의 캐리 입력에 1을 넣음
<br>A와 처리된 B를 가산기로 더함
<br>장점<br>
<br>덧셈기 회로를 그대로 활용
<br>별도의 뺄셈 회로 불필요
<br><br>구조<br>
<br>덧셈기에 XOR 게이트 추가
<br>제어 신호로 덧셈/뺄셈 선택
<br>동작<br>
<br>Sub = 0: 덧셈 모드 (B는 그대로 전달)
<br>Sub = 1: 뺄셈 모드 (B는 반전되어 전달, Cin = 1)
<br><br><br>시프트-덧셈 방법<br>
<br>승수의 각 비트를 검사
<br>1이면 피승수를 더하고, 0이면 건너뜀
<br>매번 피승수를 왼쪽으로 시프트
<br>부스 알고리즘 (Booth's Algorithm)<br>
<br>부호 있는 정수의 효율적인 곱셈
<br>연속된 1을 하나의 덧셈과 뺄셈으로 처리
<br><br>복원 나눗셈 (Restoring Division)<br>
<br>피제수에서 제수를 빼봄
<br>결과가 음수면 복원하고 몫에 0
<br>결과가 양수면 몫에 1
<br>나머지를 왼쪽으로 시프트하고 반복
]]></description><link>https://ejkiwi.github.io/2025_여름_모각코/20250721-모각코-활동-2회차.html</link><guid isPermaLink="false">2025_여름_모각코/20250721 모각코 활동 2회차.md</guid><pubDate>Tue, 29 Jul 2025 09:25:39 GMT</pubDate></item><item><title><![CDATA[2025 cnu 동계 학습동아리 (복습)]]></title><description><![CDATA[ 
 ]]></description><link>https://ejkiwi.github.io/2025-cnu-동계-학습동아리-(복습)/2025-cnu-동계-학습동아리-(복습).html</link><guid isPermaLink="false">2025 cnu 동계 학습동아리 (복습)/2025 cnu 동계 학습동아리 (복습).md</guid><pubDate>Tue, 29 Jul 2025 08:02:18 GMT</pubDate></item><item><title><![CDATA[통통이들 학습 동아리 활동]]></title><description><![CDATA[ 
 <br>비록 미적짱은 되지 못했지만 확통짱은 될 수 있을것이야... \( •̀ ω •́ )/ ]]></description><link>https://ejkiwi.github.io/2025-cnu-동계-학습동아리-(복습)/통통이들-학습-동아리-활동.html</link><guid isPermaLink="false">2025 cnu 동계 학습동아리 (복습)/통통이들 학습 동아리 활동.md</guid><pubDate>Tue, 29 Jul 2025 08:02:08 GMT</pubDate></item><item><title><![CDATA[뿌가각 모각코 활동]]></title><description><![CDATA[ 
 <br>뿌가각 = 모각코 뿌시기<br>나의 모각코 활동 다짐 : 컴퓨터구조를  뿌시겠다!!!!!!]]></description><link>https://ejkiwi.github.io/2025_여름_모각코/뿌가각-모각코-활동.html</link><guid isPermaLink="false">2025_여름_모각코/뿌가각 모각코 활동.md</guid><pubDate>Tue, 29 Jul 2025 07:34:15 GMT</pubDate></item><item><title><![CDATA[2025_겨울_모각코]]></title><description><![CDATA[ 
 <br>
<br><a data-href="파샤샥 모각코 활동" href="https://ejkiwi.github.io/2025_겨울_모각코/파샤샥-모각코-활동.html" class="internal-link" target="_self" rel="noopener nofollow">파샤샥 모각코 활동</a>
<br><a data-href="20250109 모각코 활동 1회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250109-모각코-활동-1회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250109 모각코 활동 1회차</a>
<br><a data-href="20250117 모각코 활동 2회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250117-모각코-활동-2회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250117 모각코 활동 2회차</a>
<br><a data-href="20250126 모각코 활동 3회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250126-모각코-활동-3회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250126 모각코 활동 3회차</a>
<br><a data-href="20250202 모각코 활동 4회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250202-모각코-활동-4회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250202 모각코 활동 4회차</a>
<br><a data-href="20250209 모각코 활동 5회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250209-모각코-활동-5회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250209 모각코 활동 5회차</a>
<br><a data-href="20250215 모각코 활동 6회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250215-모각코-활동-6회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250215 모각코 활동 6회차</a>
]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/2025_겨울_모각코.html</link><guid isPermaLink="false">2025_겨울_모각코/2025_겨울_모각코.md</guid><pubDate>Sat, 15 Feb 2025 17:03:58 GMT</pubDate></item><item><title><![CDATA[20250109 모각코 활동 1회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 연산자, 조건문, 반복문, 함수 복습<br><br><br>
<br>연산자 : 어떤 연산을 나타내는 기호
<br>피연산자 : 연산의 대상이 되는 숫자나 변수를 의미
<br><br>
<br>거듭제곱(지수)는 높은 우선순위를 가짐
<br>거듭제곱 표현 시 왼쪽부터 계산됨..!! abc (&lt;-방향)
<br>지수자리에는 실수도 들어올 수 있다
<br><br><br>
<br>부울BOOL형 T F의 값을 가짐
<br>붙어있는 그 자체로 하나임.
<br>참 거짓 값을 갖게 된 비교연산자 덩어리들을 변수에집어넣을수도 있음.
<br><br><br>
<br>부울BOOL형 T F의 값을 가짐
<br><br><br>
<br>붙어있는 그 자체로 하나임.
<br>비트 단위연산자에도 적용 가능
<br><br><br>
<br>숫자를 2진수로 변환해서 비트 단위로 연산하는 연산자
<br><br>? 시프트 ?<br>
<br>비트를 왼쪽 또는 오른쪽으로 이동시키는 것.
<br>왼쪽 시프트 : 비트를 왼쪽으로 밀고 오른쪽에 0을 채워넣음 (값이 두 배)
<br>오른쪽 시프트 : 비트를 오른쪽으로 밀고 왼쪽에 0을 채워넣음(값이 절반)
<br><br>
<br>0제외 모든 수 : T
<br>0 : F
<br>문자 : T
<br>공백문자 "" : F
<br>None : F
<br><br><br><br><br><br><br><br><br><br><br>
<br>횟수 제어 반복(for문) : 반복 횟수 지정
<br>조건 제어 반복(while문) : 특정 조건 만족시 계속 반복
<br><br><br>
<br>range(start=0,stop,step=1) : start와step은 따로 지정하지 않으면 0과 1로 지정. 반드시 지정해야하는 값은 stop값.
<br><br><br><br><br>
<br>T =turtle.Turtle() -&gt; T라는 변수로 거북이 객체를 생성
<br>T.shape("turtle") -&gt; 거북이 객체 T에 shape()라는 명령을 내림 이 때 shape()는 메소드
<br>t.forward(100) -&gt; 거북이 객체 T에&nbsp; forward()라는 명령을 내림 이 때 forward()는 메소드
<br>print('helloworld')&nbsp; -&gt; 'helloworld'라는 문자열을 출력하기 위한 명령을 내리는 print()함수.
<br>함수 : 여러 개의 매개변수를 가질 수 있고, 이 매개변수를 통해 일을 함. 객체가 없더라도 일을 할 수 있고, print()처럼 단독으로 사용 가능. 함수()
<br>메소드 : 매개변수 가질 수 있음. 클래스의 일부로 메소드가 존재하며( 아마 메소드가 클래스의 일부로서 존재한다는 뜻 같음 ) 객체가 있어야 일을 할 수 있음. 객체.메소드() 느낌으로 사용해야함
<br><br>
<br>코드의 재사용성 증가
<br>가독성 향상
<br>유지보수 용이
<br><br>
<br>
a='abcde'=&gt;문자열

<br>
문자열 안에 있는 개별 문자들을 추출하는 작업이 기본적. (문자열을 문자로 분해)

<br>
문자열은 인덱스로 접근 가능

<br>
인덱스(0~)/음수인덱스(-1~)

<br>
슬라이싱 : [이상:미만]

<br>
문자열 리스트 변환 : list('a')

<br>
문자열을 단어로 나누는 작업

<br>  a.split() -&gt; 띄어쓰기를 기준으로 구분
<br>a.split('.') -&gt;&nbsp; '.' 을 기준으로 구분
<br>=&gt; 즉, split()의 괄호 안에는 기준이 들어가는 거임.


<br>
단어들을 다시 붙이는 작업

<br>''.join(문자들) -&gt; 문자들이 붙음
<br>'-'.join(문자들) -&gt; 문자들이 -을 사용하여 붙여짐.


<br>
문자열의 출현 횟수 계산

<br>a.count('찾고싶은녀석')


<br>
문자열이 무엇으로 시작/끝 이 나는지 확인

<br>a.startswith() =&gt; 시작 . 부울형으로 결과가 나옴
<br>a.endswith() =&gt; 끝 . 부울형 결과 나옴


]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250109-모각코-활동-1회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250109 모각코 활동 1회차.md</guid><pubDate>Sat, 15 Feb 2025 15:46:11 GMT</pubDate></item><item><title><![CDATA[20250215 모각코 활동 6회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 프로젝트 결과 발표와 활동 마무리<br><br>
<br>
리스트, 튜플, 집합, 딕셔너리

<br>기본 데이터들의 구조이다
<br>데이터를 저장하고 정리할 때 사용된다.
<br>순서가 있는 데이터 모음은 리스트
<br>순서가 있는 데이터 모음이지만 수정이 불가능한 튜플
<br>중복과 순서 둘 다 없는 데이터 모음은 집합
<br>키 - 값 쌍의 데이터 모음은 딕셔너리


<br>
연산자, 조건문, 반복문, 함수

<br>데이터를 처리하거나 조작할 때 필수적으로 사용된다.
<br>수학적 계산이나 논리적 비교를 위해서 연산자를 사용하고
<br>특정 조건에 따라 동작을 실행하기 위해 조건문을 사용하고
<br>같은 작업을 여러 번 실행하기 위해 반복문을 사용한다.
<br>함수는 반복적인 작업을 하나로 묶어서 재사용할 수 있해주는 도구이다.


<br>
넘파이

<br>수치 데이터를 더욱 빠르고 효율적으로 다룰 수 있게 해는 파이썬 라이브러리이다.
<br>넘파이의 배열은 리스트보다 훨씬 굿굿이다. ( 벡터와 행렬 연산에 강력함 )
<br>기계학습 모델에서 데이터를 입력할 때 넘파이에서의 배열을 주로 사용함.


<br>
판다스

<br>데이터 분석에 최적화된 라이브러리이다. ( 데이터 정리와 분석 )
<br>DataFrame : 2차원 테이블 형태로 데이터를 쉽게 다룰 수 있게 해줌.
<br>주로 데이터 전처리에서 많이 사용.


<br>
기계학습

<br>데이터로부터 패턴을 학습해서 예측하는 과정.


]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250215-모각코-활동-6회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250215 모각코 활동 6회차.md</guid><pubDate>Sat, 15 Feb 2025 17:02:59 GMT</pubDate></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="백준" href="https://ejkiwi.github.io/백준/백준.html" class="internal-link" target="_self" rel="noopener nofollow">백준</a>
<br><a data-href="2024_여름_모각코" href="https://ejkiwi.github.io/2024_여름_모각코/2024_여름_모각코.html" class="internal-link" target="_self" rel="noopener nofollow">2024_여름_모각코</a>
<br><a data-href="2024 cnu 2차 학습동아리" href="https://ejkiwi.github.io/2024-cnu-2차-학습동아리/2024-cnu-2차-학습동아리.html" class="internal-link" target="_self" rel="noopener nofollow">2024 cnu 2차 학습동아리</a>
<br><a data-href="2025_겨울_모각코" href="https://ejkiwi.github.io/2025_겨울_모각코/2025_겨울_모각코.html" class="internal-link" target="_self" rel="noopener nofollow">2025_겨울_모각코</a>
<br><br>사실이건아무것도작성한게없다...&gt;_&lt;<br>
<br><a data-tooltip-position="top" aria-label="https://velog.io/@eonjikiwi/posts" rel="noopener nofollow" class="external-link" href="https://velog.io/@eonjikiwi/posts" target="_blank">ejkiwi_velog</a>
]]></description><link>https://ejkiwi.github.io/index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Fri, 14 Feb 2025 14:54:14 GMT</pubDate></item><item><title><![CDATA[20250209 모각코 활동 5회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 기계학습 구현<br><br><br>import pandas as pd  
import matplotlib.pyplot as plt  
  
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler  
from sklearn.linear_model import LogisticRegression  
  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.tree import plot_tree
<br><br># 데이터 다운로드  
wine = pd.read_csv('https://bit.ly/wine_csv_data')
<br># 데이터 구조 확인  
print(wine.head())  # 처음 5개의 샘플  
print(wine.info())  # 데이터프레임의 각 열의 데이터 타입과 누락된 데이터 확인  
print(wine.describe())  # 통계 ( 평균, 표준편차, 최소, 최대, 중간값, 1사분위수, 3사분위수 )
<br>   alcohol  sugar    pH  class<br>
0      9.4    1.9  3.51    0.0<br>
1      9.8    2.6  3.20    0.0<br>
2      9.8    2.3  3.26    0.0<br>
3      9.8    1.9  3.16    0.0<br>
4      9.4    1.9  3.51    0.0<br>
&lt;class 'pandas.core.frame.DataFrame'&gt;<br>
RangeIndex: 6497 entries, 0 to 6496<br>
Data columns (total 4 columns):<br>
Column   Non-Null Count  Dtype  <br><br> 0   alcohol  6497 non-null   float64<br>
1   sugar    6497 non-null   float64<br>
2   pH       6497 non-null   float64<br>
3   class    6497 non-null   float64<br>
dtypes: float64(4)<br>
memory usage: 203.2 KB<br>
None<br>
alcohol        sugar           pH        class<br>
count  6497.000000  6497.000000  6497.000000  6497.000000<br>
mean     10.491801     5.443235     3.218501     0.753886<br>
std       1.192712     4.757804     0.160787     0.430779<br>
min       8.000000     0.600000     2.720000     0.000000<br>
25%       9.500000     1.800000     3.110000     1.000000<br>
50%      10.300000     3.000000     3.210000     1.000000<br>
75%      11.300000     8.100000     3.320000     1.000000<br>
max      14.900000    65.800000     4.010000     1.000000<br># 판다스 데이터 프레임 -&gt; 넘파이 배열  
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()  
target = wine['class'].to_numpy()
<br># 데이터 나누기  
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
<br># 나눈 데이터 형태 확인  
print(train_input.shape, test_input.shape)
<br>(5197, 3) (1300, 3)<br># 데이터 전처리  
ss = StandardScaler()  
ss.fit(train_input)  
  
train_scaled = ss.transform(train_input)  
test_scaled = ss.transform(test_input)
<br><br>lr = LogisticRegression()  
lr.fit(train_scaled, train_target)  
  
print(lr.score(train_scaled, train_target))  
print(lr.score(test_scaled, test_target))  
  
print(lr.coef_, lr.intercept_)  # 로지스틱 회귀가 학습한 계수와 절편
<br>0.7808350971714451 0.7776923076923077 <a data-href="0.51268071  1.67335441 -0.68775646" href="https://ejkiwi.github.io/0.51268071  1.67335441 -0.68775646" class="internal-link" target="_self" rel="noopener nofollow">0.51268071  1.67335441 -0.68775646</a> [1.81773456]`<br><br>dt = DecisionTreeClassifier(random_state=42)  
dt.fit(train_scaled, train_target)  
  
print(dt.score(train_scaled, train_target))  
print(dt.score(test_scaled, test_target))
<br>0.996921300750433<br>
0.8592307692307692<br># 훈련 결과 시각화  
plt.figure(figsize=(10,7))  
plot_tree(dt)  
plt.show()
<br><img alt="wine_plot1" src="https://ejkiwi.github.io/lib/media/wine_tree1.png" referrerpolicy="no-referrer"><br># 자세히 살펴보기  
plt.figure(figsize=(10,7))  
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])  
plt.show()
<br><img alt="wine_plot2" src="https://ejkiwi.github.io/lib/media/wine_tree2.png" referrerpolicy="no-referrer"><br># 가지치기  
dt = DecisionTreeClassifier(max_depth=3, random_state=42)  
dt.fit(train_scaled, train_target)  
  
print(dt.score(train_scaled, train_target))  
print(dt.score(test_scaled, test_target))
<br>0.8454877814123533<br>
0.8415384615384616<br># 가지치고 난 뒤의 훈련 시각화  
plt.figure(figsize=(20,15))  
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])  
plt.show()
<br><img alt="wine_plot3" src="https://ejkiwi.github.io/lib/media/wine_tree3.png" referrerpolicy="no-referrer"><br># 전처리 하기 전의 데이터들로 다시 훈련해보기  -&gt;  결과는 같을 것.  
dt = DecisionTreeClassifier(max_depth=3, random_state=42)  
dt.fit(train_input, train_target)  
  
print(dt.score(train_input, train_target))  
print(dt.score(test_input, test_target))
<br>0.8454877814123533<br>
0.8415384615384616<br># 시각화 -&gt; 데이터를 전처리 하고 난 뒤에 훈련 한 것과 같은 트리를 갖지만, 특성값을 표준점수로 바꾸지 않았기 때문에 이해하기 더욱 쉬움.  
plt.figure(figsize=(20,15))  
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])  
plt.show()
<br><img alt="wine_plot4" src="https://ejkiwi.github.io/lib/media/wine_tree4.png" referrerpolicy="no-referrer"><br># 특성 중요도  
print(dt.feature_importances_)
<br>[0.12345626 0.86862934 0.0079144 ]]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250209-모각코-활동-5회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250209 모각코 활동 5회차.md</guid><pubDate>Mon, 10 Feb 2025 10:21:55 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/wine_tree1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/wine_tree1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[파샤샥 모각코 활동]]></title><description><![CDATA[ 
 <br>'파사삭' ==  '파이썬 파사삭'<br>나의 모각코 활동 다짐 : 파이썬의 짱이 되겠다!!!!!!<br>팀 모각코 계획<br>]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/파샤샥-모각코-활동.html</link><guid isPermaLink="false">2025_겨울_모각코/파샤샥 모각코 활동.md</guid><pubDate>Mon, 10 Feb 2025 08:46:23 GMT</pubDate></item><item><title><![CDATA[20250202 모각코 활동 4회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : Pandas 공부와 타이타닉 데이터 분석<br><br><br>
<br>판다스(Pandas) : 파이썬에서의 데이터 분석과 처리를 위해 사용되는 라이브러리.
<br>엑셀, CSV, SQL 같은 다양한 형식의 데이터를 쉽게 다룰 수 있음.
<br>numpy기반으로 만들어져 빠르고 효율적임.
<br><br>Series (1차원 데이터)<br>
<br>리스트나 배열과 비슷하지만, 인덱스를 가질 수 있음.
<br>data = pd.Series([10, 20, 30, 40], index=['A', 'B', 'C', 'D'])
<br>DataFrame (2차원 데이터)<br>
<br>표 형태의 데이터 구조로, 행과 열로 구성됨.
<br>data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'Score': [85, 90, 95]}
<br>데이터 불러오기<br>.read_csv('파일경로.csv')  # CSV 파일 불러오기
.read_excel('파일경로.xlsx') # 엑셀 파일 불러오기
<br><br>.info()  # 데이터 요약 정보
.describe()  # 수치형 데이터 통계 요약 (이 때, include = 'object' 라는 파라미터를 넣어주면, 범주형 변수에 대한 통계정보를 볼 수 있음. )
.shape  # (행, 열) 크기 확인
.columns  # 컬럼(열) 이름 확인
.dtypes  # 데이터 타입 확인 
.isnull().sum()  # 결측치(NaN) 개수 확인
.head()  # 상위 5개 행 출력
.tail()  # 하위 5개 행 출력
.dropna()  # 결측치가 하나라도 있는 행 삭제
.
.
.
등등...
<br><br><br># 필요한 모듈 불러오기  
import pandas as pd  
import matplotlib.pyplot as plt  
import seaborn as sns

# 데이터 불러오기  
df = sns.load_dataset('titanic')

# 데이터 기본 정보 확인  
print(df.info())  
print(df.isnull().sum())  # 결측치 확인  
  
# 결측치 처리 (결측치가 하나라도 있는 행은 그냥 없애버릴것이다~!)  
df = df.dropna()  
df = df.reset_index(drop=True)  # 인덱스 정리
<br>결과<br>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 15 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   survived     891 non-null    int64   
 1   pclass       891 non-null    int64   
 2   sex          891 non-null    object  
 3   age          714 non-null    float64 
 4   sibsp        891 non-null    int64   
 5   parch        891 non-null    int64   
 6   fare         891 non-null    float64 
 7   embarked     889 non-null    object  
 8   class        891 non-null    category
 9   who          891 non-null    object  
 10  adult_male   891 non-null    bool    
 11  deck         203 non-null    category
 12  embark_town  889 non-null    object  
 13  alive        891 non-null    object  
 14  alone        891 non-null    bool    
dtypes: bool(2), category(2), float64(2), int64(4), object(5)
memory usage: 80.7+ KB
None
survived         0
pclass           0
sex              0
age            177
sibsp            0
parch            0
fare             0
embarked         2
class            0
who              0
adult_male       0
deck           688
embark_town      2
alive            0
alone            0
dtype: int64
<br><br>print(df.describe())  
print(df.describe(include = 'object'))  # 범주형 변수에 대한 통계정보
<br>결과<br>         survived      pclass         age       sibsp       parch        fare
count  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000
mean     0.675824    1.192308   35.623187    0.467033    0.478022   78.919735
std      0.469357    0.516411   15.671615    0.645007    0.755869   76.490774
min      0.000000    1.000000    0.920000    0.000000    0.000000    0.000000
25%      0.000000    1.000000   24.000000    0.000000    0.000000   29.700000
50%      1.000000    1.000000   36.000000    0.000000    0.000000   57.000000
75%      1.000000    1.000000   47.750000    1.000000    1.000000   90.000000
max      1.000000    3.000000   80.000000    3.000000    4.000000  512.329200
         sex embarked  who  embark_town alive
count    182      182  182          182   182
unique     2        3    3            3     2
top     male        S  man  Southampton   yes
freq      94      115   87          115   123
<br><br># 성별에 따른 생존률  
print(df.groupby('sex')['survived'].mean() * 100)

# 객실 등급에 따른 생존률  
print(df.groupby('pclass')['survived'].mean() * 100)

# 연령대별 생존률  
print(df.groupby('age')['survived'].mean() * 100)
<br>결과<br>sex
female    93.181818
male      43.617021
Name: survived, dtype: float64

pclass
1    67.515924
2    80.000000
3    50.000000
Name: survived, dtype: float64

age
0.92     100.000000
1.00     100.000000
2.00      33.333333
3.00     100.000000
4.00     100.000000
            ...    
64.00      0.000000
65.00      0.000000
70.00      0.000000
71.00      0.000000
80.00    100.000000
Name: survived, Length: 63, dtype: float64
<br><br>corr_data = df.select_dtypes(include=['number']).corr()  # 숫자형 데이터만 추출하여 상관관계 분석  
  
# 상관관계 시각화  
plt.figure(figsize=(10, 6))  
sns.heatmap(corr_data, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)  
plt.title("heatmap - titanic data")  
plt.show()
<br><img alt="titanic1" src="https://ejkiwi.github.io/lib/media/titanicplot_1.png" referrerpolicy="no-referrer"><br># 주요 변수들간의 상관관계 시각화  
sns.pairplot(df[['survived', 'age', 'sibsp', 'fare']])  
plt.show()
<br><img alt="titanic2" src="https://ejkiwi.github.io/lib/media/titanicplot_2.png" referrerpolicy="no-referrer">]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250202-모각코-활동-4회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250202 모각코 활동 4회차.md</guid><pubDate>Sun, 02 Feb 2025 16:50:57 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/titanicplot_1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/titanicplot_1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20250126 모각코 활동 3회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 넘파이, 차트 공부 및 구현 (넘파이와 맷플롯립과 짱친되기)<br><br>Numerical Python : 파이썬에서 수치 계산을 빠르고 효율적으로 수행할 수 있도록 해주는 라이브러리.<br>
( ?라이브러리 : 자주 쓰이는 코드들을 묶어놓은 도구상자. ex - PyTorch는 AI 모델을 만들 때 쓰는 도구상자같은것... )<br><br>
<br>빠른 연산 : C로 구현되어 있어 배열 연산 속도가 매우 빠르다
<br>다차원 배열 지원

<br>복잡한 데이터 표현이 가능해진다 ( 이미지 3D : 높이.너비.채널, 동영상 4D : 프레임 수.높이.너비.채널)
<br>효율적인 연산이 가능해짐 -&gt; 100개의 2D이미지를 처한다고 쳤을 때, 일반 Python 리스트로는 구조가 복잡해지고 느려짐. 하지만 다차원 배열을 사용하면 연산이 단순하고 빨라짐.


<br>다양한 함수
<br>배열 연산이 간편 : 반복문 없이 배열 전체에 대해 한 번에 연산 수행 가능 ( 벡터화 연산 )
<br>import numpy as np ( 보통은 np라는 이름으로 불러옴 )
<br><br># 1D 배열
a = np.array([1, 2, 3])

# 2D 배열
b = np.array([[1, 2, 3], [4, 5, 6]])

# 0으로 채워진 배열
c = np.zeros((3, 3))

# 1로 채워진 배열
d = np.ones((2, 4))

# 임의의 숫자로 초기화된 배열
e = np.full((2, 2), 7)

# 연속된 숫자로 이루어진 배열
f = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]

# 균등 분할된 값
g = np.linspace(0, 1, 5)  # [0. , 0.25, 0.5 , 0.75, 1.]
<br><img alt="배열 생성" src="https://ejkiwi.github.io/lib/media/numpy01.png" referrerpolicy="no-referrer"><br><br>arr = np.array([[1, 2, 3], [4, 5, 6]])

print(arr.shape)  # 배열 형태 (2, 3)
print(arr.ndim)   # 차원 개수 (2D 배열이면 2)
print(arr.size)   # 전체 요소 개수 (6)
print(arr.dtype)  # 데이터 타입 (int32, float64 등)
<br><img alt="배열 속성 확인" src="https://ejkiwi.github.io/lib/media/numpy02.png" referrerpolicy="no-referrer"><br><br>arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# 특정 요소 접근
print(arr[0, 1])  # 0번째 행, 1번째 열 (2)

# 행, 열 슬라이싱
print(arr[1, :])  # 1번째 행 전체 [4, 5, 6]
print(arr[:, 2])  # 모든 행의 2번째 열 [3, 6, 9]

# 범위 슬라이싱
print(arr[0:2, 1:3])  # 첫 두 행의 1~2번째 열 [[2, 3], [5, 6]]
<br><img alt="배열 인덱싱과 슬라이싱" src="https://ejkiwi.github.io/lib/media/numpy03.png" referrerpolicy="no-referrer"><br><br>x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

# 요소별 연산
print(x + y)  # [5, 7, 9]
print(x * y)  # [4, 10, 18]
print(x ** 2)  # [1, 4, 9]

# 스칼라와 연산
print(x + 10)  # [11, 12, 13]
<br><img alt="배열 연산" src="https://ejkiwi.github.io/lib/media/numpy04.png" referrerpolicy="no-referrer"><br><br>arr = np.array([3, 1, 4, 2])

# 합, 평균, 표준편차, 최댓값, 최솟값
print(np.sum(arr))  # 10
print(np.mean(arr))  # 2.5
print(np.std(arr))  # 표준편차
print(np.max(arr))  # 4
print(np.min(arr))  # 1

# 오름차순 정렬
print(np.sort(arr)) # [1, 2, 3, 4]
# 내림차순 정렬
print(np.sort(arr)[::-1]) # [4, 3, 2, 1]
<br><img alt="그 외 유용한 함수" src="https://ejkiwi.github.io/lib/media/numpy05.png" referrerpolicy="no-referrer"><br><br>arr = np.array([[1, 2], [3, 4]])

# 배열 형태 변경
reshaped = arr.reshape(4, 1)
print(reshaped)
# [[1]
#  [2]
#  [3]
#  [4]]

# 전치 (Transpose)
print(arr.T)
# [[1 3]
#  [2 4]]

# 1차원으로 펼치기
flattened = arr.flatten()
print(flattened)  # [1, 2, 3, 4]
<br><img alt="배열 변환" src="https://ejkiwi.github.io/lib/media/numpy06.png" referrerpolicy="no-referrer"><br><br>Python에서의 데이터 시각화를 위한 라이브러리<br><br>
<br>다양한 시각화 제공 : 간단한 선 그래프, 막대 그래프, 히스토그램부터 복잡한 3D 그래프까지 다양한 시각화를 제공함.
<br>보통 matplotlib.pyplot 모듈을 통해 그래프를 그림.
<br>보통 import matplotlib.pyplot as plt plt라는 이름으로 불러옴.
<br><br># 데이터  
x = [0, 1, 2, 3, 4, 5]  
y = [0, 2, 4, 6, 8, 10]  
  
# 선 그래프 시각화  
plt.plot(x, y, label="y = 2x", color="blue", linestyle="--", marker="o")  
plt.title("Line Plot Example")  # 그래프 제목  
plt.xlabel("X-axis")  # x축 이름  
plt.ylabel("Y-axis")  # y축 이름  
plt.legend()  # 범례 추가  
plt.show()  # 시각화
<br><img alt="선 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib01.png" referrerpolicy="no-referrer"><br><br># 데이터  
x = [1, 2, 3, 4, 5]  
y = [5, 7, 6, 8, 7]  
sizes = [100, 200, 300, 400, 500]  
  
# 산점도 시각화  
plt.scatter(x, y, s=sizes, color="green", alpha=0.6)  
plt.title("Scatter Plot Example")  # 제목  
plt.xlabel("X-axis")  # x축 이름  
plt.ylabel("Y-axis")  # y축 이름  
plt.show()  # 시각화
<br><img alt="산점도 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib02.png" referrerpolicy="no-referrer"><br><br># 데이터  
categories = ['A', 'B', 'C', 'D']  
values = [3, 7, 8, 5]  
  
# 막대 그래프 시각화  
plt.bar(categories, values, color="orange")  
plt.title("Bar Plot Example")  # 제목  
plt.xlabel("Categories")  # x축 이름  
plt.ylabel("Values")  # y축 이름  
plt.show()  # 시각화
<br><img alt="막대 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib03.png" referrerpolicy="no-referrer"><br><br># 데이터  
data = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 6, 7, 8, 9, 10]  
  
# 히스토그램 시각화  
plt.hist(data, bins=5, color="purple", alpha=0.7)  
plt.title("Histogram Example")  # 제목  
plt.xlabel("Value Ranges")  # x축 이름  
plt.ylabel("Frequency")  # y축 이름  
plt.show()  # 시각화
<br><img alt="히스토그램" src="https://ejkiwi.github.io/lib/media/matplotlib04.png" referrerpolicy="no-referrer"><br><br># 데이터  
data = [  
    [2, 3, 5, 6, 8],  # Group 1  
    [1, 4, 4, 5, 9],  # Group 2  
    [3, 5, 7, 7, 10]  # Group 3  
]  
  
# 박스 플롯 시각화  
plt.boxplot(data, tick_labels=['Group 1', 'Group 2', 'Group 3'])  ## tick_labels -&gt; 이전에는 labels로 쓰임  
plt.title("Box Plot Example")  # 제목  
plt.ylabel("Values")  # y축 이름  
plt.show()
<br><img alt="박스 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib05.png" referrerpolicy="no-referrer"><br><br># 데이터  
labels = ['Apples', 'Bananas', 'Cherries', 'Mangoes']  
sizes = [35, 25, 20, 20]  
  
# 파이 차트 시각화  
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=['red', 'yellow', 'pink', 'brown'])  
plt.title("Pie Chart Example")  # 제목  
plt.show()  # 시각화
<br><img alt="원형 차트" src="https://ejkiwi.github.io/lib/media/matplotlib06.png" referrerpolicy="no-referrer"><br><br># 데이터  
x = [1, 2, 3, 4, 5]  
y1 = [1, 2, 4, 8, 16]  
y2 = [1, 1, 2, 3, 5]  
  
# 면적 그래프 시각화  
plt.fill_between(x, y1, color="skyblue", alpha=0.5, label="Area 1")  
plt.fill_between(x, y2, color="lightgreen", alpha=0.7, label="Area 2")  
plt.title("Area Plot Example")  # 제목  
plt.xlabel("X-axis")  # x축 이름  
plt.ylabel("Y-axis")  # y축 이름  
plt.legend()  # 범례 추가  
plt.show()  # 시각화
<br><img alt="면적 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib07.png" referrerpolicy="no-referrer"><br><br># 데이터  
x = [1, 2, 3, 4, 5]  
y1 = [2, 4, 6, 8, 10]  
y2 = [1, 3, 5, 7, 9]  
  
# 다중 그래프 시각화  
plt.subplot(1, 2, 1)  # ( 1행 2열 )plt.plot(x, y1, color="blue", marker="o")  # 첫 번째 그래프  
plt.title("Graph 1")  # 첫 그래프 이름  
  
plt.subplot(1, 2, 2)  
plt.plot(x, y2, color="red", linestyle="--")  # 두 번째 그래프  
plt.title("Graph 2")  # 두 번째 그래프 이름  
  
plt.tight_layout()  # 간격 조정  
plt.show()  # 시각화
<br><img alt="다중 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib08.png" referrerpolicy="no-referrer"><br><br># 데이터  
categories = ['A', 'B', 'C']  
group1 = [3, 5, 7]  
group2 = [4, 6, 8]  
  
# 스택형 막대 그래프 시각화  
plt.bar(categories, group1, label="Group 1", color="lightblue")  
plt.bar(categories, group2, bottom=group1, label="Group 2", color="orange")  # 스택 쌓기  
plt.title("Stacked Bar Plot Example")  # 제목  
plt.xlabel("Categories")  # x축 이름  
plt.ylabel("Values")  # y축 이름  
plt.legend()  # 범례 추가  
plt.show()  # 시각화
<br><img alt="스택형 막대 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib09.png" referrerpolicy="no-referrer">]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250126-모각코-활동-3회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250126 모각코 활동 3회차.md</guid><pubDate>Sun, 26 Jan 2025 08:15:20 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/numpy01.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/numpy01.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20250117 모각코 활동 2회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 자료구조 개념과 종류(리스트, 튜플, 집합, 딕셔너리) 공부 및 구현<br><br> 여러가지 자료를 저장할 수 있는 데이터 구조체 &nbsp;[ , , ]<br>
<br>요소(value) : 리스트 내 데이터를 의미
<br>인덱스(index) : 리스트 내 데이터의 주소
<br>순서가 있는 데이터 구조 
<br>수정 가능 (mutable).
<br>중복된 값을 허용.
<br>my_list = [1, 2, 3, 4]  # 리스트 생성
my_list.append(5)       # 값 추가
my_list[0] = 10         # 값 수정
print(my_list)  # [10, 2, 3, 4, 5]
<br><br>수정이 불가능한, 여러가지 자료를 저장할 수 있는 데이터 구조체 ( , , )<br>
<br>순서가 있는 데이터 구조.
<br>수정 불가능 (immutable).
<br>중복된 값을 허용.
<br>반복문에서 리스트보다 빠름
<br>딕셔너리의 키가  될 수 있음
<br>리스트와 튜플 서로 변환 가능
<br>*&lt;주의&gt; 요소가 1개인 튜플은 요소의 끝에 반드시 쉼표(,) 추가
<br>my_tuple = (1, 2, 3, 4)  # 튜플 생성
# my_tuple[0] = 10  # 수정 불가능 (오류 발생)
print(my_tuple[1])  # 2
<br><br>고유한 값들을 순서 없이 저장할 수 있는 데이터 구조체 { , , }<br>
<br>순서가 없는 데이터 구조 ( = 인덱스 접근 불가 )
<br>중복된 값을 허용하지 않음.
<br>수학의 집합 연산 가능 (합집합, 교집합 등).
<br>리스트와 집합&nbsp;서로 변환 가능
<br>my_set = {1, 2, 3, 3} # 중복된 값은 자동 제거
my_set.add(4) # 값 추가
print(my_set) # {1, 2, 3, 4}
<br><br>키key를 기반으로 값value을 저장할 수 있는 데이터 구조체 { key:value, key:value, ..., key:value }<br>
<br>키-값 (key-value) 쌍으로 데이터를 저장.
<br>키는 중복될 수 없으나, 값은 중복 가능.
<br>순서는 유지
<br>key는 변수로 지정 가능
<br>my_dict = {"name": "Alice", "age": 25}  # 딕셔너리 생성
my_dict["age"] = 26  # 값 수정
my_dict["city"] = "Seoul"  # 새 키-값 추가
print(my_dict)  # {'name': 'Alice', 'age': 26, 'city': 'Seoul'}
]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250117-모각코-활동-2회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250117 모각코 활동 2회차.md</guid><pubDate>Fri, 17 Jan 2025 15:14:09 GMT</pubDate></item><item><title><![CDATA[2024 cnu 2차 학습동아리]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://colab.research.google.com/drive/1n9Pjthb3W4q-MO891Ce_-y5Jh2uOw4uv?usp=sharing" rel="noopener nofollow" class="external-link" href="https://colab.research.google.com/drive/1n9Pjthb3W4q-MO891Ce_-y5Jh2uOw4uv?usp=sharing" target="_blank">2024 cnu 2차 학습동아리 실습코드</a>]]></description><link>https://ejkiwi.github.io/2024-cnu-2차-학습동아리/2024-cnu-2차-학습동아리.html</link><guid isPermaLink="false">2024 cnu 2차 학습동아리/2024 cnu 2차 학습동아리.md</guid><pubDate>Thu, 02 Jan 2025 09:04:05 GMT</pubDate></item><item><title><![CDATA[2024_여름_모각코]]></title><description><![CDATA[ 
 <br>
<br><a data-href="모.구.모.구 모각코 활동" href="https://ejkiwi.github.io/2024_여름_모각코/모.구.모.구-모각코-활동.html" class="internal-link" target="_self" rel="noopener nofollow">모.구.모.구 모각코 활동</a>
<br><a data-href="20240707 모각코 활동 1회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240707-모각코-활동-1회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240707 모각코 활동 1회차</a>
<br><a data-href="20240709 모각코 활동 2회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240709-모각코-활동-2회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240709 모각코 활동 2회차</a>
<br><a data-href="20240716 모각코 활동 3회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240716-모각코-활동-3회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240716 모각코 활동 3회차</a>
<br><a data-href="20240723 모각코 활동 4회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240723-모각코-활동-4회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240723 모각코 활동 4회차</a>
<br><a data-href="20240730 모각코 활동 5회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240730-모각코-활동-5회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240730 모각코 활동 5회차</a>
<br><a data-href="20240806 모각코 활동 6회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240806-모각코-활동-6회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240806 모각코 활동 6회차</a>
<br><a data-href="20240813 모각코 활동 7회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240813-모각코-활동-7회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240813 모각코 활동 7회차</a>
]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/2024_여름_모각코.html</link><guid isPermaLink="false">2024_여름_모각코/2024_여름_모각코.md</guid><pubDate>Sun, 01 Dec 2024 08:02:21 GMT</pubDate></item><item><title><![CDATA[모.구.모.구 모각코 활동]]></title><description><![CDATA[ 
 <br>팀 모각코 목표 : 1. 절대 포기하지 않기, 2. 모르는 거 그냥 넘어가지 않기<br>나의 모각코 활동 다짐 : 활동 계획을 완벽히 마무리 할 수 있도록 노력하겠습니다!<br>나의 모각코 활동 계획<br>
<br>7월 7일

<br>모각코 활동 동안 공부할 주제 전체적으로 톺아보기, 팀원들과 친해지기


<br>7월 9일, 7월 16일

<br>파이토치 사용 익히기
<br>선배님 프로젝트의 레포지토리에 있는 코드 분석해보며 공부하기


<br>7월 23일, 7월 30일, 8월 6일, 8월 13일

<br>CNN 구조 공부하기
<br>RESNET 구조 공부하기


<br>모각코 팀블로그<br>
<a data-tooltip-position="top" aria-label="https://jolly-exoplanet-ef1.notion.site/1868305015324f9f84670142f4029fb7" rel="noopener nofollow" class="external-link" href="https://jolly-exoplanet-ef1.notion.site/1868305015324f9f84670142f4029fb7" target="_blank">모.구.모.구_팀블로그</a>]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/모.구.모.구-모각코-활동.html</link><guid isPermaLink="false">2024_여름_모각코/모.구.모.구 모각코 활동.md</guid><pubDate>Tue, 26 Nov 2024 17:34:04 GMT</pubDate></item><item><title><![CDATA[20240707 모각코 활동 1회차]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.모각코 활동 동안 공부할 주제 전체적으로 톺아보기<br>
2.팀원들과 친해지기<br>파이토치<br>
<br>python을 바탕으로 제작된, 딥러닝과  인공지능 분야에서 주로 활용되는 라이브러리
<br>pytorch의 연산은 tensor를 기본으로 하여 작동
<br>tensor : 파이토치의 기본 데이터 타입. 배열이나 행렬과 유사한 구조(다차원 배열)이다.
<br>파이토치를 기반으로 구성된 모델은 학습을 위한 그래디언트를 자동으로 계산한다. -&gt; 자동 미분
<br>그래디언트 : 벡터 미분의 결과 (=함수의 기울기, 각 변수에 대한 변화율)
<br>선배님의 프로젝트<br>
<br><a data-tooltip-position="top" aria-label="https://github.com/b-re-w/2024-1_BPL_STalk_Model_Research" rel="noopener nofollow" class="external-link" href="https://github.com/b-re-w/2024-1_BPL_STalk_Model_Research" target="_blank">2024-1_BPL_STalk_Model_Research</a>
<br>CNN 모델<br>
<br>2차원 데이터 (이미지 등)의 패턴을 인식하고 분석하는 데 사용되는 딥러닝 모델.
<br>여러개의 층으로 구성됨.
<br>인간의 시신경 구조를 모방한 구조임
<br>이미지의 
<br>RESNET 모델<br>
<br>CNN 모델에 잔차 연결 개념을 도입한 것.
<br>잔차 연결 : 각 층의 출력을 다음 층으로 직접 보내는 대신에, 입력을 더한 뒤 다음 층으로 전달하는 연결.
]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240707-모각코-활동-1회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240707 모각코 활동 1회차.md</guid><pubDate>Mon, 15 Jul 2024 07:48:10 GMT</pubDate></item><item><title><![CDATA[20240709 모각코 활동 2회차]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.파이토치 공부하기 - youtube에 있는 파이토치 설명 강좌(<a rel="noopener nofollow" class="external-link" href="https://youtube.com/playlist?list=PLS8gIc2q83Oit-utRso2iblvt00fZOw85&amp;si=i0CZi4e5g_dVJ3dx" target="_blank">https://youtube.com/playlist?list=PLS8gIc2q83Oit-utRso2iblvt00fZOw85&amp;si=i0CZi4e5g_dVJ3dx</a>) 1,2,3강 들으며 공부<br>
2.선배님의 프로젝트 코드 절반 분석하기 - whisper 부분<br>파이토치<br>
al분야에서 google tensorflow와 함께 딥러닝 모델을 구축하고 학습하는 데 가장 많이 사용되고 있는 오픈 소스 기반의 딥러닝 프레임워크임.<br>
<br>오픈소스 : 개방형 협업을 장려하는 소프트웨어 개발 모델
<br>프레임워크 : 소프트웨어 개발에 있어 하나의 뼈대와 같은 역할을 하는 것으로, 목적에 필요한 것을 고민할 필요 없이 이용할 수 있도록 일괄로 가져다 쓰도록 만들어 놓은 구조화된 틀임.<br>
텐서 : 파이토치의 기본 데이터 타입
<br>배열이나 행렬과 유사한 자료 구조이다
<br>일반적으로는 1차원 - 벡터 , 2차원 - 행렬, 3차원 이상 - 벡터 이지만, 파이토치에서는 입력과 출력 그리고 학습에 필요한모든 데이터들을 모두 텐서 데이터타입으로 정의하고 있다.
<br>텐서의 속성으로는 모양,자료형,저장되는 위치가 있다
<br>보통 저장되는 위치는 cpu인데, gpu를 사용할 수 있다면, .to("cuda")를 사용해서 텐서를 gpu로 이동시킬 수 있다.

<br>gpu : 컴퓨터 그래픽을 처리하는 장치로 그래픽 카드를 구성하는 가장 중요한 핵심 요소.


<br>1.파이썬의 리스트 데이터로부터 직접 텐서를 만들 수 있다.<br>
- listdata = [[10,20],[30,40]] 	tensor1 = torch.Tensor(listdata)`
<br>2.파이썬의 넘파이 데이터로부터 직접 텐서를 만들 수도 있다.(넘파이로만들어진건 보통 int로 생성되기때문에 원래 데이터가 float의 형태인 경우, 캐스팅해주는 작업이 필요하기도 하다.)
<br>3.파이썬의 랜덤 데이터로부터 직접 텐서를 만들 수도 있다.<br>
- tensor3 = torch.rand(2,2) -&gt; rand()메서드는 0~1사이의 균일 분포 랜덤값을 생성함 ( randn()메서드는 정규분포를 가지는 랜덤값을 생성 )
<br>텐서를 넘파이로 바꿀 수도 있다.<br>
- tensor.numpy()
<br>인덱싱과 슬라이싱이 가능하다
<br>elment-wise product 연산 
<br>matrix multiplication 연산 (행렬곱)
<br>텐서를 합칠 수 있다. Tensor Concatenate (dim=0 세로, dim=1 가로)<br>
파이토치 딥러닝 모델 구조 :<br>
1.데이터정의<br>
- 기본 데이터타입인 TENSOR로 생성해야함.<br>
- TensorDataset(x_train,y_train) : 텐서 데이터셋 생성<br>
- DataLoader(dataset, batch_size, shuffle) : 미니 배치 학습과 데이터 셔플, 멀티 프로세싱 등을 간단하게 수행할 수 있음.<br>
- 미니 배치 학습 : 전체 데이터를 n등분 하여 각각의 학습 데이터를 배치 방식으로 학습시키는 것.<br>
- 데이터 셔플 : train데이터와test데이터 간의 동일한 분포를 가지도록 섞어는 것.<br>
- 멀티 프로세싱 : 여러 작업을 별도의 프로세스를 생성 후 병렬처리를 하는 과정을 거치기 때문에 더 빠르게 결과를 얻을 수 있다.<br>
2.모델구축<br>
- nn.Module을 상속받는 class를 생성하여 정의하는 것이 일반적이다.<br>
- 클래스 속 __init__함수에서 계층(신경망 모델을 구성하는)을 정의.<br>
- 클래스 속 forward 함수에서 신경망에 데이터 전달하기를 수하고, 결과값을 리턴함<br>
3.피드포워드<br>
- 모델 학습을 위해서는 피드 포워드 계산값과 정답의 차이 계산이 필요  -&gt; 이 계산을 위해서는 손실함수와 옵티마이저가 필요함.<br>
- 손실함수 : MSE 등<br>
- 옵티마이저 : SDG, ADAM<br>
4.손실함수계산<br>
- nn.MSELoss(model(x_train),y_train) : 피드포워드 계산 값과 정답과의 오차 계산.<br>
- 이 때, model에 데이터를 전달하면 model 클래스 안에 있는 forward()함수자동으로 forward()함수를 호출하기 때문에 우리가 따로 호출해줄 필요가 없다.<br>
5.모델학습<br>
-역전파 코드 : 학습이 진행됨에 따라서 모델 파라미터(가중치와 바이어스)를 업데이트하면서 최적화 시킨다<br>
- optimizer.zero.grad()<br>
- loss.backward()<br>
- optimizer.step()<br>
- 모델(model) : 각 층을 포함하고 있는 인공신경망 그 자체 (이를 레고처럼 순차적으로 쌓기 -&gt; CNN, RNN 등 다양한 모델 구축 가능)<br>
- 3&gt;4&gt;5의 반복 -&gt; 딥러닝 학습<br>
- 손실함수가 최소가 될 때까지 모델 파라미터(가중치, 바이어스) 값을 찾아감.
<br>선배님 프로젝트 분석 - whisper<br>
1.from faster_whisper import WhisperModel<br>
2.def get_whisper() :  	 3.   model_size = "medium"  #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3'] 	 4.   compute_type = "int8"  #@param ['float16', 'int8']<br>
5.   return WhisperModel(model_size, device=DEVICE, cpu_threads=12, compute_type=compute_type).transcribe<br>1: faster_whisper 에서 WhisperModel 모듈 불러오기<br>
2: get_whisper 라는 이름의 함수 설정하기<br>
3: model_size는 "medium"이다. model_size가 가질 수 있는 옵션으로는 "tiny","base","small","medium","large","large-v3" 이 있다. -&gt; model_size는 모델의 크기를 뜻한다.<br>
4: compute_type은 "int8"이다. compute_type이 가질 수 있는 옵션으로는 "float16","int8"이 있다. -&gt; compute_type은 계산 유형을 뜻한다.<br>
5: WhisperModel은 4가지의 매개변수를 사용하는데, 여기에서 model_size는 앞서 정한 크기와 같고, device는 모델이 실행될 장치를 지정한다. cpu_threads는 CPU의 스레드 수를 뜻한다. compute_type또한 앞서 정한 계산 유형과 같다. 이 때 .transcribe는 모델의 음성 인식 기능을 호출해서 음성을 텍스트로 변환해준다.]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240709-모각코-활동-2회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240709 모각코 활동 2회차.md</guid><pubDate>Mon, 15 Jul 2024 06:27:57 GMT</pubDate></item><item><title><![CDATA[20240716 모각코 활동 3회차]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.파이토치 공부하기  - 실습해보기<br>
2.선배님의 프로젝트 코드 절반 분석하기 - resnet 부분<br>파이토치 실습<br>import torch #파이토치 불러오기
from torch import nn #토치에서 nn 불러오기
  

#텐서 형태로 train데이터 가져오기
x_train = torch.Tensor([1,2,3,4,5,6]).view(6,1)
y_train = torch.Tensor([3,6,9,12,15,18]).view(6,1)

  
#MyNeuralNetwork 클래스 만들기. nn.Module이 부모클래스가 됨.
class MyNeuralNetwork(nn.Module):
&nbsp; def __init__(self):
&nbsp; &nbsp; super().__init__()
&nbsp; &nbsp; self.linear_relu_stack = nn.Sequential(nn.Linear(1,1))

&nbsp; def forward(self, x):
&nbsp; &nbsp; logits = self.linear_relu_stack(x)
&nbsp; &nbsp; return logits


#모델
model = MyNeuralNetwork()
#손실함수
loss_function = nn.MSELoss()
#옵티마이저
optimizer = torch.optim.SGD(model.parameters(),lr=1e-2)

nums_epoch = 2000


#학습시키기
for epoch in range(nums_epoch + 1):
&nbsp; prediction = model(x_train)
&nbsp; loss = loss_function(prediction, y_train)

&nbsp; optimizer.zero_grad()
&nbsp; loss.backward()
&nbsp; optimizer.step()

&nbsp; if epoch % 100 == 0:
&nbsp; &nbsp; print('epoch = ', epoch, 'current loss = ', loss.item())
<br>#예측하기
x_test = torch.Tensor([8,9,10,11]).view(4,1)
pred = model(x_test)
pred
<br>선배님의 프로젝트 코드<br>from huggingface_hub import hf_hub_download
import wespeaker
<br>from huggingface_hub import hf_hub_download<br>
huggingface_hub 라이브러리를 통해서 hf_hub_download함수를 가져와준다.<br>
hf_hub_download함수를 통해서 모델을 다운로드 할 수 있다.<br>
기본적으로, 함수에는 repo_id와 repo_type을 인자로 넘겨준다. (revision - 특정 버전의 파일을 다운로드 하고 싶을 시. / local_dir 특정 위치에 저장하고 싶을 시.)<br>
import wespeaker<br>
wespeaker을 가져와준다.<br> def get_resnet152():
    model_id = "Wespeaker/wespeaker-voxceleb-resnet152-LM"
    model_name = model_id.replace("Wespeaker/wespeaker-", "").replace("-", "_")
 
    root_dir = hf_hub_download(model_id, filename=model_name+".onnx").replace(model_name+".onnx", "")

    import os
    if not os.path.isfile(root_dir+"avg_model.pt"):
        os.rename(hf_hub_download(model_id, filename=model_name+".pt"), root_dir+"avg_model.pt")
    if not os.path.isfile(root_dir+"config.yaml"):
        os.rename(hf_hub_download(model_id, filename=model_name+".yaml"), root_dir+"config.yaml")

    resnet = wespeaker.load_model_local(root_dir)

    #print("Compile model for the NPU")
    #resnet.model = intel_npu_acceleration_library.compile(resnet.model)

    def resnet152(ado, sample_rate=None):
        if isinstance(ado, str):
            return resnet.recognize(ado)
        else:
            return recognize(resnet, ado, sample_rate)

    resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)

    return resnet152
<br>분석<br>
def get_resnet152():<br>
get_resnet 152 라는 이름의 함수를 정의<br>model_id = "Wespeaker/wespeaker-voxceleb-resnet152-LM"<br>
model_id라는 변수에 "Wespeaker/wespeaker-voxceleb-resnet152-LM"를 지정. 아마  모델 아이디에 모델의 이름을 저장한 것일 것.<br>moldelname = model.id.replace("Wespeaker/wespeaker-",").replace("-", " ")<br>
model_name이라는 변수를 만들어서, model_id를 약간 변형시킨 이름으로 지정해줌. "voxceleb_resnet152_LM"이 될 것.<br>root_dir = hf_hub_download(model_id, filename = model_name+" .onnx").replace(model_name+" .onnx", "")<br>
hf_hub_download : huggingface_hub 라이브러리를 통해서 가져왔던 함수. 함수를 사용해서 모델 파일을 다운로드하고, 다운로드한 파일을 root_dir에 저장함.<br>import os<br>
os 모듈을 가져옴<br>
os 모듈 : 파일 및 디렉토리 작업, 프로세스 및 스레드 관리, 시스템 정보와 관련한 작업들을 수행할 수 있는 모듈이다.<br>if not os.path.isfile(root_dir+"avg_model.pt"):<br>
os.rename(hf_hub_download(model_id, filename=model_name+".pt"), root_dir+"avg_model.pt")<br>
만약 avg_model.pt이름을 가진 파일이 없다면, 모델의 pt파일을 다운로드 한 뒤 이름을 avg_model.pt로 바꾸어서 root_dir 변수에 저장함.<br>
os.path.isfile(path) : path가 파일인 경우 true를 리턴, 아니면 false를 리턴.<br>
os.rename : 파일 또는 폴더의 이름을 간단히 변경할 수 있다.<br>if not os.path.isfile(root_dir+"config.yaml"):<br>
os.rename(hf_hub_download(model_id, filename=model_name+".yaml"), root_dir+"config.yaml")<br>
앞 코드와 같은 느낌인데, 만약 config.yaml파일이 없으면 모델의 yaml파일을 다운로드 한 뒤 이름을 바꾸어서 root_dir변수에 저장함.<br>resnet = wespeaker.load_model_local(root_dir)<br>
resnet이라는 변수를 지정해줄건데, wespeaker 라이브러리의 load_model_local 함수를 사용할거임. 이 때 root_dir에 있는 파일들을 불러오게 됨.<br>def resnet152(ado, sample_rate=None):<br>
if isinstance(ado, str):<br>
return resnet.recognize(ado)<br>
else:<br>
return recognize(resnet, ado, sample_rate) 	 resnet152라는 함수를 정의해주는데, 이 함수는 입력으로 ado를 받음.<br>
instance(객체, 타입) : isinstance함수는 지정된 객체(여기에서는 ado)가 지정된 타입이면 true를 반환하고 아니면 false를 반환한다.<br>
ado가 문자열이라면  resnet.recognize(ado)를 리턴하고<br>
그렇지 않다면  recognize(resnet, ado, sample_rate)을 리턴함.<br>(recognize함수는 이전에 지정해둔 함수이다.)<br>def recognize(model, pcm, sample_rate):
    q = extract_embedding(model, pcm, sample_rate)
    best_score = 0.0
    best_name = ''
    for name, e in model.table.items():
        score = model.cosine_similarity(q, e)
        if best_score &lt; score:
            best_score = score
            best_name = name
        del score
        gc.collect()
    return {'name': best_name, 'confidence': best_score}
<br>resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)<br>
resnet152라는 함수에 register라는 기능을 추가(대체?)<br>
lambda함수를 통해서 resnet152에서 register메서드를 사용하려고 할 때, resnet객체의 register 메서드를 가져와서 사용하게 된다.<br>
args, kwargs : 몇 개의 인자를 받아야 할지 정할 수 없을 때 args와 kwargs(keyword arguments)를 파라미터로 써줌. args 앞에 붙는  * 는 여러개의 인자를 묶어서 하나의 튜플로 묶어주고 이를 args에 할당해준다. kwargs 앞에 붙는 ** 는 여러개의 키워드 아규먼트들을 묶어서 딕셔너리로 만들어준다. <br>return resnet152<br>
get_resnet152라는 함수는 resnet152를 반환함.<br>resnet152 = get_resnet152()
print("INFO: ResNet152 Ready -", resnet152)
<br>분석<br>
resnet152 = get_resnet152()<br>
get_resnet152함수를 가져와서 resnet152함수에 저장함<br>
print("INFO: ResNet152 Ready -", resnet152)<br>
모델이 준비되었다는 메시지를 출력한 뒤, resnet152를 출력함.]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240716-모각코-활동-3회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240716 모각코 활동 3회차.md</guid><pubDate>Tue, 30 Jul 2024 04:01:59 GMT</pubDate></item><item><title><![CDATA[20240723 모각코 활동 4회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
1.cnn공부 - 정의와 구조 살펴보기<br>
2.resnet공부 - 정의와 구조 살펴보기<br>딥 러닝 : 심층 신경망을 주로 다루는 ai분야. 심층 신경망은 신경망을 여러 계층으로 구성한 것.<br>
기존 신경망의 큰 단점 : 입력 데이터의 구조 고려 안 함. -&gt; 이미지와 같은 공간적 구조를 가지는 데이터 다루기 적합하지 않음.<br>
기존 신경망에서의 단점(공간적 구조 데이터 다루기 어려움)을 극복하기 위해 cnn 등장<br>CNN<br>
합성곱 신경망<br>
-2차원 구조를 고려하는 신경망<br>
-가중치와 바이어스로 이루어진 뉴런으로 구성<br>
- 입력데이터를 받고, 처리한 후 특정한 결과를 출력함.<br>
-입력 계층에 들어온 미가공 이미지 데이터에 해당하는 클래스를 예측하는 것이 목적.<br>
-예측된 클래스는 출력 계층의 결과 값 형태(클래스 점수 변환됨)로 출력됨.<br>
-계층의 종류<br>
1. 입력층<br>
- 미가공 이미지 데이터를 받음.<br>
2. 합성곱층<br>
- 합성곱 연산을 수행함.<br>
- 커널(n  m의 행렬)로 이미지(높이  너비)를 처음부터 끝까지 겹쳐 훑는다. 겹쳐지는 부분의 각 이미지와 원소의 값을 곱해서 모두 더한 값을 출력함.<br>
- 스트라이드 : 커널이 입력을 훑는데, 이 때의 보폭을 뜻함.<br>
- 이 때 출력되는 것(입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과)은 '출력 특성 맵(output feature map)' 이라 함.<br>
- CNN에서는 합성곱 계층의 입출력 데이터를 특성 맵(feature map) 이라 함.<br>
- <img alt="cnn 연산 방법" src="https://ejkiwi.github.io/lib/media/cnn1.png" referrerpolicy="no-referrer"><br>
3.ReLU층<br>
- 인공신경망에서 사용되는 활성화함수 f(x) = max(0, x) -&gt; 입력값이 0보다 크면 그 값을 그대로 출력하고, 0 이하면 0을 출력.<br>
4. 풀링층<br>
- 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄임.<br>
- 합성곱 연산과 유사함 (커널과 스트라이드 개념이 존재)<br>
- 최대풀링 : 커널과 겹치는 영역 안에서 최대값을 추출<br>
- 평균풀링 : 커널과 겹치는 영역 안에서 평균값을 추출<br>RESNET<br>
CNN의 한 종류<br>
-신경망의 깊이가 깊어짐에 따라 발생하는 훈련 문제를 해결하기 위해 '잔여학습'이라는 개념을 도입함.<br>
- 훈련 문제 : 기존 모델들은 레이어를 깊게 쌓을수록 더 성능이 좋아질 것이라고 예상했지만 실제로는 20층 이상부터 성능이 낮아지는 현상이 발생.<br>
- 잔여학습 : 스킵연결(입력값이 일정층들을 건너뛰어서 출력에 더할 수 있게 하는 역할) -&gt; 기존신경망은 k번째 층과 (i+1)번째 층의 연결로 이루어져있는데, resnet은 (i+r)층의 연결을 허용(shortcut connection).<br>
- <img alt="ResNet 구조" src="https://ejkiwi.github.io/lib/media/resnet.png" referrerpolicy="no-referrer"><br>
-최대 152개 층까지 쌓을 수 있게 됨.<br>
<img alt="cnn2" src="https://ejkiwi.github.io/lib/media/cnn2.png" referrerpolicy="no-referrer">]]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240723-모각코-활동-4회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240723 모각코 활동 4회차.md</guid><pubDate>Tue, 23 Jul 2024 13:38:23 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/cnn1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/cnn1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20240730 모각코 활동 5회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
1.cnn공부 - 합성곱 계층에서의filter, Padding에 대해 더 알아보기.<br>
2.resnet공부 - Residual Block과 Skip-Connection 에 대해 더 깊이 알아보기<br>합성곱 계층에서의 filter<br>
CNN에서 filter는 커널(n * m의 행렬)와 같은 의미이다. (mask라고도 불린다.)<br>
filter를 사용하는 이유는 사진에서 feature(특징)를 뽑아내기 위함이다.<br>
<br>입력 데이터의 전체 이미지에서, filter를 통해 천제 이미지를 순환하며, 특정 filter모양과 일치할수록 더 큰 값을 가지게 될 것인데, 이는 전체 이미지서 특정 filter와 유사한 모양을 가진 부분에 대한 feature들만 얻게 된다는 것을 의미한다. =&gt; 특정 filter에 부합하는 feature정보를 얻는 과정.
<br>Padding<br>
cnn구조에서, 합성곱층을 지나게 되면, 합성곱 연산으로 인해서 Feature Map의 크기는 입력데이터보다 크기가 작아지게 된다. 이렇게 크기가 작아지는것을 피하기 위해서 Padding 이라는 방법을 사용할 수 있다.<br>
<br>zero padding : 입력 데이터(이미지) 주위를 0으로 둘러주는 padding의 방법이다.<br>
<img alt="zero padding" src="https://ejkiwi.github.io/lib/media/zero_Padding.png" referrerpolicy="no-referrer">

<br>P : padding layer의 수
<br>n : 이미지의 크기가 n * n
<br>f : 커널의 크기(filter의 크기)가  f * f
<br>(n+2p) * (n+2p) : 패딩된 이미지의 크기
<br>((n + 2p – f + 1) * (n + 2p – f + 1)) :  합성곱층을 지난 출력 이미지의 크기


<br>padding이 필요한 이유

<br>이미지 데이터의 축소를 막을 수 있다. -  여러번의 계산을 거쳐야 하는데 초반부터 이미지가 너무 작아져버린다면 학습을 별로 하지 못하고 끝나버릴 수 있기 때문에 padding을 통해 이미지의 크기를 조절해줘야한다.
<br>모서리에 있는 중요한 정보를 충분히 활용할 수 있다. - padding을 사용하지 않는 경우, 모서를 학습할 기회가 적어지게 된다. 만약 중요한 정보가 모서리쪽에 있다면, 모델의 성능이 떨어지기 때문에 padding을 사용하여 모서리의 정보들도 충분히 학습할 수 있도록 해주어야 한다.<br>
<img alt="패딩과 모서리~" src="https://ejkiwi.github.io/lib/media/CNN_Padding_Edge.png" referrerpolicy="no-referrer">


<br>Valid Padding과 Same Padding : 각각 순서대로 패딩하지 않는 것, 입력데이터와 출력데이터가 동일하도록 하는 패딩을 뜻한다.
<br>Residual Block 과 Skip-Connection<br>
Residual Block은 층이 깊어지더라도 성능이 뒤떨어지지 않게 하기 위해 제시된 것.<br>
Residual 은 "잔여" 라는 뜻을 가지고 있는데, x를 입력 H(x)를 x의 분포로 가정하면 residual은 최종으로 구하고자 하는 H(x)와 x의 차이로 볼 수 있다.<br>
즉, Residual = R(x) = H(x) - x 가 되며 H(x) = R(x) + x 로 정리가능하다. <br>
<br><img alt="residualblock" src="https://ejkiwi.github.io/lib/media/residual%20block.png" referrerpolicy="no-referrer">
<br>위 신경망층에서는 F(x)r가 R(x)의 역할을 하기 때문에 Residual Block이라 불리게 된다.<br>
Residual Block은 그레디언트 소실 문제를 약화시키고, 이에 따라 신경망의 깊이가 깊어져도 성능이 떨어지지 않게 되는 것.
<br>그레디언트 소실 문제<br>
- 신경망을 학습시는 과정에서 -&gt; 역전파 알고리즘을 통해 출력층에서 입력층으로 손실함수에 대한 그레디언트를 전파하고, 경사 하강법을 통해 이 그레디언트를 사용하여 각 파라미터를 수정하는 단계를 거치게 됨.<br>
-  이 때 신경망의 하위층으로 진행될수록 그레디언트가 점점 작아지게 되는 문제가 그레디언트 소실 문제이다.<br>
residual block에서는 x, x+1, x+2 층이 있다고 할 때, x+2층은 x+1층뿐만 아니라 x로부터도 정보를 받을 수 있게 된다. 따라서 역전파 알고리즘이 실행될 때 그레디언트가 작아지는것을 어느정도 막아주는 효과가 발생한다.<br>
이러한 residual block의 방식을 하나의 합성곱층을 기준으로 살펴보았을 때,<br>
한 층의 입력값을 출력값과 합쳐서 다음 층으로 넘겨주는 방식이 그 층의 입력값이 해당 층을 통과하지 않고 다음 층으로 넘어가는 것과 같기 때문에 Skip Connection이라 부르게 되는 것이다.<br>
즉, Residual Block의 핵심은 Skip Connection이라 할 수 있다.
]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240730-모각코-활동-5회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240730 모각코 활동 5회차.md</guid><pubDate>Wed, 31 Jul 2024 12:40:41 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/zero_Padding.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/zero_Padding.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20240806 모각코 활동 6회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
CNN 실습 - MNIST 이미지 분류 ( RESNET은 7회차에 진행할 예정 )<br>import torch #pytorch 가져오기
<br>데이터 가져오기<br>#데이터셋불러오고 텐서로 바꿔주기

from torchvision import datasets #데이터셋 불러오고

from torchvision.transforms import ToTensor #텐서로 바꿔주기

  

#datasets에서 MNIST 가져와서 훈련데이터와 테스트데이터 가져와주기.

#datasets.MNIST(root - 데이터가 저장될 경로, train - train이 true 이면 train data이고 false면 test data, download - 데이터 없으면 인터넷에서 다운로드해줌 , transform - transform을 ToTensor로 지정해주지 않으면 텐서의 형식이 아닌, PIL이미지로 데이터가 가져와지게 된다)

  

train_data = datasets.MNIST(

&nbsp; &nbsp; root = "data",

&nbsp; &nbsp; train = True, #train data를 다운로드

&nbsp; &nbsp; transform = ToTensor(),

&nbsp; &nbsp; download = True

)

test_data = datasets.MNIST(

&nbsp; &nbsp; root = 'data',

&nbsp; &nbsp; train = False, #test data를 다운로드

&nbsp; &nbsp; transform = ToTensor()

)
<br>데이터 확인하기<br>#학습데이터 확인

print(train_data)

print(train_data.data.size())

# 데이터셋의 이름은 MNIST

# 데이터의 수는 60000개

# 훈련데이터

# StandardTransform(데이터셋에 일관되게 적용되는 변환의 표준을 정의) -&gt; Transform: ToTensor() #이미지 데이터들을 모두 일관되게 텐서 형태로 변환하겠다는 것을 의미.


#테스트데이터 확인

print(test_data)

print(test_data.data.size())

#데이터의 수가 10000 인 것과 테스트데이터라는 것을 제외하면 나머지 속성은 학습데이터와 동일함.
<br>#데이터 시각적으로 확인

import matplotlib.pyplot as plt #시각적 확인을 위해 matplotlib을 사용.

fig, ax = plt.subplots() # fig -&gt; 데이터가 담기는 프레임 / ax -&gt; 실제 데이터가 그려지는 캔버스

ax.imshow(train_data.data[0], cmap='gray') #데이터의 모습



#이미지 위에 각 픽셀 값을 표시해서 나타내보기

for i in range(train_data.data[0].shape[0]): # i와j는 텍스트를 표시할 위치를 지정하기 위함.

&nbsp; for j in range(train_data.data[0].shape[1]):

&nbsp; &nbsp; c = 1 if train_data.data[0][i, j].item() &lt; 125 else 0 # 이미지의 각 픽셀 값( train_data.data[0][i,j].item() )이 125보다 작으면 c = 1 흰색을 사용, 크면 c = 0 검정 사용.

&nbsp; &nbsp; ax.text(j, i, str(train_data.data[0][i, j].item()), color=(c, c, c), ha='center', va='center', fontsize=5) # text()를 사용하여 이미지 위에 텍스트 그리기

  

plt.title("%i" % train_data.targets[0])

plt.show
<br><img alt="mnist_1" src="https://ejkiwi.github.io/lib/media/MNIST.png" referrerpolicy="no-referrer"><br>데이터 준비하기<br>from torch.utils.data import DataLoader
# DataLoader -&gt; &nbsp;데이터를 미니배치 형태로 만들어서 우리가 실제로 학습할 때 이용할 수 있도록 함.
#DataLoader(dataset 데이터 , batch_size=1 한 번의 배치 안에 있는 샘플 사이즈, shuffle=False 데이터셋을 섞어서 데이터가 학습되는 순서를 바꿈, num_workers=0 동시에 처리하는 프로세서의 수. 하나 더 추가하면 20%정도 속도가 빨라짐.)
#배치 학습 -&gt; 전체 데이터를 n등분 하여 학습.

loaders = {

&nbsp; &nbsp; 'train' : torch.utils.data.DataLoader(train_data,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; batch_size=100,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; shuffle=True,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; num_workers=1),

&nbsp; &nbsp; 'test' : torch.utils.data.DataLoader(test_data,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=100,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;shuffle=True,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num_workers=1)

}

loaders
<br>CNN 모델 설정하기<br>class CNN(torch.nn.Module):

  

&nbsp; def __init__(self):

&nbsp; &nbsp; super(CNN, self).__init__()

&nbsp; &nbsp; self.layer1 = torch.nn.Sequential(

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), #컨볼루션 레이어(합성곱층) #1차원(1개채널) 데이터를 받아 16개의 feature(16개의채널)로 나누겟다!!임.

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.ReLU(), #ReLU층

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.MaxPool2d(kernel_size=2, stride=2)) #풀링층

&nbsp; &nbsp; self.layer2 = torch.nn.Sequential(

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.ReLU(),

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.MaxPool2d(kernel_size=2, stride=2))
&nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; # layer 1, layer2 층까지는 이미지를 형상으로 분할하고 분석하는 부분
&nbsp; &nbsp; &nbsp; &nbsp; # 다음 fc 층에서는 이미지를 분류 예측하는 부분.
&nbsp; &nbsp; &nbsp; &nbsp; 

&nbsp; &nbsp; self.fc = torch.nn.Linear(32 * 7 * 7, 10, bias=True) #32*7*7만큼의 입력을 linear레이어에 의해 계산되게 해서... 10개의 출력( MNIST 이미지를 0부터 9까지 분류해야하기때문 )이 나오도록 함.

&nbsp; &nbsp; torch.nn.init.xavier_uniform_(self.fc.weight) # 신경망의 가중치를 초기화 ( 신경망의 가중치를 학습 전에 적절한 값으로 설정하는 과정 )



&nbsp; &nbsp; # __init__에서는 필요한 레이어들을 정의내렸다고 볼 수 있음.
&nbsp; &nbsp; # 아래 forward(얘가 실제적인 모델의 형태가 됨)에서 사용한다.

&nbsp; def forward(self, x): #순전파 #순전파만 지정해주어도 pytorch에서는 역전파 과정을 매우 쉽게 할 수 있도록 해준다.

&nbsp; &nbsp; out = self.layer1(x)

&nbsp; &nbsp; out = self.layer2(out)

&nbsp; &nbsp; out = out.view(out.size(0), -1) #  view() 함수는 텐서의 크기를 변경하는 데 사용 # 데이터를 완전 연결(fc) 층에 전달하기 위해 2차원 또는 3차원 텐서를 1차원 벡터로 평탄화 하는 과정이 필요함.
&nbsp; &nbsp; out = self.fc(out)

&nbsp; &nbsp; return out
<br>model = CNN()

model
<br>CNN(<br>
(layer1): Sequential(<br>
(0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br>
(1): ReLU()<br>
(2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>
)<br>
(layer2): Sequential(<br>
(0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br>
(1): ReLU()<br>
(2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>
)<br>
(fc): Linear(in_features=1568, out_features=10, bias=True)<br>
)<br><br>학습하기<br>learning_rate = 0.01 # 파라미터를 얼마나 업데이트할 것인지를 결정. 학습률, step size. 너무 크지도 작지도 않아야 함.

loss_func = torch.nn.CrossEntropyLoss() # 모델 예측과 실제값 간의 차이를 측정하는 손실함수.

optimizer = &nbsp;torch.optim.Adam(model.parameters(), lr=learning_rate) # 손실함수를 통해 나온 을 최소화하기 위해 가중치를 업데이트하는 방법

training_epochs = 10 # 전체 데이터셋을 몇 번 반복할 것인지 결정.
<br># 반복의 횟수는 epoch과 batch의 크기에 따라 결정

total_batch = len(loaders['train'])

for epoch in range(training_epochs):

&nbsp; avg_cost = 0

&nbsp; for X, Y in loaders['train']:

&nbsp; &nbsp; optimizer.zero_grad() # 학습에서, 역전파를 거칠 때 마다 각 .grad 값에 변화도가 저장이 되는데,  이어지는 다음 학습에서 .grad의 값을 0으로 초기화시켜주지 않으면 이전에 저장된 변화도 값이 다음 학습에 영향을 주기 때문에 원하는 방향으로 학습하기 힘들다. 그래서zero_grad를 통해 .grad 의 값들을 0으로 초기화시켜준다.

&nbsp; &nbsp; pred = model(X) #순전파

&nbsp; &nbsp; cost = loss_func(pred, Y) #손실함수계산

&nbsp; &nbsp; cost.backward() #역전파

&nbsp; &nbsp; optimizer.step() # 역전파 단계에서 수집된 변화도로 매개변수를 조정

  

&nbsp; &nbsp; avg_cost += cost / total_batch

  

&nbsp; print('[Epoch: {:&gt;4}] cost = {:&gt;.9}'.format(epoch + 1, avg_cost))
&nbsp; # `epoch + 1` 값을 최소 4칸의 너비로 오른쪽 정렬하여 출력
&nbsp; # `avg_cost` 값을 최소 9자리까지 나타내어 오른쪽 정렬하여 출력

print('Learning Finished....&gt;_&lt;')
<br>[Epoch: 1] cost = 0.0461711548<br>
[Epoch: 2] cost = 0.0472225286<br>
[Epoch: 3] cost = 0.0413064063<br>
[Epoch: 4] cost = 0.0417594947<br>
[Epoch: 5] cost = 0.0395734794<br>
[Epoch: 6] cost = 0.0441303253<br>
[Epoch: 7] cost = 0.0408433564<br>
[Epoch: 8] cost = 0.043582622<br>
[Epoch: 9] cost = 0.0441764817<br>
[Epoch: 10] cost = 0.0412645154<br>
Learning Finished....&gt;_&lt;]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240806-모각코-활동-6회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240806 모각코 활동 6회차.md</guid><pubDate>Wed, 07 Aug 2024 13:56:13 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/MNIST.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/MNIST.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20240813 모각코 활동 7회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
RESNET 실습 - CIFAR10 이미지 분류<br>
양자화 공부<br>#필요한 모듈 불러오기

import torch
import torch.nn as nn #다양한 종류의 레이어 제공 -&gt; 모델 만들기 도우미!
import torch.nn.functional as F #활성화 함수, 손실함수 등을 함수 형태로 제공.
import torch.backends.cudnn as cudnn
<br>모델링<br>#BasicBlock 클래스 정의  
  
class BasicBlock(nn.Module): # nn.Module 상속받기  
    def __init__(self, in_planes, planes, stride = 1):  
        super(BasicBlock, self).__init__() #BasicBlock의 부모클래스인 nn.Module의 __init__함수를 먼저 호출해서 사용.  
                  
        #conv1과 conv2 설정  
        #2D 컨볼루션 레이어 설정  
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size = 3, stride = stride, padding = 1, bias = False) # in_planes 입력채널 수 / planes 출력채널 수 / kernel_size 3*3 필터(커널) 사용 / stride (커널로 훑을 때의 보폭) 기본값은 1 / padding 패딩의 크기 1 / bias = False 바이어스(출력값을 조절하기 위해 사용되는  값) 를 사용하지 않겠다. -&gt; 바로 다음 줄의 코드(배치정규화)에서 바이어스의 역할을 해주기 때문에 여기에선 사용하지 않는다.  
        #배치 정규화 설정  
        self.bn1 = nn.BatchNorm2d(planes) # planes 배치정규화를 적용할 채널의 수. 앞의 출력 채널의 수와 동일해야함(당연함)  
  
        #2D 컨볼루션 레이어 설정  
        self.conv2 = nn.Conv2d(planes, planes, kernel_size = 3, stride = 1, padding = 1, bias = False)  
        #배치 정규화 설정  
        self.bn2 = nn.BatchNorm2d(planes)  
                    
# shortcut 설정 -&gt; `H(x) = R(x) + x`에서의 x를 위한 작업  
        self.shortcut = nn.Sequential() # nn.Sequential : pytorch에서 여러 레이어들을 순서대로 쌓을 때 사용하는 도구 # x를 그대로 더할 수 있는 경우  
        if stride != 1: #stride의 값이 1인경우(입력과 출력의 채널 수가 다른 경우 = x를 그대로 더할 수 없는 경우)   
self.shortcut = nn.Sequential(  
                nn.Conv2d(in_planes, planes, kernel_size = 1, stride = stride, bias = False),  
                nn.BatchNorm2d(planes)  
            ) # nn.Sequential을 사용해서 Conv2d와 BatchNorm레이어들을 이어줬음  
                  
#순전파 함수 # __init__에서 설정해뒀던 거 실제로 사용하는 부분.  
    def forward(self,x):  
        out = F.relu(self.bn1(self.conv1(x))) #conv1 거치고, relu함수 거치기  
        out = self.bn2(self.conv2(out)) #그다음 conv2 거치기  
        out += self.shortcut(x) # resnet의 핵심인 skip connection : H(x) = R(x) + x
<br>#ResNet 클래스 정의  
class ResNet(nn.Module):  
    def __init__(self, block, num_blocks, num_classes = 10):  
        super(ResNet, self).__init__() #ResNet의 부모클래스인 nn.Module의 __init__함수를 먼저 호출해서 사용.  
        self.in_planes = 64 # 입력 채널 수 64        # 2D 컨볼루션레이어 설정  
        self.conv1 = nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1, bias = False) # 입력채널 수 3 / 출력채널 수 64 / kernel_size 3*3 필터(커널) 사용 / stride (커널로 훑을 때의 보폭) 1 / padding 패딩의 크기 1 / bias = False 바이어스(출력값을 조절하기 위해 사용되는  값) 를 사용하지 않겠다.  
        # 배치정규화 설정  
        self.bn1 = nn.BatchNorm2d(64) # 배치정규화를 위해 사용할 채널 수 = 이전 채널에서의 출력 채널 수 = 64        # 레이어블록 설정(각 블록은 앞서 정의한 BASIC BLOCK으로 구성될거임. 인자 block 자리에, BasicBlock이 들어갈거니까아아아~~)  
        # _make_layer() : (블록의 종류, 출력 채널 수, 쌓을 블럭의 수, 레이어의 첫 블럭에서 사용할 stride의 값)  
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride = 1) #  
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride = 2)  
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride = 2)  
        # self._make_layer()에서 self는 현재 클래스의 인스턴스를 가리킴.  
        # 클래스 예측값 계산  
        self.linear = nn.Linear(512, num_classes) # 입력 채널 수 512, 출력 채널 수 num_classes        # _make_layer 함수 설정  
    def _make_layer(self, block, planes, num_blocks, stride):  
        strides = [stride] + [1] * (num_blocks -1) # stride 값 설정 # 첫 번째 블록의 stride는 지정된 값을 사용하고 이후 블럭들은 stride = 1이 된다.  
        layers = [] # 블럭을 담을 빈 리스트 생성  
        for stride in strides:  
            layers.append(block(self.in_planes, planes, stride)) # 입력 채널 수 self.in_planes, 출력 채널 수 planes, 스트라이드 값 stride            self.in_planes = planes # 채널 수 변경해주기(다음 레이어를 위해)  
        return nn.Sequential(*layers) # 생성한 블록들을 하나의 레이어로 묶어서 반환.  
    # 순전파 함수 # __init__ 설정해뒀던거랑 _make_layer 함수 만든 거 실제로 사용하는 부분.  
    def forward(self, x):  
        out = F.relu(self.bn1(self.conv1(x)))  
        out = self.layer1(out)  
        out = self.layer2(out)  
        out = self.layer3(out)  
        out = self.layer4(out)  
        out = F.avg_pool2d(out, 4) # 풀링층  
        out = out.view(out.size(0),-1) # 텐서의 차원 변경  
        out = self.linear(out) #완전 연결층  
        return out
<br># ResNet 18 함수 정의  
def ResNet18():  
    return ResNet(BasicBlock, [2,2,2,2])
<br>데이터 불러오기<br>import torchvision
import torchvision.transforms as transforms


transform_train = transforms.Compose([
&nbsp; &nbsp; transforms.RandomCrop(32, padding=4),
&nbsp; &nbsp; transforms.RandomHorizontalFlip(),
&nbsp; &nbsp; transforms.ToTensor(),
])


transform_test = transforms.Compose([
&nbsp; &nbsp; transforms.ToTensor(),
])

  
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)


train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)

<br>학습시키기<br>device = 'cuda'
net = ResNet18()
net = net.to(device)
learning_rate = 0.1
file_name = 'resnet18_cifar10.pth'
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)


def train(epoch):
&nbsp; &nbsp; print('\n[ Train epoch: %d ]' % epoch)
&nbsp; &nbsp; net.train()
&nbsp; &nbsp; train_loss = 0
&nbsp; &nbsp; correct = 0
&nbsp; &nbsp; total = 0

&nbsp; &nbsp; for batch_idx, (inputs, targets) in enumerate(train_loader):
&nbsp; &nbsp; &nbsp; &nbsp; inputs, targets = inputs.to(device), targets.to(device)
&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()

&nbsp; &nbsp; &nbsp; &nbsp; outputs = net(inputs)
&nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(outputs, targets)
&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()

&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()
&nbsp; &nbsp; &nbsp; &nbsp; train_loss += loss.item()
&nbsp; &nbsp; &nbsp; &nbsp; _, predicted = outputs.max(1)

&nbsp; &nbsp; &nbsp; &nbsp; total += targets.size(0)
&nbsp; &nbsp; &nbsp; &nbsp;  current_correct = predicted.eq(targets).sum().item()
&nbsp; &nbsp; &nbsp; &nbsp; correct += current_correct
&nbsp; &nbsp; &nbsp; &nbsp; if batch_idx % 100 == 0:

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print('\nCurrent batch:', str(batch_idx))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print('Current batch average train accuracy:', current_correct / targets.size(0))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print('Current batch average train loss:', loss.item() / targets.size(0))

&nbsp; &nbsp; print('\nTotal average train accuarcy:', correct / total)
&nbsp; &nbsp; print('Total average train loss:', train_loss / total)


def test(epoch):
&nbsp; &nbsp; print('\n[ Test epoch: %d ]' % epoch)
&nbsp; &nbsp; net.eval()
&nbsp; &nbsp; loss = 0
&nbsp; &nbsp; correct = 0
&nbsp; &nbsp; total = 0

  
&nbsp; &nbsp; for batch_idx, (inputs, targets) in enumerate(test_loader):
&nbsp; &nbsp; &nbsp; &nbsp; inputs, targets = inputs.to(device), targets.to(device)
&nbsp; &nbsp; &nbsp; &nbsp; total += targets.size(0)


&nbsp; &nbsp; &nbsp; &nbsp; outputs = net(inputs)
&nbsp; &nbsp; &nbsp; &nbsp; loss += criterion(outputs, targets).item()


&nbsp; &nbsp; &nbsp; &nbsp; _, predicted = outputs.max(1)
&nbsp; &nbsp; &nbsp; &nbsp; correct += predicted.eq(targets).sum().item()


&nbsp; &nbsp; print('\nTotal average test accuarcy:', correct / total)
&nbsp; &nbsp; print('Total average test loss:', loss / total)


&nbsp; &nbsp; state = {
&nbsp; &nbsp; &nbsp; &nbsp; 'net': net.state_dict()
&nbsp; &nbsp; }
&nbsp; &nbsp; if not os.path.isdir('checkpoint'):
&nbsp; &nbsp; &nbsp; &nbsp; os.mkdir('checkpoint')
&nbsp; &nbsp; torch.save(state, './checkpoint/' + file_name)
&nbsp; &nbsp; print('Model Saved!')



import time

def adjust_learning_rate(optimizer, epoch):
&nbsp; &nbsp; lr = learning_rate
&nbsp; &nbsp; if epoch &gt;= 50:
&nbsp; &nbsp; &nbsp; &nbsp; lr /= 10
&nbsp; &nbsp; if epoch &gt;= 100:
&nbsp; &nbsp; &nbsp; &nbsp; lr /= 10
&nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; for param_group in optimizer.param_groups:
&nbsp; &nbsp; &nbsp; &nbsp; param_group['lr'] = lr
  
start_time = time.time()

for epoch in range(0, 150):
&nbsp; &nbsp; adjust_learning_rate(optimizer, epoch)
&nbsp; &nbsp; train(epoch)
&nbsp; &nbsp; test(epoch)
&nbsp; &nbsp; print('\nTime elapsed:', time.time() - start_time)

<br>양자화 공부<br>
양자화 : 실수형 변수(floating-point type)를 정수형 변수(integer or fixed point)로 변환하는 과정<br>
양자화 하는 이유 : 인공지능 모델에 큰 비트수의 자료형을 사용 -&gt; 학습 과정에서 계산량과 필요한 메모리 크기 등이 커지게 됨. -&gt; 학습을 시키기 위해 많은 리소스가 필요해지고, 추론도 오래 걸리는 문제가 발생. 양자화를 통하여 효과적인 모델 최적화를 할 수 있는데, float 타입을 int형으로 줄이면서 용량을 줄일 수 있고 bit 수를 줄임으로써 계산 복잡도도 줄일 수 있음<br>
Pipeline<br>
-HuggingFace의 가장 기본 기능으로, 자연어 처리 작업, inference(추론)을 빠르게 할 수 있게 해준다.<br>
-(hugging face에 대한 내용은 처음 보낸 코랩 파일 가장 위에 있으니 더 알아보고싶으시면 참고하시면 됩니다!)<br>
-pretrained model(사전학습 모델)을 사용하는 가장 쉬운 방법.<br>
-사전학습모델이란 : 예를 들어 텍스트 유사도 예측 모델을 만들기 위해서, 감정 분석 문제를 학습했던 모델의 가중치를 활용하는 방법. 즉, 감정 분석 문제를 학습하면서 얻은 언어에 대한 이해를 텍스트 유사도 문제를 학습하는 데 활용하는 방식이다.<br>
pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, kwargs) 매개변수 설명**<br>
-task : 어떤 작업을 할것인가? -&gt; 여기에서는 'text-generation' 텍스트 생성 작업을 할거임. ( 그 외 question-answering, translation 등등이 있음 ) 이건 pipeline을 사용할 때 꼭 지정해주어야 함. 나머지것들은 기본으로 지정된 것들이 있기 때문에 따로 필요한 경우만 지정해주면 됨.<br>
-model : 어떤 모델을 사용할것인가? -&gt; 여기에서는 "meta-llama/Meta-Llama-3-8B-Instruct" 라는 hugging face에서 미리 가져온 모델을 사용.<br>
-device map : 모델이 어디서(GPU 또는 CPU) 실행되어야할까? -&gt; 여기에서는 "auto" 로, 현재 기기에서 사용가능한 장소를 자동으로 감지하고, GPU가 있다면 이를 우선적으로 사용<br>
-model_kwargs : 추가로 전달할 매개변수(예를 들어 특정 설정을 변경하는 경우 사용) -&gt; 여기에서는 {"quantization_config": quantization_config} 이라는 quantization(양자화) 에 대한 설정을 포함하구 있음.<br>#준비
!pip install bitsandbytes # 양자화 기법을 사용할 수 있게 해주는 파이썬 모듈 다운로드
!pip install -U bitsandbytes
from transformers import pipeline, BitsAndBytesConfig # BitsAndBytesConfig 허깅페이스에서 양자화를 위한 라이브러리

  

#허깅페이스 로그인("meta-llama/Meta-Llama-3-8B-Instruct"를 사용하기 위함)
from huggingface_hub import login
login("내 TOKEN")

  

#양자화 옵션 설정
#4bit로 되어있긴 하지만, 8bit도 가능.

quantization_config = BitsAndBytesConfig(load_in_4bit=True) &nbsp;# You can also try load_in_8bit
pipe = pipeline("text-generation", "meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", model_kwargs={"quantization_config": quantization_config})



#양자화 한 후 실행
chat = [
&nbsp; &nbsp; {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
&nbsp; &nbsp; {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
chat.append(
&nbsp; &nbsp; {"role": "user", "content": "Wait, what's so wild about soup cans?"}
)
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])

]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240813-모각코-활동-7회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240813 모각코 활동 7회차.md</guid><pubDate>Wed, 25 Sep 2024 07:11:10 GMT</pubDate></item></channel></rss>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[ejkiwi.github.io]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://ejkiwi.github.io/</link><image><url>https://ejkiwi.github.io/lib/media/favicon.png</url><title>ejkiwi.github.io</title><link>https://ejkiwi.github.io/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 10 Feb 2025 10:22:14 GMT</lastBuildDate><atom:link href="https://ejkiwi.github.io/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 10 Feb 2025 10:22:13 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[20250209 모각코 활동 5회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 기계학습 구현<br><br><br>import pandas as pd  
import matplotlib.pyplot as plt  
  
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler  
from sklearn.linear_model import LogisticRegression  
  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.tree import plot_tree
<br><br># 데이터 다운로드  
wine = pd.read_csv('https://bit.ly/wine_csv_data')
<br># 데이터 구조 확인  
print(wine.head())  # 처음 5개의 샘플  
print(wine.info())  # 데이터프레임의 각 열의 데이터 타입과 누락된 데이터 확인  
print(wine.describe())  # 통계 ( 평균, 표준편차, 최소, 최대, 중간값, 1사분위수, 3사분위수 )
<br>   alcohol  sugar    pH  class<br>
0      9.4    1.9  3.51    0.0<br>
1      9.8    2.6  3.20    0.0<br>
2      9.8    2.3  3.26    0.0<br>
3      9.8    1.9  3.16    0.0<br>
4      9.4    1.9  3.51    0.0<br>
&lt;class 'pandas.core.frame.DataFrame'&gt;<br>
RangeIndex: 6497 entries, 0 to 6496<br>
Data columns (total 4 columns):<br>
Column   Non-Null Count  Dtype  <br><br> 0   alcohol  6497 non-null   float64<br>
1   sugar    6497 non-null   float64<br>
2   pH       6497 non-null   float64<br>
3   class    6497 non-null   float64<br>
dtypes: float64(4)<br>
memory usage: 203.2 KB<br>
None<br>
alcohol        sugar           pH        class<br>
count  6497.000000  6497.000000  6497.000000  6497.000000<br>
mean     10.491801     5.443235     3.218501     0.753886<br>
std       1.192712     4.757804     0.160787     0.430779<br>
min       8.000000     0.600000     2.720000     0.000000<br>
25%       9.500000     1.800000     3.110000     1.000000<br>
50%      10.300000     3.000000     3.210000     1.000000<br>
75%      11.300000     8.100000     3.320000     1.000000<br>
max      14.900000    65.800000     4.010000     1.000000<br># 판다스 데이터 프레임 -&gt; 넘파이 배열  
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()  
target = wine['class'].to_numpy()
<br># 데이터 나누기  
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
<br># 나눈 데이터 형태 확인  
print(train_input.shape, test_input.shape)
<br>(5197, 3) (1300, 3)<br># 데이터 전처리  
ss = StandardScaler()  
ss.fit(train_input)  
  
train_scaled = ss.transform(train_input)  
test_scaled = ss.transform(test_input)
<br><br>lr = LogisticRegression()  
lr.fit(train_scaled, train_target)  
  
print(lr.score(train_scaled, train_target))  
print(lr.score(test_scaled, test_target))  
  
print(lr.coef_, lr.intercept_)  # 로지스틱 회귀가 학습한 계수와 절편
<br>0.7808350971714451 0.7776923076923077 <a data-href="0.51268071  1.67335441 -0.68775646" href="https://ejkiwi.github.io/0.51268071  1.67335441 -0.68775646" class="internal-link" target="_self" rel="noopener nofollow">0.51268071  1.67335441 -0.68775646</a> [1.81773456]`<br><br>dt = DecisionTreeClassifier(random_state=42)  
dt.fit(train_scaled, train_target)  
  
print(dt.score(train_scaled, train_target))  
print(dt.score(test_scaled, test_target))
<br>0.996921300750433<br>
0.8592307692307692<br># 훈련 결과 시각화  
plt.figure(figsize=(10,7))  
plot_tree(dt)  
plt.show()
<br><img alt="wine_plot1" src="https://ejkiwi.github.io/lib/media/wine_tree1.png" referrerpolicy="no-referrer"><br># 자세히 살펴보기  
plt.figure(figsize=(10,7))  
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])  
plt.show()
<br><img alt="wine_plot2" src="https://ejkiwi.github.io/lib/media/wine_tree2.png" referrerpolicy="no-referrer"><br># 가지치기  
dt = DecisionTreeClassifier(max_depth=3, random_state=42)  
dt.fit(train_scaled, train_target)  
  
print(dt.score(train_scaled, train_target))  
print(dt.score(test_scaled, test_target))
<br>0.8454877814123533<br>
0.8415384615384616<br># 가지치고 난 뒤의 훈련 시각화  
plt.figure(figsize=(20,15))  
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])  
plt.show()
<br><img alt="wine_plot3" src="https://ejkiwi.github.io/lib/media/wine_tree3.png" referrerpolicy="no-referrer"><br># 전처리 하기 전의 데이터들로 다시 훈련해보기  -&gt;  결과는 같을 것.  
dt = DecisionTreeClassifier(max_depth=3, random_state=42)  
dt.fit(train_input, train_target)  
  
print(dt.score(train_input, train_target))  
print(dt.score(test_input, test_target))
<br>0.8454877814123533<br>
0.8415384615384616<br># 시각화 -&gt; 데이터를 전처리 하고 난 뒤에 훈련 한 것과 같은 트리를 갖지만, 특성값을 표준점수로 바꾸지 않았기 때문에 이해하기 더욱 쉬움.  
plt.figure(figsize=(20,15))  
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])  
plt.show()
<br><img alt="wine_plot4" src="https://ejkiwi.github.io/lib/media/wine_tree4.png" referrerpolicy="no-referrer"><br># 특성 중요도  
print(dt.feature_importances_)
<br>[0.12345626 0.86862934 0.0079144 ]]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250209-모각코-활동-5회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250209 모각코 활동 5회차.md</guid><pubDate>Mon, 10 Feb 2025 10:21:55 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/wine_tree1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/wine_tree1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2025_겨울_모각코]]></title><description><![CDATA[ 
 <br>
<br><a data-href="파샤샥 모각코 활동" href="https://ejkiwi.github.io/2025_겨울_모각코/파샤샥-모각코-활동.html" class="internal-link" target="_self" rel="noopener nofollow">파샤샥 모각코 활동</a>
<br><a data-href="20250109 모각코 활동 1회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250109-모각코-활동-1회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250109 모각코 활동 1회차</a>
<br><a data-href="20250117 모각코 활동 2회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250117-모각코-활동-2회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250117 모각코 활동 2회차</a>
<br><a data-href="20250126 모각코 활동 3회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250126-모각코-활동-3회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250126 모각코 활동 3회차</a>
<br><a data-href="20250202 모각코 활동 4회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250202-모각코-활동-4회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250202 모각코 활동 4회차</a>
<br><a data-href="20250209 모각코 활동 5회차" href="https://ejkiwi.github.io/2025_겨울_모각코/20250209-모각코-활동-5회차.html" class="internal-link" target="_self" rel="noopener nofollow">20250209 모각코 활동 5회차</a>
]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/2025_겨울_모각코.html</link><guid isPermaLink="false">2025_겨울_모각코/2025_겨울_모각코.md</guid><pubDate>Mon, 10 Feb 2025 10:20:38 GMT</pubDate></item><item><title><![CDATA[파샤샥 모각코 활동]]></title><description><![CDATA[ 
 <br>'파사삭' ==  '파이썬 파사삭'<br>나의 모각코 활동 다짐 : 파이썬의 짱이 되겠다!!!!!!<br>팀 모각코 계획<br>]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/파샤샥-모각코-활동.html</link><guid isPermaLink="false">2025_겨울_모각코/파샤샥 모각코 활동.md</guid><pubDate>Mon, 10 Feb 2025 08:46:23 GMT</pubDate></item><item><title><![CDATA[2025 cnu 동계 학습동아리]]></title><description><![CDATA[ 
 <br>
<br><a data-href="통통이들 학습 동아리 활동" href="https://ejkiwi.github.io/2025-cnu-동계-학습동아리/통통이들-학습-동아리-활동.html" class="internal-link" target="_self" rel="noopener nofollow">통통이들 학습 동아리 활동</a>
<br><a data-href="1주차" href="https://ejkiwi.github.io/2025-cnu-동계-학습동아리/1주차.html" class="internal-link" target="_self" rel="noopener nofollow">1주차</a>
]]></description><link>https://ejkiwi.github.io/2025-cnu-동계-학습동아리/2025-cnu-동계-학습동아리.html</link><guid isPermaLink="false">2025 cnu 동계 학습동아리/2025 cnu 동계 학습동아리.md</guid><pubDate>Sun, 26 Jan 2025 09:21:18 GMT</pubDate></item><item><title><![CDATA[통통이들 학습 동아리 활동]]></title><description><![CDATA[ 
 <br>비록 미적짱은 되지 못했지만 확통짱은 될 수 있을것이야... \( •̀ ω •́ )/ ]]></description><link>https://ejkiwi.github.io/2025-cnu-동계-학습동아리/통통이들-학습-동아리-활동.html</link><guid isPermaLink="false">2025 cnu 동계 학습동아리/통통이들 학습 동아리 활동.md</guid><pubDate>Sun, 26 Jan 2025 09:21:00 GMT</pubDate></item><item><title><![CDATA[20250202 모각코 활동 4회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : Pandas 공부와 타이타닉 데이터 분석<br><br><br>
<br>판다스(Pandas) : 파이썬에서의 데이터 분석과 처리를 위해 사용되는 라이브러리.
<br>엑셀, CSV, SQL 같은 다양한 형식의 데이터를 쉽게 다룰 수 있음.
<br>numpy기반으로 만들어져 빠르고 효율적임.
<br><br>Series (1차원 데이터)<br>
<br>리스트나 배열과 비슷하지만, 인덱스를 가질 수 있음.
<br>data = pd.Series([10, 20, 30, 40], index=['A', 'B', 'C', 'D'])
<br>DataFrame (2차원 데이터)<br>
<br>표 형태의 데이터 구조로, 행과 열로 구성됨.
<br>data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'Score': [85, 90, 95]}
<br>데이터 불러오기<br>.read_csv('파일경로.csv')  # CSV 파일 불러오기
.read_excel('파일경로.xlsx') # 엑셀 파일 불러오기
<br><br>.info()  # 데이터 요약 정보
.describe()  # 수치형 데이터 통계 요약 (이 때, include = 'object' 라는 파라미터를 넣어주면, 범주형 변수에 대한 통계정보를 볼 수 있음. )
.shape  # (행, 열) 크기 확인
.columns  # 컬럼(열) 이름 확인
.dtypes  # 데이터 타입 확인 
.isnull().sum()  # 결측치(NaN) 개수 확인
.head()  # 상위 5개 행 출력
.tail()  # 하위 5개 행 출력
.dropna()  # 결측치가 하나라도 있는 행 삭제
.
.
.
등등...
<br><br><br># 필요한 모듈 불러오기  
import pandas as pd  
import matplotlib.pyplot as plt  
import seaborn as sns

# 데이터 불러오기  
df = sns.load_dataset('titanic')

# 데이터 기본 정보 확인  
print(df.info())  
print(df.isnull().sum())  # 결측치 확인  
  
# 결측치 처리 (결측치가 하나라도 있는 행은 그냥 없애버릴것이다~!)  
df = df.dropna()  
df = df.reset_index(drop=True)  # 인덱스 정리
<br>결과<br>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 15 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   survived     891 non-null    int64   
 1   pclass       891 non-null    int64   
 2   sex          891 non-null    object  
 3   age          714 non-null    float64 
 4   sibsp        891 non-null    int64   
 5   parch        891 non-null    int64   
 6   fare         891 non-null    float64 
 7   embarked     889 non-null    object  
 8   class        891 non-null    category
 9   who          891 non-null    object  
 10  adult_male   891 non-null    bool    
 11  deck         203 non-null    category
 12  embark_town  889 non-null    object  
 13  alive        891 non-null    object  
 14  alone        891 non-null    bool    
dtypes: bool(2), category(2), float64(2), int64(4), object(5)
memory usage: 80.7+ KB
None
survived         0
pclass           0
sex              0
age            177
sibsp            0
parch            0
fare             0
embarked         2
class            0
who              0
adult_male       0
deck           688
embark_town      2
alive            0
alone            0
dtype: int64
<br><br>print(df.describe())  
print(df.describe(include = 'object'))  # 범주형 변수에 대한 통계정보
<br>결과<br>         survived      pclass         age       sibsp       parch        fare
count  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000
mean     0.675824    1.192308   35.623187    0.467033    0.478022   78.919735
std      0.469357    0.516411   15.671615    0.645007    0.755869   76.490774
min      0.000000    1.000000    0.920000    0.000000    0.000000    0.000000
25%      0.000000    1.000000   24.000000    0.000000    0.000000   29.700000
50%      1.000000    1.000000   36.000000    0.000000    0.000000   57.000000
75%      1.000000    1.000000   47.750000    1.000000    1.000000   90.000000
max      1.000000    3.000000   80.000000    3.000000    4.000000  512.329200
         sex embarked  who  embark_town alive
count    182      182  182          182   182
unique     2        3    3            3     2
top     male        S  man  Southampton   yes
freq      94      115   87          115   123
<br><br># 성별에 따른 생존률  
print(df.groupby('sex')['survived'].mean() * 100)

# 객실 등급에 따른 생존률  
print(df.groupby('pclass')['survived'].mean() * 100)

# 연령대별 생존률  
print(df.groupby('age')['survived'].mean() * 100)
<br>결과<br>sex
female    93.181818
male      43.617021
Name: survived, dtype: float64

pclass
1    67.515924
2    80.000000
3    50.000000
Name: survived, dtype: float64

age
0.92     100.000000
1.00     100.000000
2.00      33.333333
3.00     100.000000
4.00     100.000000
            ...    
64.00      0.000000
65.00      0.000000
70.00      0.000000
71.00      0.000000
80.00    100.000000
Name: survived, Length: 63, dtype: float64
<br><br>corr_data = df.select_dtypes(include=['number']).corr()  # 숫자형 데이터만 추출하여 상관관계 분석  
  
# 상관관계 시각화  
plt.figure(figsize=(10, 6))  
sns.heatmap(corr_data, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)  
plt.title("heatmap - titanic data")  
plt.show()
<br><img alt="titanic1" src="https://ejkiwi.github.io/lib/media/titanicplot_1.png" referrerpolicy="no-referrer"><br># 주요 변수들간의 상관관계 시각화  
sns.pairplot(df[['survived', 'age', 'sibsp', 'fare']])  
plt.show()
<br><img alt="titanic2" src="https://ejkiwi.github.io/lib/media/titanicplot_2.png" referrerpolicy="no-referrer">]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250202-모각코-활동-4회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250202 모각코 활동 4회차.md</guid><pubDate>Sun, 02 Feb 2025 16:50:57 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/titanicplot_1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/titanicplot_1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="백준" href="https://ejkiwi.github.io/백준/백준.html" class="internal-link" target="_self" rel="noopener nofollow">백준</a>
<br><a data-href="2024_여름_모각코" href="https://ejkiwi.github.io/2024_여름_모각코/2024_여름_모각코.html" class="internal-link" target="_self" rel="noopener nofollow">2024_여름_모각코</a>
<br><a data-href="2024 cnu 2차 학습동아리" href="https://ejkiwi.github.io/2024-cnu-2차-학습동아리/2024-cnu-2차-학습동아리.html" class="internal-link" target="_self" rel="noopener nofollow">2024 cnu 2차 학습동아리</a>
<br><a data-href="2025_겨울_모각코" href="https://ejkiwi.github.io/2025_겨울_모각코/2025_겨울_모각코.html" class="internal-link" target="_self" rel="noopener nofollow">2025_겨울_모각코</a>
<br>
<br><br>사실이건아무것도작성한게없다...&gt;_&lt;<br>
<br><a data-tooltip-position="top" aria-label="https://velog.io/@eonjikiwi/posts" rel="noopener nofollow" class="external-link" href="https://velog.io/@eonjikiwi/posts" target="_blank">ejkiwi_velog</a>
]]></description><link>https://ejkiwi.github.io/index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Sun, 26 Jan 2025 08:51:25 GMT</pubDate></item><item><title><![CDATA[20250126 모각코 활동 3회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 넘파이, 차트 공부 및 구현 (넘파이와 맷플롯립과 짱친되기)<br><br>Numerical Python : 파이썬에서 수치 계산을 빠르고 효율적으로 수행할 수 있도록 해주는 라이브러리.<br>
( ?라이브러리 : 자주 쓰이는 코드들을 묶어놓은 도구상자. ex - PyTorch는 AI 모델을 만들 때 쓰는 도구상자같은것... )<br><br>
<br>빠른 연산 : C로 구현되어 있어 배열 연산 속도가 매우 빠르다
<br>다차원 배열 지원

<br>복잡한 데이터 표현이 가능해진다 ( 이미지 3D : 높이.너비.채널, 동영상 4D : 프레임 수.높이.너비.채널)
<br>효율적인 연산이 가능해짐 -&gt; 100개의 2D이미지를 처한다고 쳤을 때, 일반 Python 리스트로는 구조가 복잡해지고 느려짐. 하지만 다차원 배열을 사용하면 연산이 단순하고 빨라짐.


<br>다양한 함수
<br>배열 연산이 간편 : 반복문 없이 배열 전체에 대해 한 번에 연산 수행 가능 ( 벡터화 연산 )
<br>import numpy as np ( 보통은 np라는 이름으로 불러옴 )
<br><br># 1D 배열
a = np.array([1, 2, 3])

# 2D 배열
b = np.array([[1, 2, 3], [4, 5, 6]])

# 0으로 채워진 배열
c = np.zeros((3, 3))

# 1로 채워진 배열
d = np.ones((2, 4))

# 임의의 숫자로 초기화된 배열
e = np.full((2, 2), 7)

# 연속된 숫자로 이루어진 배열
f = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]

# 균등 분할된 값
g = np.linspace(0, 1, 5)  # [0. , 0.25, 0.5 , 0.75, 1.]
<br><img alt="배열 생성" src="https://ejkiwi.github.io/lib/media/numpy01.png" referrerpolicy="no-referrer"><br><br>arr = np.array([[1, 2, 3], [4, 5, 6]])

print(arr.shape)  # 배열 형태 (2, 3)
print(arr.ndim)   # 차원 개수 (2D 배열이면 2)
print(arr.size)   # 전체 요소 개수 (6)
print(arr.dtype)  # 데이터 타입 (int32, float64 등)
<br><img alt="배열 속성 확인" src="https://ejkiwi.github.io/lib/media/numpy02.png" referrerpolicy="no-referrer"><br><br>arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# 특정 요소 접근
print(arr[0, 1])  # 0번째 행, 1번째 열 (2)

# 행, 열 슬라이싱
print(arr[1, :])  # 1번째 행 전체 [4, 5, 6]
print(arr[:, 2])  # 모든 행의 2번째 열 [3, 6, 9]

# 범위 슬라이싱
print(arr[0:2, 1:3])  # 첫 두 행의 1~2번째 열 [[2, 3], [5, 6]]
<br><img alt="배열 인덱싱과 슬라이싱" src="https://ejkiwi.github.io/lib/media/numpy03.png" referrerpolicy="no-referrer"><br><br>x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

# 요소별 연산
print(x + y)  # [5, 7, 9]
print(x * y)  # [4, 10, 18]
print(x ** 2)  # [1, 4, 9]

# 스칼라와 연산
print(x + 10)  # [11, 12, 13]
<br><img alt="배열 연산" src="https://ejkiwi.github.io/lib/media/numpy04.png" referrerpolicy="no-referrer"><br><br>arr = np.array([3, 1, 4, 2])

# 합, 평균, 표준편차, 최댓값, 최솟값
print(np.sum(arr))  # 10
print(np.mean(arr))  # 2.5
print(np.std(arr))  # 표준편차
print(np.max(arr))  # 4
print(np.min(arr))  # 1

# 오름차순 정렬
print(np.sort(arr)) # [1, 2, 3, 4]
# 내림차순 정렬
print(np.sort(arr)[::-1]) # [4, 3, 2, 1]
<br><img alt="그 외 유용한 함수" src="https://ejkiwi.github.io/lib/media/numpy05.png" referrerpolicy="no-referrer"><br><br>arr = np.array([[1, 2], [3, 4]])

# 배열 형태 변경
reshaped = arr.reshape(4, 1)
print(reshaped)
# [[1]
#  [2]
#  [3]
#  [4]]

# 전치 (Transpose)
print(arr.T)
# [[1 3]
#  [2 4]]

# 1차원으로 펼치기
flattened = arr.flatten()
print(flattened)  # [1, 2, 3, 4]
<br><img alt="배열 변환" src="https://ejkiwi.github.io/lib/media/numpy06.png" referrerpolicy="no-referrer"><br><br>Python에서의 데이터 시각화를 위한 라이브러리<br><br>
<br>다양한 시각화 제공 : 간단한 선 그래프, 막대 그래프, 히스토그램부터 복잡한 3D 그래프까지 다양한 시각화를 제공함.
<br>보통 matplotlib.pyplot 모듈을 통해 그래프를 그림.
<br>보통 import matplotlib.pyplot as plt plt라는 이름으로 불러옴.
<br><br># 데이터  
x = [0, 1, 2, 3, 4, 5]  
y = [0, 2, 4, 6, 8, 10]  
  
# 선 그래프 시각화  
plt.plot(x, y, label="y = 2x", color="blue", linestyle="--", marker="o")  
plt.title("Line Plot Example")  # 그래프 제목  
plt.xlabel("X-axis")  # x축 이름  
plt.ylabel("Y-axis")  # y축 이름  
plt.legend()  # 범례 추가  
plt.show()  # 시각화
<br><img alt="선 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib01.png" referrerpolicy="no-referrer"><br><br># 데이터  
x = [1, 2, 3, 4, 5]  
y = [5, 7, 6, 8, 7]  
sizes = [100, 200, 300, 400, 500]  
  
# 산점도 시각화  
plt.scatter(x, y, s=sizes, color="green", alpha=0.6)  
plt.title("Scatter Plot Example")  # 제목  
plt.xlabel("X-axis")  # x축 이름  
plt.ylabel("Y-axis")  # y축 이름  
plt.show()  # 시각화
<br><img alt="산점도 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib02.png" referrerpolicy="no-referrer"><br><br># 데이터  
categories = ['A', 'B', 'C', 'D']  
values = [3, 7, 8, 5]  
  
# 막대 그래프 시각화  
plt.bar(categories, values, color="orange")  
plt.title("Bar Plot Example")  # 제목  
plt.xlabel("Categories")  # x축 이름  
plt.ylabel("Values")  # y축 이름  
plt.show()  # 시각화
<br><img alt="막대 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib03.png" referrerpolicy="no-referrer"><br><br># 데이터  
data = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 6, 7, 8, 9, 10]  
  
# 히스토그램 시각화  
plt.hist(data, bins=5, color="purple", alpha=0.7)  
plt.title("Histogram Example")  # 제목  
plt.xlabel("Value Ranges")  # x축 이름  
plt.ylabel("Frequency")  # y축 이름  
plt.show()  # 시각화
<br><img alt="히스토그램" src="https://ejkiwi.github.io/lib/media/matplotlib04.png" referrerpolicy="no-referrer"><br><br># 데이터  
data = [  
    [2, 3, 5, 6, 8],  # Group 1  
    [1, 4, 4, 5, 9],  # Group 2  
    [3, 5, 7, 7, 10]  # Group 3  
]  
  
# 박스 플롯 시각화  
plt.boxplot(data, tick_labels=['Group 1', 'Group 2', 'Group 3'])  ## tick_labels -&gt; 이전에는 labels로 쓰임  
plt.title("Box Plot Example")  # 제목  
plt.ylabel("Values")  # y축 이름  
plt.show()
<br><img alt="박스 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib05.png" referrerpolicy="no-referrer"><br><br># 데이터  
labels = ['Apples', 'Bananas', 'Cherries', 'Mangoes']  
sizes = [35, 25, 20, 20]  
  
# 파이 차트 시각화  
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=['red', 'yellow', 'pink', 'brown'])  
plt.title("Pie Chart Example")  # 제목  
plt.show()  # 시각화
<br><img alt="원형 차트" src="https://ejkiwi.github.io/lib/media/matplotlib06.png" referrerpolicy="no-referrer"><br><br># 데이터  
x = [1, 2, 3, 4, 5]  
y1 = [1, 2, 4, 8, 16]  
y2 = [1, 1, 2, 3, 5]  
  
# 면적 그래프 시각화  
plt.fill_between(x, y1, color="skyblue", alpha=0.5, label="Area 1")  
plt.fill_between(x, y2, color="lightgreen", alpha=0.7, label="Area 2")  
plt.title("Area Plot Example")  # 제목  
plt.xlabel("X-axis")  # x축 이름  
plt.ylabel("Y-axis")  # y축 이름  
plt.legend()  # 범례 추가  
plt.show()  # 시각화
<br><img alt="면적 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib07.png" referrerpolicy="no-referrer"><br><br># 데이터  
x = [1, 2, 3, 4, 5]  
y1 = [2, 4, 6, 8, 10]  
y2 = [1, 3, 5, 7, 9]  
  
# 다중 그래프 시각화  
plt.subplot(1, 2, 1)  # ( 1행 2열 )plt.plot(x, y1, color="blue", marker="o")  # 첫 번째 그래프  
plt.title("Graph 1")  # 첫 그래프 이름  
  
plt.subplot(1, 2, 2)  
plt.plot(x, y2, color="red", linestyle="--")  # 두 번째 그래프  
plt.title("Graph 2")  # 두 번째 그래프 이름  
  
plt.tight_layout()  # 간격 조정  
plt.show()  # 시각화
<br><img alt="다중 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib08.png" referrerpolicy="no-referrer"><br><br># 데이터  
categories = ['A', 'B', 'C']  
group1 = [3, 5, 7]  
group2 = [4, 6, 8]  
  
# 스택형 막대 그래프 시각화  
plt.bar(categories, group1, label="Group 1", color="lightblue")  
plt.bar(categories, group2, bottom=group1, label="Group 2", color="orange")  # 스택 쌓기  
plt.title("Stacked Bar Plot Example")  # 제목  
plt.xlabel("Categories")  # x축 이름  
plt.ylabel("Values")  # y축 이름  
plt.legend()  # 범례 추가  
plt.show()  # 시각화
<br><img alt="스택형 막대 그래프" src="https://ejkiwi.github.io/lib/media/matplotlib09.png" referrerpolicy="no-referrer">]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250126-모각코-활동-3회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250126 모각코 활동 3회차.md</guid><pubDate>Sun, 26 Jan 2025 08:15:20 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/numpy01.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/numpy01.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20250117 모각코 활동 2회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 자료구조 개념과 종류(리스트, 튜플, 집합, 딕셔너리) 공부 및 구현<br><br> 여러가지 자료를 저장할 수 있는 데이터 구조체 &nbsp;[ , , ]<br>
<br>요소(value) : 리스트 내 데이터를 의미
<br>인덱스(index) : 리스트 내 데이터의 주소
<br>순서가 있는 데이터 구조 
<br>수정 가능 (mutable).
<br>중복된 값을 허용.
<br>my_list = [1, 2, 3, 4]  # 리스트 생성
my_list.append(5)       # 값 추가
my_list[0] = 10         # 값 수정
print(my_list)  # [10, 2, 3, 4, 5]
<br><br>수정이 불가능한, 여러가지 자료를 저장할 수 있는 데이터 구조체 ( , , )<br>
<br>순서가 있는 데이터 구조.
<br>수정 불가능 (immutable).
<br>중복된 값을 허용.
<br>반복문에서 리스트보다 빠름
<br>딕셔너리의 키가  될 수 있음
<br>리스트와 튜플 서로 변환 가능
<br>*&lt;주의&gt; 요소가 1개인 튜플은 요소의 끝에 반드시 쉼표(,) 추가
<br>my_tuple = (1, 2, 3, 4)  # 튜플 생성
# my_tuple[0] = 10  # 수정 불가능 (오류 발생)
print(my_tuple[1])  # 2
<br><br>고유한 값들을 순서 없이 저장할 수 있는 데이터 구조체 { , , }<br>
<br>순서가 없는 데이터 구조 ( = 인덱스 접근 불가 )
<br>중복된 값을 허용하지 않음.
<br>수학의 집합 연산 가능 (합집합, 교집합 등).
<br>리스트와 집합&nbsp;서로 변환 가능
<br>my_set = {1, 2, 3, 3} # 중복된 값은 자동 제거
my_set.add(4) # 값 추가
print(my_set) # {1, 2, 3, 4}
<br><br>키key를 기반으로 값value을 저장할 수 있는 데이터 구조체 { key:value, key:value, ..., key:value }<br>
<br>키-값 (key-value) 쌍으로 데이터를 저장.
<br>키는 중복될 수 없으나, 값은 중복 가능.
<br>순서는 유지
<br>key는 변수로 지정 가능
<br>my_dict = {"name": "Alice", "age": 25}  # 딕셔너리 생성
my_dict["age"] = 26  # 값 수정
my_dict["city"] = "Seoul"  # 새 키-값 추가
print(my_dict)  # {'name': 'Alice', 'age': 26, 'city': 'Seoul'}
]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250117-모각코-활동-2회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250117 모각코 활동 2회차.md</guid><pubDate>Fri, 17 Jan 2025 15:14:09 GMT</pubDate></item><item><title><![CDATA[20250109 모각코 활동 1회차]]></title><description><![CDATA[ 
 <br>오늘의 목표 : 연산자, 조건문, 반복문, 함수 복습<br><br>수식 : 하나의 값으로 평가될 수 있는 표현. 수학적 관계를 나타내는 것이 아님. 연산자와&nbsp; 피연산자로 구성.<br>
<br>연산자 : 어떤 연산을 나타내는 기호
<br>피연산자 : 연산의 대상이 되는 숫자나 변수를 의미
<br>산술 연산자:<br>
<br>거듭제곱(지수)는 높은 우선순위를 가짐
<br>거듭제곱 표현 시 왼쪽부터 계산됨..!! abc (&lt;-방향)
<br>지수자리에는 실수도 들어올 수 있다
<br><br>비교연산자:<br>
<br>부울BOOL형 T F의 값을 가짐
<br>붙어있는 그 자체로 하나임.
<br>참 거짓 값을 갖게 된 비교연산자 덩어리들을 변수에집어넣을수도 있음.
<br><br>논리연산자:<br>
<br>부울BOOL형 T F의 값을 가짐
<br><br>복합 할당 연산자:<br>
<br>붙어있는 그 자체로 하나임.
<br>비트 단위연산자에도 적용 가능
<br><br>그 외:<br>
<br>0제외 모든 수 : T
<br>0 : F
<br>문자 : T
<br>공백문자 "" : F
<br>None : F
<br>연산자 우선순위:<br><br><br>순차구조 : 여러 명령이 순차적으로 실행되는 구조<br>선택구조 : 여러 개 중 하나의 명령문을 선택하여 실행하는 구조<br>반복구조 : 동일한 명령이 반복되면서 실행되는 구조<br>조건식 : 조건을 만족하는지 그렇지 않은지를 판정하는 식, 부울형으로 평가됨<br><br><br>반복<br>
<br>횟수 제어 반복(for문) : 반복 횟수 지정
<br>조건 제어 반복(while문) : 특정 조건 만족시 계속 반복
<br><br>range()함수 : 숫자 시퀀스를 생산하는 공장임.<br>
<br>range(start=0,stop,step=1) : start와step은 따로 지정하지 않으면 0과 1로 지정. 반드시 지정해야하는 값은 stop값.
<br>break : 반복 루프를 즉시 빠져나감.<br>continue : 루프를 빠져나가지 않고 아래쪽 문장을 건너뛰기만 함.<br><br>객체와 메소드와 함수:<br>
<br>T =turtle.Turtle() -&gt; T라는 변수로 거북이 객체를 생성
<br>T.shape("turtle") -&gt; 거북이 객체 T에 shape()라는 명령을 내림 이 때 shape()는 메소드
<br>t.forward(100) -&gt; 거북이 객체 T에&nbsp; forward()라는 명령을 내림 이 때 forward()는 메소드
<br>print('helloworld')&nbsp; -&gt; 'helloworld'라는 문자열을 출력하기 위한 명령을 내리는 print()함수.
<br>함수 : 여러 개의 매개변수를 가질 수 있고, 이 매개변수를 통해 일을 함. 객체가 없더라도 일을 할 수 있고, print()처럼 단독으로 사용 가능. 함수()
<br>메소드 : 매개변수 가질 수 있음. 클래스의 일부로 메소드가 존재하며( 아마 메소드가 클래스의 일부로서 존재한다는 뜻 같음 ) 객체가 있어야 일을 할 수 있음. 객체.메소드() 느낌으로 사용해야함
<br>문자열 다루기(텍스트 처리)<br>
<br>
a='abcde'=&gt;문자열

<br>
문자열 안에 있는 개별 문자들을 추출하는 작업이 기본적. (문자열을 문자로 분해)

<br>
문자열은 인덱스로 접근 가능

<br>
인덱스(0~)/음수인덱스(-1~)

<br>
슬라이싱 : [이상:미만]

<br>
문자열 리스트 변환 : list('a')

<br>
문자열을 단어로 나누는 작업

<br>  a.split() -&gt; 띄어쓰기를 기준으로 구분
<br>a.split('.') -&gt;&nbsp; '.' 을 기준으로 구분
<br>=&gt; 즉, split()의 괄호 안에는 기준이 들어가는 거임.


<br>
단어들을 다시 붙이는 작업

<br>''.join(문자들) -&gt; 문자들이 붙음
<br>'-'.join(문자들) -&gt; 문자들이 -을 사용하여 붙여짐.


<br>
문자열의 출현 횟수 계산

<br>a.count('찾고싶은녀석')


<br>
문자열이 무엇으로 시작/끝 이 나는지 확인

<br>a.startswith() =&gt; 시작 . 부울형으로 결과가 나옴
<br>a.endswith() =&gt; 끝 . 부울형 결과 나옴


]]></description><link>https://ejkiwi.github.io/2025_겨울_모각코/20250109-모각코-활동-1회차.html</link><guid isPermaLink="false">2025_겨울_모각코/20250109 모각코 활동 1회차.md</guid><pubDate>Thu, 09 Jan 2025 11:43:48 GMT</pubDate></item><item><title><![CDATA[문제 1011]]></title><description><![CDATA[ 
 <br>Fly me to the Alpha Centauri<br>입력 : 입력의 첫 줄에는 테스트케이스의 개수 T가 주어진다. 각각의 테스트 케이스에 대해 현재 위치 x 와 목표 위치 y 가 정수로 주어지며, x는 항상 y보다 작은 값을 갖는다. (0 ≤ x &lt; y &lt; 231)<br>
출력 : 각 테스트 케이스에 대해 x지점으로부터 y지점까지 정확히 도달하는데 필요한 최소한의 공간이동 장치 작동 횟수를 출력한다.<br>case = []
for i in range(int(input())):
    d, e = map(int,input().split())
    case.append([d, e, e-d])

result = []
for i in case:
    r = 0
    count = 0
    m = 0

    while True:
        m += 1
        r += m
        count += 1
        if i[2] &lt;= r:
            result.append(count)
            break
        r += m
        count += 1
        if i[2] &lt;= r:
            result.append(count)
            break

for i in result:
    print(i)
<br>
<br>r: 누적 이동 거리
<br>count: 이동 횟수
<br>m: 현재 단계에서 한 번에 이동할 수 있는 거리<br>
이동 거리는 1부터 시작하며, 매 단계마다 1, 2, 3, ... 식으로 증가.(같은 거리만큼 두 번 반복)
<br><br>11 22 33 44 55 66 77 88 99 1010 1111 1212 1313 1414 ,,, -&gt; 개수  
12 34 56 78 910 ,,,, -&gt; 횟수의 수  
구간의 크기가 순서대로 1부터 20까지 있다고 하면, 그 구간에 따른 정답은  -&gt; 1 2 33 44 555 666 7777 8888이 됨.  
* (예) 구간의 크기가 8이면 정답은 5가 됨. / 구간의 크기가 10이면 정답은 6이 됨.
]]></description><link>https://ejkiwi.github.io/백준/문제-1011.html</link><guid isPermaLink="false">백준/문제 1011.md</guid><pubDate>Thu, 02 Jan 2025 08:39:47 GMT</pubDate></item><item><title><![CDATA[문제 1931]]></title><description><![CDATA[ 
 <br>회의실 배정<br>입력 : 첫째 줄에 회의의 수 N(1 ≤ N ≤ 100,000)이 주어진다. 둘째 줄부터 N+1 줄까지 각 회의의 정보가 주어지는데 이것은 공백을 사이에 두고 회의의 시작시간과 끝나는 시간이 주어진다. 시작 시간과 끝나는 시간은 231-1보다 작거나 같은 자연수 또는 0이다.<br>
출력 : 첫째 줄에 최대 사용할 수 있는 회의의 최대 개수를 출력한다.<br>import sys
input = sys.stdin.readline

case = []
for _ in range(int(input())):
    a = list(map(int,input().split()))
    case.append(a)
case.sort(key=lambda x: (x[0], x[1]))

CASE = []
cAse = set()
c = len(cAse)
for _ in case: #어차피 시작시간도 같은데 끝시간이 더 긴건 필요업냠냠냠냠냠,,..
    cAse.add(_[0])
    if _[1] == _[0]: #시작하자마자끝나는건필요해...
        CASE.append(_)
    elif len(cAse) != c: #정렬한 뒤, 저장하는거니까... 시작시각이 같은 회의들 중 일찍끝나는것만 필요
        c = len(cAse)
        CASE.append(_)

result = 1
now = case[0]
for _ in case[1::]:
    if now[1] &gt; _[1]: #now보다 지금 _의 끝 시작이 더 작다? -&gt; 이녀석은 더 효율적인 회의임 얘 선택해야해.. 근데 회의 카운트 수를 높일 수는 없음.
        now = _ #그냥 now를 더 좋은 회의로 바꾸는거임. 3 100, 4 5인 경우에 해당..

    elif now[1] &lt;= _[0]: #근데..? _의 시작시간이..? now의 끝나는 시작과 같다? -&gt; 이건 바로 회의 수 추가mood ~~ 완전 그 느낌임...
        if _[0] == _[1]: #시작시각과 끝 시각이 같은 경우.
            result += 1 #무조건 추가
        else:
            result += 1 #추가하고
            now = _ #현재 회의 바꾸고...

print(result)
<br>
<br>푼 방식은.. 주석과같다...~_~
]]></description><link>https://ejkiwi.github.io/백준/문제-1931.html</link><guid isPermaLink="false">백준/문제 1931.md</guid><pubDate>Thu, 02 Jan 2025 08:57:47 GMT</pubDate></item><item><title><![CDATA[문제 14369]]></title><description><![CDATA[ 
 <br>전화번호 수수께끼(Small)<br>입력 : 첫 줄에 테스트케이스의 개수 T가 주어진다. 각 테스트케이스에는 상대방이 제시한 스트링 S가 주어진다. S는 영어 대문자로만 이루어져 있다.<br>
1≤&nbsp;T&nbsp;≤ 100이고, S의 길이는 3 이상 20 이하이다. 모든 테스트케이스에는 유일한 해답이 있다.<br>
출력 : 각 줄에 테스트케이스 번호 x와 전화번호 y를 Case # x: y의 형태로 출력한다.<br>첫 시도 -&gt; 시간초과<br>ZERO = [1,0,0,0,0,0,1,1,0,0,0,0,0,0,1]
ONE =  [1,0,0,0,0,1,1,0,0,0,0,0,0,0,0]
TWO =  [0,0,0,0,0,0,1,0,0,1,0,0,1,0,0]
THREE =[2,0,0,1,0,0,0,1,0,1,0,0,0,0,0]
FOUR = [0,1,0,0,0,0,1,1,0,0,1,0,0,0,0]
FIVE = [1,1,0,0,1,0,0,0,0,0,0,1,0,0,0]
SIX =  [0,0,0,0,1,0,0,0,1,0,0,0,0,1,0]
SEVEN =[2,0,0,0,0,1,0,0,1,0,0,1,0,0,0]
EIGHT =[1,0,1,1,1,0,0,0,0,1,0,0,0,0,0]
NINE = [1,0,0,0,1,2,0,0,0,0,0,0,0,0,0]
NUMBER = [ ["0",ZERO], ["1", ONE], ["2", TWO], ["3", THREE], ["4", FOUR], ["5", FIVE], ["6", SIX], ["7", SEVEN], ["8", EIGHT], ["9",NINE] ]
ALPHABET = ['E', 'F', 'G', 'H', 'I', 'N', 'O', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z']


def CASE(TEXT, ALPHABET = ALPHABET):
    result = []
    for i in ALPHABET:
        result.append(TEXT.count(i))
    return result


def CLEAR_NUMBER(NUMBER = NUMBER, case = []):
    number = []
    #전처리
    for i in NUMBER:
        detect = 0
        for j in range(15):
            if i[1][j] &gt; case[j]:
                detect = 1
                break
        if detect == 0:
            number.append(i)
            case = [y-x for x,y in zip(i[1], case)]

    return number, case


def ANSWER(case, number = NUMBER):
    answer = []
    case = CASE(case)
    while True:
        if case == [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]:
            break
        number, case = CLEAR_NUMBER(number, case)
        for i in number:
            answer.append(i[0])

    return answer

answer = []
for i in range(int(input())):
    case = input()
    a = ""
    for j in sorted(ANSWER(case)):
        a += j
    answer.append(a)

m = 1
for i in answer:
    print("Case "+"#"+str(m)+": "+i)
    m += 1
<br>'E', 'F', 'G', 'H', 'I', 'N', 'O', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z' 를 차례대로 묶어서, 해당 알파벳이 있으면 1, 없으면 0을 매겨주었다. 그리고 주어진 case에서, 해당되는 알파벳 부분을 전체에서 빼주는 작업을<br>
[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] 이 될 때까지 반복해서<br>
정답을 찾는 방법으로 작성했다.<br>
<br>CLEAR_NUMBER 함수는 NUMBER 리스트를 순회하면서 각 숫자의 패턴과 case를 비교하는데, 매 숫자마다 15개의 값을 비교하고, 이를 매번 갱신하며 반복적으로 수행하는 점이 시간을 많이 잡아먹었고.. case가 0으로 수렴하는 과정에서 반복 횟수가 많아지고, 내부적으로 CLEAR_NUMBER를 여러 번 호출하므로 시간이 크게 소요되어서 시간초과가 났을 것이다.
<br>네 번째 시도<br>from collections import Counter

DIGITS = [
    ["0", "Z", "ZERO"],
    ["2", "W", "TWO"],
    ["4", "U", "FOUR"],
    ["6", "X", "SIX"],
    ["8", "G", "EIGHT"],
    ["3", "H", "THREE"],
    ["5", "F", "FIVE"],
    ["7", "V", "SEVEN"],
    ["1", "O", "ONE"],
    ["9", "I", "NINE"],
]

def solve_case(case):
    case_count = Counter(case)
    result = []

    for digit, unique_char, word in DIGITS:
        count = case_count[unique_char]
        if count &gt; 0:
            result.extend([digit] * count)
            for char in word:
                case_count[char] -= count

    return "".join(sorted(result))

# 입력 처리
t = int(input())
answers = []
for i in range(t):
    case = input().strip()
    answers.append(f"Case #{i + 1}: {solve_case(case)}")

print("\n".join(answers))
<br>그냥 아예 각 알파벳이 고유하게 가지고 있는 문자열을 비교해서 빼주는 방식으로 진행했다.<br>
<br>["1", "O", "ONE"] 같은 경우 "O"가  "ZERO"에도 있고 "TWO"에도 있고 "FOUR" 에도 있기 때문에 문제가 될 거 같지만 애당초 "ZERO"는 유일한 "Z"에 의해 다 걸러지게 되고, 마찬가지로 "TWO"는 "W"에, "FOUR"는 "U"에 걸러지게 되므로 상관 없다.
<br>from collections import Counter를 사용해서 시간을 줄였다.
]]></description><link>https://ejkiwi.github.io/백준/문제-14369.html</link><guid isPermaLink="false">백준/문제 14369.md</guid><pubDate>Thu, 02 Jan 2025 08:54:49 GMT</pubDate></item><item><title><![CDATA[백준]]></title><description><![CDATA[ 
 <br>
<br><a data-href="문제 1011" href="https://ejkiwi.github.io/백준/문제-1011.html" class="internal-link" target="_self" rel="noopener nofollow">문제 1011</a>
<br><a data-href="문제 14369" href="https://ejkiwi.github.io/백준/문제-14369.html" class="internal-link" target="_self" rel="noopener nofollow">문제 14369</a>
<br><a data-href="문제 1931" href="https://ejkiwi.github.io/백준/문제-1931.html" class="internal-link" target="_self" rel="noopener nofollow">문제 1931</a>
]]></description><link>https://ejkiwi.github.io/백준/백준.html</link><guid isPermaLink="false">백준/백준.md</guid><pubDate>Sat, 04 Jan 2025 15:33:19 GMT</pubDate></item><item><title><![CDATA[백준 풀이]]></title><description><![CDATA[ 
 <br>🌱<a data-tooltip-position="top" aria-label="https://solved.ac/profile/eonjikiwi" rel="noopener nofollow" class="external-link" href="https://solved.ac/profile/eonjikiwi" target="_blank">백준 프로필</a>🌱<br>열심히 풀었던 문제들/ 왕큰 깨달음을 얻었던 문제들을 정리해놓았다!!!!!!!!!!!!!]]></description><link>https://ejkiwi.github.io/백준/백준-풀이.html</link><guid isPermaLink="false">백준/백준 풀이.md</guid><pubDate>Sat, 04 Jan 2025 15:28:47 GMT</pubDate></item><item><title><![CDATA[2024 cnu 2차 학습동아리]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://colab.research.google.com/drive/1n9Pjthb3W4q-MO891Ce_-y5Jh2uOw4uv?usp=sharing" rel="noopener nofollow" class="external-link" href="https://colab.research.google.com/drive/1n9Pjthb3W4q-MO891Ce_-y5Jh2uOw4uv?usp=sharing" target="_blank">2024 cnu 2차 학습동아리 실습코드</a>]]></description><link>https://ejkiwi.github.io/2024-cnu-2차-학습동아리/2024-cnu-2차-학습동아리.html</link><guid isPermaLink="false">2024 cnu 2차 학습동아리/2024 cnu 2차 학습동아리.md</guid><pubDate>Thu, 02 Jan 2025 09:04:05 GMT</pubDate></item><item><title><![CDATA[2024_여름_모각코]]></title><description><![CDATA[ 
 <br>
<br><a data-href="모.구.모.구 모각코 활동" href="https://ejkiwi.github.io/2024_여름_모각코/모.구.모.구-모각코-활동.html" class="internal-link" target="_self" rel="noopener nofollow">모.구.모.구 모각코 활동</a>
<br><a data-href="20240707 모각코 활동 1회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240707-모각코-활동-1회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240707 모각코 활동 1회차</a>
<br><a data-href="20240709 모각코 활동 2회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240709-모각코-활동-2회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240709 모각코 활동 2회차</a>
<br><a data-href="20240716 모각코 활동 3회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240716-모각코-활동-3회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240716 모각코 활동 3회차</a>
<br><a data-href="20240723 모각코 활동 4회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240723-모각코-활동-4회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240723 모각코 활동 4회차</a>
<br><a data-href="20240730 모각코 활동 5회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240730-모각코-활동-5회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240730 모각코 활동 5회차</a>
<br><a data-href="20240806 모각코 활동 6회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240806-모각코-활동-6회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240806 모각코 활동 6회차</a>
<br><a data-href="20240813 모각코 활동 7회차" href="https://ejkiwi.github.io/2024_여름_모각코/20240813-모각코-활동-7회차.html" class="internal-link" target="_self" rel="noopener nofollow">20240813 모각코 활동 7회차</a>
]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/2024_여름_모각코.html</link><guid isPermaLink="false">2024_여름_모각코/2024_여름_모각코.md</guid><pubDate>Sun, 01 Dec 2024 08:02:21 GMT</pubDate></item><item><title><![CDATA[모.구.모.구 모각코 활동]]></title><description><![CDATA[ 
 <br>팀 모각코 목표 : 1. 절대 포기하지 않기, 2. 모르는 거 그냥 넘어가지 않기<br>나의 모각코 활동 다짐 : 활동 계획을 완벽히 마무리 할 수 있도록 노력하겠습니다!<br>나의 모각코 활동 계획<br>
<br>7월 7일

<br>모각코 활동 동안 공부할 주제 전체적으로 톺아보기, 팀원들과 친해지기


<br>7월 9일, 7월 16일

<br>파이토치 사용 익히기
<br>선배님 프로젝트의 레포지토리에 있는 코드 분석해보며 공부하기


<br>7월 23일, 7월 30일, 8월 6일, 8월 13일

<br>CNN 구조 공부하기
<br>RESNET 구조 공부하기


<br>모각코 팀블로그<br>
<a data-tooltip-position="top" aria-label="https://jolly-exoplanet-ef1.notion.site/1868305015324f9f84670142f4029fb7" rel="noopener nofollow" class="external-link" href="https://jolly-exoplanet-ef1.notion.site/1868305015324f9f84670142f4029fb7" target="_blank">모.구.모.구_팀블로그</a>]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/모.구.모.구-모각코-활동.html</link><guid isPermaLink="false">2024_여름_모각코/모.구.모.구 모각코 활동.md</guid><pubDate>Tue, 26 Nov 2024 17:34:04 GMT</pubDate></item><item><title><![CDATA[20240707 모각코 활동 1회차]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.모각코 활동 동안 공부할 주제 전체적으로 톺아보기<br>
2.팀원들과 친해지기<br>파이토치<br>
<br>python을 바탕으로 제작된, 딥러닝과  인공지능 분야에서 주로 활용되는 라이브러리
<br>pytorch의 연산은 tensor를 기본으로 하여 작동
<br>tensor : 파이토치의 기본 데이터 타입. 배열이나 행렬과 유사한 구조(다차원 배열)이다.
<br>파이토치를 기반으로 구성된 모델은 학습을 위한 그래디언트를 자동으로 계산한다. -&gt; 자동 미분
<br>그래디언트 : 벡터 미분의 결과 (=함수의 기울기, 각 변수에 대한 변화율)
<br>선배님의 프로젝트<br>
<br><a data-tooltip-position="top" aria-label="https://github.com/b-re-w/2024-1_BPL_STalk_Model_Research" rel="noopener nofollow" class="external-link" href="https://github.com/b-re-w/2024-1_BPL_STalk_Model_Research" target="_blank">2024-1_BPL_STalk_Model_Research</a>
<br>CNN 모델<br>
<br>2차원 데이터 (이미지 등)의 패턴을 인식하고 분석하는 데 사용되는 딥러닝 모델.
<br>여러개의 층으로 구성됨.
<br>인간의 시신경 구조를 모방한 구조임
<br>이미지의 
<br>RESNET 모델<br>
<br>CNN 모델에 잔차 연결 개념을 도입한 것.
<br>잔차 연결 : 각 층의 출력을 다음 층으로 직접 보내는 대신에, 입력을 더한 뒤 다음 층으로 전달하는 연결.
]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240707-모각코-활동-1회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240707 모각코 활동 1회차.md</guid><pubDate>Mon, 15 Jul 2024 07:48:10 GMT</pubDate></item><item><title><![CDATA[20240709 모각코 활동 2회차]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.파이토치 공부하기 - youtube에 있는 파이토치 설명 강좌(<a rel="noopener nofollow" class="external-link" href="https://youtube.com/playlist?list=PLS8gIc2q83Oit-utRso2iblvt00fZOw85&amp;si=i0CZi4e5g_dVJ3dx" target="_blank">https://youtube.com/playlist?list=PLS8gIc2q83Oit-utRso2iblvt00fZOw85&amp;si=i0CZi4e5g_dVJ3dx</a>) 1,2,3강 들으며 공부<br>
2.선배님의 프로젝트 코드 절반 분석하기 - whisper 부분<br>파이토치<br>
al분야에서 google tensorflow와 함께 딥러닝 모델을 구축하고 학습하는 데 가장 많이 사용되고 있는 오픈 소스 기반의 딥러닝 프레임워크임.<br>
<br>오픈소스 : 개방형 협업을 장려하는 소프트웨어 개발 모델
<br>프레임워크 : 소프트웨어 개발에 있어 하나의 뼈대와 같은 역할을 하는 것으로, 목적에 필요한 것을 고민할 필요 없이 이용할 수 있도록 일괄로 가져다 쓰도록 만들어 놓은 구조화된 틀임.<br>
텐서 : 파이토치의 기본 데이터 타입
<br>배열이나 행렬과 유사한 자료 구조이다
<br>일반적으로는 1차원 - 벡터 , 2차원 - 행렬, 3차원 이상 - 벡터 이지만, 파이토치에서는 입력과 출력 그리고 학습에 필요한모든 데이터들을 모두 텐서 데이터타입으로 정의하고 있다.
<br>텐서의 속성으로는 모양,자료형,저장되는 위치가 있다
<br>보통 저장되는 위치는 cpu인데, gpu를 사용할 수 있다면, .to("cuda")를 사용해서 텐서를 gpu로 이동시킬 수 있다.

<br>gpu : 컴퓨터 그래픽을 처리하는 장치로 그래픽 카드를 구성하는 가장 중요한 핵심 요소.


<br>1.파이썬의 리스트 데이터로부터 직접 텐서를 만들 수 있다.<br>
- listdata = [[10,20],[30,40]] 	tensor1 = torch.Tensor(listdata)`
<br>2.파이썬의 넘파이 데이터로부터 직접 텐서를 만들 수도 있다.(넘파이로만들어진건 보통 int로 생성되기때문에 원래 데이터가 float의 형태인 경우, 캐스팅해주는 작업이 필요하기도 하다.)
<br>3.파이썬의 랜덤 데이터로부터 직접 텐서를 만들 수도 있다.<br>
- tensor3 = torch.rand(2,2) -&gt; rand()메서드는 0~1사이의 균일 분포 랜덤값을 생성함 ( randn()메서드는 정규분포를 가지는 랜덤값을 생성 )
<br>텐서를 넘파이로 바꿀 수도 있다.<br>
- tensor.numpy()
<br>인덱싱과 슬라이싱이 가능하다
<br>elment-wise product 연산 
<br>matrix multiplication 연산 (행렬곱)
<br>텐서를 합칠 수 있다. Tensor Concatenate (dim=0 세로, dim=1 가로)<br>
파이토치 딥러닝 모델 구조 :<br>
1.데이터정의<br>
- 기본 데이터타입인 TENSOR로 생성해야함.<br>
- TensorDataset(x_train,y_train) : 텐서 데이터셋 생성<br>
- DataLoader(dataset, batch_size, shuffle) : 미니 배치 학습과 데이터 셔플, 멀티 프로세싱 등을 간단하게 수행할 수 있음.<br>
- 미니 배치 학습 : 전체 데이터를 n등분 하여 각각의 학습 데이터를 배치 방식으로 학습시키는 것.<br>
- 데이터 셔플 : train데이터와test데이터 간의 동일한 분포를 가지도록 섞어는 것.<br>
- 멀티 프로세싱 : 여러 작업을 별도의 프로세스를 생성 후 병렬처리를 하는 과정을 거치기 때문에 더 빠르게 결과를 얻을 수 있다.<br>
2.모델구축<br>
- nn.Module을 상속받는 class를 생성하여 정의하는 것이 일반적이다.<br>
- 클래스 속 __init__함수에서 계층(신경망 모델을 구성하는)을 정의.<br>
- 클래스 속 forward 함수에서 신경망에 데이터 전달하기를 수하고, 결과값을 리턴함<br>
3.피드포워드<br>
- 모델 학습을 위해서는 피드 포워드 계산값과 정답의 차이 계산이 필요  -&gt; 이 계산을 위해서는 손실함수와 옵티마이저가 필요함.<br>
- 손실함수 : MSE 등<br>
- 옵티마이저 : SDG, ADAM<br>
4.손실함수계산<br>
- nn.MSELoss(model(x_train),y_train) : 피드포워드 계산 값과 정답과의 오차 계산.<br>
- 이 때, model에 데이터를 전달하면 model 클래스 안에 있는 forward()함수자동으로 forward()함수를 호출하기 때문에 우리가 따로 호출해줄 필요가 없다.<br>
5.모델학습<br>
-역전파 코드 : 학습이 진행됨에 따라서 모델 파라미터(가중치와 바이어스)를 업데이트하면서 최적화 시킨다<br>
- optimizer.zero.grad()<br>
- loss.backward()<br>
- optimizer.step()<br>
- 모델(model) : 각 층을 포함하고 있는 인공신경망 그 자체 (이를 레고처럼 순차적으로 쌓기 -&gt; CNN, RNN 등 다양한 모델 구축 가능)<br>
- 3&gt;4&gt;5의 반복 -&gt; 딥러닝 학습<br>
- 손실함수가 최소가 될 때까지 모델 파라미터(가중치, 바이어스) 값을 찾아감.
<br>선배님 프로젝트 분석 - whisper<br>
1.from faster_whisper import WhisperModel<br>
2.def get_whisper() :  	 3.   model_size = "medium"  #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3'] 	 4.   compute_type = "int8"  #@param ['float16', 'int8']<br>
5.   return WhisperModel(model_size, device=DEVICE, cpu_threads=12, compute_type=compute_type).transcribe<br>1: faster_whisper 에서 WhisperModel 모듈 불러오기<br>
2: get_whisper 라는 이름의 함수 설정하기<br>
3: model_size는 "medium"이다. model_size가 가질 수 있는 옵션으로는 "tiny","base","small","medium","large","large-v3" 이 있다. -&gt; model_size는 모델의 크기를 뜻한다.<br>
4: compute_type은 "int8"이다. compute_type이 가질 수 있는 옵션으로는 "float16","int8"이 있다. -&gt; compute_type은 계산 유형을 뜻한다.<br>
5: WhisperModel은 4가지의 매개변수를 사용하는데, 여기에서 model_size는 앞서 정한 크기와 같고, device는 모델이 실행될 장치를 지정한다. cpu_threads는 CPU의 스레드 수를 뜻한다. compute_type또한 앞서 정한 계산 유형과 같다. 이 때 .transcribe는 모델의 음성 인식 기능을 호출해서 음성을 텍스트로 변환해준다.]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240709-모각코-활동-2회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240709 모각코 활동 2회차.md</guid><pubDate>Mon, 15 Jul 2024 06:27:57 GMT</pubDate></item><item><title><![CDATA[20240716 모각코 활동 3회차]]></title><description><![CDATA[ 
 <br>오늘의 목표<br>
1.파이토치 공부하기  - 실습해보기<br>
2.선배님의 프로젝트 코드 절반 분석하기 - resnet 부분<br>파이토치 실습<br>import torch #파이토치 불러오기
from torch import nn #토치에서 nn 불러오기
  

#텐서 형태로 train데이터 가져오기
x_train = torch.Tensor([1,2,3,4,5,6]).view(6,1)
y_train = torch.Tensor([3,6,9,12,15,18]).view(6,1)

  
#MyNeuralNetwork 클래스 만들기. nn.Module이 부모클래스가 됨.
class MyNeuralNetwork(nn.Module):
&nbsp; def __init__(self):
&nbsp; &nbsp; super().__init__()
&nbsp; &nbsp; self.linear_relu_stack = nn.Sequential(nn.Linear(1,1))

&nbsp; def forward(self, x):
&nbsp; &nbsp; logits = self.linear_relu_stack(x)
&nbsp; &nbsp; return logits


#모델
model = MyNeuralNetwork()
#손실함수
loss_function = nn.MSELoss()
#옵티마이저
optimizer = torch.optim.SGD(model.parameters(),lr=1e-2)

nums_epoch = 2000


#학습시키기
for epoch in range(nums_epoch + 1):
&nbsp; prediction = model(x_train)
&nbsp; loss = loss_function(prediction, y_train)

&nbsp; optimizer.zero_grad()
&nbsp; loss.backward()
&nbsp; optimizer.step()

&nbsp; if epoch % 100 == 0:
&nbsp; &nbsp; print('epoch = ', epoch, 'current loss = ', loss.item())
<br>#예측하기
x_test = torch.Tensor([8,9,10,11]).view(4,1)
pred = model(x_test)
pred
<br>선배님의 프로젝트 코드<br>from huggingface_hub import hf_hub_download
import wespeaker
<br>from huggingface_hub import hf_hub_download<br>
huggingface_hub 라이브러리를 통해서 hf_hub_download함수를 가져와준다.<br>
hf_hub_download함수를 통해서 모델을 다운로드 할 수 있다.<br>
기본적으로, 함수에는 repo_id와 repo_type을 인자로 넘겨준다. (revision - 특정 버전의 파일을 다운로드 하고 싶을 시. / local_dir 특정 위치에 저장하고 싶을 시.)<br>
import wespeaker<br>
wespeaker을 가져와준다.<br> def get_resnet152():
    model_id = "Wespeaker/wespeaker-voxceleb-resnet152-LM"
    model_name = model_id.replace("Wespeaker/wespeaker-", "").replace("-", "_")
 
    root_dir = hf_hub_download(model_id, filename=model_name+".onnx").replace(model_name+".onnx", "")

    import os
    if not os.path.isfile(root_dir+"avg_model.pt"):
        os.rename(hf_hub_download(model_id, filename=model_name+".pt"), root_dir+"avg_model.pt")
    if not os.path.isfile(root_dir+"config.yaml"):
        os.rename(hf_hub_download(model_id, filename=model_name+".yaml"), root_dir+"config.yaml")

    resnet = wespeaker.load_model_local(root_dir)

    #print("Compile model for the NPU")
    #resnet.model = intel_npu_acceleration_library.compile(resnet.model)

    def resnet152(ado, sample_rate=None):
        if isinstance(ado, str):
            return resnet.recognize(ado)
        else:
            return recognize(resnet, ado, sample_rate)

    resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)

    return resnet152
<br>분석<br>
def get_resnet152():<br>
get_resnet 152 라는 이름의 함수를 정의<br>model_id = "Wespeaker/wespeaker-voxceleb-resnet152-LM"<br>
model_id라는 변수에 "Wespeaker/wespeaker-voxceleb-resnet152-LM"를 지정. 아마  모델 아이디에 모델의 이름을 저장한 것일 것.<br>moldelname = model.id.replace("Wespeaker/wespeaker-",").replace("-", " ")<br>
model_name이라는 변수를 만들어서, model_id를 약간 변형시킨 이름으로 지정해줌. "voxceleb_resnet152_LM"이 될 것.<br>root_dir = hf_hub_download(model_id, filename = model_name+" .onnx").replace(model_name+" .onnx", "")<br>
hf_hub_download : huggingface_hub 라이브러리를 통해서 가져왔던 함수. 함수를 사용해서 모델 파일을 다운로드하고, 다운로드한 파일을 root_dir에 저장함.<br>import os<br>
os 모듈을 가져옴<br>
os 모듈 : 파일 및 디렉토리 작업, 프로세스 및 스레드 관리, 시스템 정보와 관련한 작업들을 수행할 수 있는 모듈이다.<br>if not os.path.isfile(root_dir+"avg_model.pt"):<br>
os.rename(hf_hub_download(model_id, filename=model_name+".pt"), root_dir+"avg_model.pt")<br>
만약 avg_model.pt이름을 가진 파일이 없다면, 모델의 pt파일을 다운로드 한 뒤 이름을 avg_model.pt로 바꾸어서 root_dir 변수에 저장함.<br>
os.path.isfile(path) : path가 파일인 경우 true를 리턴, 아니면 false를 리턴.<br>
os.rename : 파일 또는 폴더의 이름을 간단히 변경할 수 있다.<br>if not os.path.isfile(root_dir+"config.yaml"):<br>
os.rename(hf_hub_download(model_id, filename=model_name+".yaml"), root_dir+"config.yaml")<br>
앞 코드와 같은 느낌인데, 만약 config.yaml파일이 없으면 모델의 yaml파일을 다운로드 한 뒤 이름을 바꾸어서 root_dir변수에 저장함.<br>resnet = wespeaker.load_model_local(root_dir)<br>
resnet이라는 변수를 지정해줄건데, wespeaker 라이브러리의 load_model_local 함수를 사용할거임. 이 때 root_dir에 있는 파일들을 불러오게 됨.<br>def resnet152(ado, sample_rate=None):<br>
if isinstance(ado, str):<br>
return resnet.recognize(ado)<br>
else:<br>
return recognize(resnet, ado, sample_rate) 	 resnet152라는 함수를 정의해주는데, 이 함수는 입력으로 ado를 받음.<br>
instance(객체, 타입) : isinstance함수는 지정된 객체(여기에서는 ado)가 지정된 타입이면 true를 반환하고 아니면 false를 반환한다.<br>
ado가 문자열이라면  resnet.recognize(ado)를 리턴하고<br>
그렇지 않다면  recognize(resnet, ado, sample_rate)을 리턴함.<br>(recognize함수는 이전에 지정해둔 함수이다.)<br>def recognize(model, pcm, sample_rate):
    q = extract_embedding(model, pcm, sample_rate)
    best_score = 0.0
    best_name = ''
    for name, e in model.table.items():
        score = model.cosine_similarity(q, e)
        if best_score &lt; score:
            best_score = score
            best_name = name
        del score
        gc.collect()
    return {'name': best_name, 'confidence': best_score}
<br>resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)<br>
resnet152라는 함수에 register라는 기능을 추가(대체?)<br>
lambda함수를 통해서 resnet152에서 register메서드를 사용하려고 할 때, resnet객체의 register 메서드를 가져와서 사용하게 된다.<br>
args, kwargs : 몇 개의 인자를 받아야 할지 정할 수 없을 때 args와 kwargs(keyword arguments)를 파라미터로 써줌. args 앞에 붙는  * 는 여러개의 인자를 묶어서 하나의 튜플로 묶어주고 이를 args에 할당해준다. kwargs 앞에 붙는 ** 는 여러개의 키워드 아규먼트들을 묶어서 딕셔너리로 만들어준다. <br>return resnet152<br>
get_resnet152라는 함수는 resnet152를 반환함.<br>resnet152 = get_resnet152()
print("INFO: ResNet152 Ready -", resnet152)
<br>분석<br>
resnet152 = get_resnet152()<br>
get_resnet152함수를 가져와서 resnet152함수에 저장함<br>
print("INFO: ResNet152 Ready -", resnet152)<br>
모델이 준비되었다는 메시지를 출력한 뒤, resnet152를 출력함.]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240716-모각코-활동-3회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240716 모각코 활동 3회차.md</guid><pubDate>Tue, 30 Jul 2024 04:01:59 GMT</pubDate></item><item><title><![CDATA[20240723 모각코 활동 4회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
1.cnn공부 - 정의와 구조 살펴보기<br>
2.resnet공부 - 정의와 구조 살펴보기<br>딥 러닝 : 심층 신경망을 주로 다루는 ai분야. 심층 신경망은 신경망을 여러 계층으로 구성한 것.<br>
기존 신경망의 큰 단점 : 입력 데이터의 구조 고려 안 함. -&gt; 이미지와 같은 공간적 구조를 가지는 데이터 다루기 적합하지 않음.<br>
기존 신경망에서의 단점(공간적 구조 데이터 다루기 어려움)을 극복하기 위해 cnn 등장<br>CNN<br>
합성곱 신경망<br>
-2차원 구조를 고려하는 신경망<br>
-가중치와 바이어스로 이루어진 뉴런으로 구성<br>
- 입력데이터를 받고, 처리한 후 특정한 결과를 출력함.<br>
-입력 계층에 들어온 미가공 이미지 데이터에 해당하는 클래스를 예측하는 것이 목적.<br>
-예측된 클래스는 출력 계층의 결과 값 형태(클래스 점수 변환됨)로 출력됨.<br>
-계층의 종류<br>
1. 입력층<br>
- 미가공 이미지 데이터를 받음.<br>
2. 합성곱층<br>
- 합성곱 연산을 수행함.<br>
- 커널(n  m의 행렬)로 이미지(높이  너비)를 처음부터 끝까지 겹쳐 훑는다. 겹쳐지는 부분의 각 이미지와 원소의 값을 곱해서 모두 더한 값을 출력함.<br>
- 스트라이드 : 커널이 입력을 훑는데, 이 때의 보폭을 뜻함.<br>
- 이 때 출력되는 것(입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과)은 '출력 특성 맵(output feature map)' 이라 함.<br>
- CNN에서는 합성곱 계층의 입출력 데이터를 특성 맵(feature map) 이라 함.<br>
- <img alt="cnn 연산 방법" src="https://ejkiwi.github.io/lib/media/cnn1.png" referrerpolicy="no-referrer"><br>
3.ReLU층<br>
- 인공신경망에서 사용되는 활성화함수 f(x) = max(0, x) -&gt; 입력값이 0보다 크면 그 값을 그대로 출력하고, 0 이하면 0을 출력.<br>
4. 풀링층<br>
- 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄임.<br>
- 합성곱 연산과 유사함 (커널과 스트라이드 개념이 존재)<br>
- 최대풀링 : 커널과 겹치는 영역 안에서 최대값을 추출<br>
- 평균풀링 : 커널과 겹치는 영역 안에서 평균값을 추출<br>RESNET<br>
CNN의 한 종류<br>
-신경망의 깊이가 깊어짐에 따라 발생하는 훈련 문제를 해결하기 위해 '잔여학습'이라는 개념을 도입함.<br>
- 훈련 문제 : 기존 모델들은 레이어를 깊게 쌓을수록 더 성능이 좋아질 것이라고 예상했지만 실제로는 20층 이상부터 성능이 낮아지는 현상이 발생.<br>
- 잔여학습 : 스킵연결(입력값이 일정층들을 건너뛰어서 출력에 더할 수 있게 하는 역할) -&gt; 기존신경망은 k번째 층과 (i+1)번째 층의 연결로 이루어져있는데, resnet은 (i+r)층의 연결을 허용(shortcut connection).<br>
- <img alt="ResNet 구조" src="https://ejkiwi.github.io/lib/media/resnet.png" referrerpolicy="no-referrer"><br>
-최대 152개 층까지 쌓을 수 있게 됨.<br>
<img alt="cnn2" src="https://ejkiwi.github.io/lib/media/cnn2.png" referrerpolicy="no-referrer">]]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240723-모각코-활동-4회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240723 모각코 활동 4회차.md</guid><pubDate>Tue, 23 Jul 2024 13:38:23 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/cnn1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/cnn1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20240730 모각코 활동 5회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
1.cnn공부 - 합성곱 계층에서의filter, Padding에 대해 더 알아보기.<br>
2.resnet공부 - Residual Block과 Skip-Connection 에 대해 더 깊이 알아보기<br>합성곱 계층에서의 filter<br>
CNN에서 filter는 커널(n * m의 행렬)와 같은 의미이다. (mask라고도 불린다.)<br>
filter를 사용하는 이유는 사진에서 feature(특징)를 뽑아내기 위함이다.<br>
<br>입력 데이터의 전체 이미지에서, filter를 통해 천제 이미지를 순환하며, 특정 filter모양과 일치할수록 더 큰 값을 가지게 될 것인데, 이는 전체 이미지서 특정 filter와 유사한 모양을 가진 부분에 대한 feature들만 얻게 된다는 것을 의미한다. =&gt; 특정 filter에 부합하는 feature정보를 얻는 과정.
<br>Padding<br>
cnn구조에서, 합성곱층을 지나게 되면, 합성곱 연산으로 인해서 Feature Map의 크기는 입력데이터보다 크기가 작아지게 된다. 이렇게 크기가 작아지는것을 피하기 위해서 Padding 이라는 방법을 사용할 수 있다.<br>
<br>zero padding : 입력 데이터(이미지) 주위를 0으로 둘러주는 padding의 방법이다.<br>
<img alt="zero padding" src="https://ejkiwi.github.io/lib/media/zero_Padding.png" referrerpolicy="no-referrer">

<br>P : padding layer의 수
<br>n : 이미지의 크기가 n * n
<br>f : 커널의 크기(filter의 크기)가  f * f
<br>(n+2p) * (n+2p) : 패딩된 이미지의 크기
<br>((n + 2p – f + 1) * (n + 2p – f + 1)) :  합성곱층을 지난 출력 이미지의 크기


<br>padding이 필요한 이유

<br>이미지 데이터의 축소를 막을 수 있다. -  여러번의 계산을 거쳐야 하는데 초반부터 이미지가 너무 작아져버린다면 학습을 별로 하지 못하고 끝나버릴 수 있기 때문에 padding을 통해 이미지의 크기를 조절해줘야한다.
<br>모서리에 있는 중요한 정보를 충분히 활용할 수 있다. - padding을 사용하지 않는 경우, 모서를 학습할 기회가 적어지게 된다. 만약 중요한 정보가 모서리쪽에 있다면, 모델의 성능이 떨어지기 때문에 padding을 사용하여 모서리의 정보들도 충분히 학습할 수 있도록 해주어야 한다.<br>
<img alt="패딩과 모서리~" src="https://ejkiwi.github.io/lib/media/CNN_Padding_Edge.png" referrerpolicy="no-referrer">


<br>Valid Padding과 Same Padding : 각각 순서대로 패딩하지 않는 것, 입력데이터와 출력데이터가 동일하도록 하는 패딩을 뜻한다.
<br>Residual Block 과 Skip-Connection<br>
Residual Block은 층이 깊어지더라도 성능이 뒤떨어지지 않게 하기 위해 제시된 것.<br>
Residual 은 "잔여" 라는 뜻을 가지고 있는데, x를 입력 H(x)를 x의 분포로 가정하면 residual은 최종으로 구하고자 하는 H(x)와 x의 차이로 볼 수 있다.<br>
즉, Residual = R(x) = H(x) - x 가 되며 H(x) = R(x) + x 로 정리가능하다. <br>
<br><img alt="residualblock" src="https://ejkiwi.github.io/lib/media/residual%20block.png" referrerpolicy="no-referrer">
<br>위 신경망층에서는 F(x)r가 R(x)의 역할을 하기 때문에 Residual Block이라 불리게 된다.<br>
Residual Block은 그레디언트 소실 문제를 약화시키고, 이에 따라 신경망의 깊이가 깊어져도 성능이 떨어지지 않게 되는 것.
<br>그레디언트 소실 문제<br>
- 신경망을 학습시는 과정에서 -&gt; 역전파 알고리즘을 통해 출력층에서 입력층으로 손실함수에 대한 그레디언트를 전파하고, 경사 하강법을 통해 이 그레디언트를 사용하여 각 파라미터를 수정하는 단계를 거치게 됨.<br>
-  이 때 신경망의 하위층으로 진행될수록 그레디언트가 점점 작아지게 되는 문제가 그레디언트 소실 문제이다.<br>
residual block에서는 x, x+1, x+2 층이 있다고 할 때, x+2층은 x+1층뿐만 아니라 x로부터도 정보를 받을 수 있게 된다. 따라서 역전파 알고리즘이 실행될 때 그레디언트가 작아지는것을 어느정도 막아주는 효과가 발생한다.<br>
이러한 residual block의 방식을 하나의 합성곱층을 기준으로 살펴보았을 때,<br>
한 층의 입력값을 출력값과 합쳐서 다음 층으로 넘겨주는 방식이 그 층의 입력값이 해당 층을 통과하지 않고 다음 층으로 넘어가는 것과 같기 때문에 Skip Connection이라 부르게 되는 것이다.<br>
즉, Residual Block의 핵심은 Skip Connection이라 할 수 있다.
]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240730-모각코-활동-5회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240730 모각코 활동 5회차.md</guid><pubDate>Wed, 31 Jul 2024 12:40:41 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/zero_Padding.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/zero_Padding.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20240806 모각코 활동 6회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
CNN 실습 - MNIST 이미지 분류 ( RESNET은 7회차에 진행할 예정 )<br>import torch #pytorch 가져오기
<br>데이터 가져오기<br>#데이터셋불러오고 텐서로 바꿔주기

from torchvision import datasets #데이터셋 불러오고

from torchvision.transforms import ToTensor #텐서로 바꿔주기

  

#datasets에서 MNIST 가져와서 훈련데이터와 테스트데이터 가져와주기.

#datasets.MNIST(root - 데이터가 저장될 경로, train - train이 true 이면 train data이고 false면 test data, download - 데이터 없으면 인터넷에서 다운로드해줌 , transform - transform을 ToTensor로 지정해주지 않으면 텐서의 형식이 아닌, PIL이미지로 데이터가 가져와지게 된다)

  

train_data = datasets.MNIST(

&nbsp; &nbsp; root = "data",

&nbsp; &nbsp; train = True, #train data를 다운로드

&nbsp; &nbsp; transform = ToTensor(),

&nbsp; &nbsp; download = True

)

test_data = datasets.MNIST(

&nbsp; &nbsp; root = 'data',

&nbsp; &nbsp; train = False, #test data를 다운로드

&nbsp; &nbsp; transform = ToTensor()

)
<br>데이터 확인하기<br>#학습데이터 확인

print(train_data)

print(train_data.data.size())

# 데이터셋의 이름은 MNIST

# 데이터의 수는 60000개

# 훈련데이터

# StandardTransform(데이터셋에 일관되게 적용되는 변환의 표준을 정의) -&gt; Transform: ToTensor() #이미지 데이터들을 모두 일관되게 텐서 형태로 변환하겠다는 것을 의미.


#테스트데이터 확인

print(test_data)

print(test_data.data.size())

#데이터의 수가 10000 인 것과 테스트데이터라는 것을 제외하면 나머지 속성은 학습데이터와 동일함.
<br>#데이터 시각적으로 확인

import matplotlib.pyplot as plt #시각적 확인을 위해 matplotlib을 사용.

fig, ax = plt.subplots() # fig -&gt; 데이터가 담기는 프레임 / ax -&gt; 실제 데이터가 그려지는 캔버스

ax.imshow(train_data.data[0], cmap='gray') #데이터의 모습



#이미지 위에 각 픽셀 값을 표시해서 나타내보기

for i in range(train_data.data[0].shape[0]): # i와j는 텍스트를 표시할 위치를 지정하기 위함.

&nbsp; for j in range(train_data.data[0].shape[1]):

&nbsp; &nbsp; c = 1 if train_data.data[0][i, j].item() &lt; 125 else 0 # 이미지의 각 픽셀 값( train_data.data[0][i,j].item() )이 125보다 작으면 c = 1 흰색을 사용, 크면 c = 0 검정 사용.

&nbsp; &nbsp; ax.text(j, i, str(train_data.data[0][i, j].item()), color=(c, c, c), ha='center', va='center', fontsize=5) # text()를 사용하여 이미지 위에 텍스트 그리기

  

plt.title("%i" % train_data.targets[0])

plt.show
<br><img alt="mnist_1" src="https://ejkiwi.github.io/lib/media/MNIST.png" referrerpolicy="no-referrer"><br>데이터 준비하기<br>from torch.utils.data import DataLoader
# DataLoader -&gt; &nbsp;데이터를 미니배치 형태로 만들어서 우리가 실제로 학습할 때 이용할 수 있도록 함.
#DataLoader(dataset 데이터 , batch_size=1 한 번의 배치 안에 있는 샘플 사이즈, shuffle=False 데이터셋을 섞어서 데이터가 학습되는 순서를 바꿈, num_workers=0 동시에 처리하는 프로세서의 수. 하나 더 추가하면 20%정도 속도가 빨라짐.)
#배치 학습 -&gt; 전체 데이터를 n등분 하여 학습.

loaders = {

&nbsp; &nbsp; 'train' : torch.utils.data.DataLoader(train_data,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; batch_size=100,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; shuffle=True,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; num_workers=1),

&nbsp; &nbsp; 'test' : torch.utils.data.DataLoader(test_data,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=100,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;shuffle=True,

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num_workers=1)

}

loaders
<br>CNN 모델 설정하기<br>class CNN(torch.nn.Module):

  

&nbsp; def __init__(self):

&nbsp; &nbsp; super(CNN, self).__init__()

&nbsp; &nbsp; self.layer1 = torch.nn.Sequential(

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), #컨볼루션 레이어(합성곱층) #1차원(1개채널) 데이터를 받아 16개의 feature(16개의채널)로 나누겟다!!임.

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.ReLU(), #ReLU층

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.MaxPool2d(kernel_size=2, stride=2)) #풀링층

&nbsp; &nbsp; self.layer2 = torch.nn.Sequential(

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.ReLU(),

&nbsp; &nbsp; &nbsp; &nbsp; torch.nn.MaxPool2d(kernel_size=2, stride=2))
&nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; # layer 1, layer2 층까지는 이미지를 형상으로 분할하고 분석하는 부분
&nbsp; &nbsp; &nbsp; &nbsp; # 다음 fc 층에서는 이미지를 분류 예측하는 부분.
&nbsp; &nbsp; &nbsp; &nbsp; 

&nbsp; &nbsp; self.fc = torch.nn.Linear(32 * 7 * 7, 10, bias=True) #32*7*7만큼의 입력을 linear레이어에 의해 계산되게 해서... 10개의 출력( MNIST 이미지를 0부터 9까지 분류해야하기때문 )이 나오도록 함.

&nbsp; &nbsp; torch.nn.init.xavier_uniform_(self.fc.weight) # 신경망의 가중치를 초기화 ( 신경망의 가중치를 학습 전에 적절한 값으로 설정하는 과정 )



&nbsp; &nbsp; # __init__에서는 필요한 레이어들을 정의내렸다고 볼 수 있음.
&nbsp; &nbsp; # 아래 forward(얘가 실제적인 모델의 형태가 됨)에서 사용한다.

&nbsp; def forward(self, x): #순전파 #순전파만 지정해주어도 pytorch에서는 역전파 과정을 매우 쉽게 할 수 있도록 해준다.

&nbsp; &nbsp; out = self.layer1(x)

&nbsp; &nbsp; out = self.layer2(out)

&nbsp; &nbsp; out = out.view(out.size(0), -1) #  view() 함수는 텐서의 크기를 변경하는 데 사용 # 데이터를 완전 연결(fc) 층에 전달하기 위해 2차원 또는 3차원 텐서를 1차원 벡터로 평탄화 하는 과정이 필요함.
&nbsp; &nbsp; out = self.fc(out)

&nbsp; &nbsp; return out
<br>model = CNN()

model
<br>CNN(<br>
(layer1): Sequential(<br>
(0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br>
(1): ReLU()<br>
(2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>
)<br>
(layer2): Sequential(<br>
(0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))<br>
(1): ReLU()<br>
(2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)<br>
)<br>
(fc): Linear(in_features=1568, out_features=10, bias=True)<br>
)<br><br>학습하기<br>learning_rate = 0.01 # 파라미터를 얼마나 업데이트할 것인지를 결정. 학습률, step size. 너무 크지도 작지도 않아야 함.

loss_func = torch.nn.CrossEntropyLoss() # 모델 예측과 실제값 간의 차이를 측정하는 손실함수.

optimizer = &nbsp;torch.optim.Adam(model.parameters(), lr=learning_rate) # 손실함수를 통해 나온 을 최소화하기 위해 가중치를 업데이트하는 방법

training_epochs = 10 # 전체 데이터셋을 몇 번 반복할 것인지 결정.
<br># 반복의 횟수는 epoch과 batch의 크기에 따라 결정

total_batch = len(loaders['train'])

for epoch in range(training_epochs):

&nbsp; avg_cost = 0

&nbsp; for X, Y in loaders['train']:

&nbsp; &nbsp; optimizer.zero_grad() # 학습에서, 역전파를 거칠 때 마다 각 .grad 값에 변화도가 저장이 되는데,  이어지는 다음 학습에서 .grad의 값을 0으로 초기화시켜주지 않으면 이전에 저장된 변화도 값이 다음 학습에 영향을 주기 때문에 원하는 방향으로 학습하기 힘들다. 그래서zero_grad를 통해 .grad 의 값들을 0으로 초기화시켜준다.

&nbsp; &nbsp; pred = model(X) #순전파

&nbsp; &nbsp; cost = loss_func(pred, Y) #손실함수계산

&nbsp; &nbsp; cost.backward() #역전파

&nbsp; &nbsp; optimizer.step() # 역전파 단계에서 수집된 변화도로 매개변수를 조정

  

&nbsp; &nbsp; avg_cost += cost / total_batch

  

&nbsp; print('[Epoch: {:&gt;4}] cost = {:&gt;.9}'.format(epoch + 1, avg_cost))
&nbsp; # `epoch + 1` 값을 최소 4칸의 너비로 오른쪽 정렬하여 출력
&nbsp; # `avg_cost` 값을 최소 9자리까지 나타내어 오른쪽 정렬하여 출력

print('Learning Finished....&gt;_&lt;')
<br>[Epoch: 1] cost = 0.0461711548<br>
[Epoch: 2] cost = 0.0472225286<br>
[Epoch: 3] cost = 0.0413064063<br>
[Epoch: 4] cost = 0.0417594947<br>
[Epoch: 5] cost = 0.0395734794<br>
[Epoch: 6] cost = 0.0441303253<br>
[Epoch: 7] cost = 0.0408433564<br>
[Epoch: 8] cost = 0.043582622<br>
[Epoch: 9] cost = 0.0441764817<br>
[Epoch: 10] cost = 0.0412645154<br>
Learning Finished....&gt;_&lt;]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240806-모각코-활동-6회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240806 모각코 활동 6회차.md</guid><pubDate>Wed, 07 Aug 2024 13:56:13 GMT</pubDate><enclosure url="https://ejkiwi.github.io/lib/media/MNIST.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://ejkiwi.github.io/lib/media/MNIST.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[20240813 모각코 활동 7회차]]></title><description><![CDATA[ 
 <br>오늘의목표<br>
RESNET 실습 - CIFAR10 이미지 분류<br>
양자화 공부<br>#필요한 모듈 불러오기

import torch
import torch.nn as nn #다양한 종류의 레이어 제공 -&gt; 모델 만들기 도우미!
import torch.nn.functional as F #활성화 함수, 손실함수 등을 함수 형태로 제공.
import torch.backends.cudnn as cudnn
<br>모델링<br>#BasicBlock 클래스 정의  
  
class BasicBlock(nn.Module): # nn.Module 상속받기  
    def __init__(self, in_planes, planes, stride = 1):  
        super(BasicBlock, self).__init__() #BasicBlock의 부모클래스인 nn.Module의 __init__함수를 먼저 호출해서 사용.  
                  
        #conv1과 conv2 설정  
        #2D 컨볼루션 레이어 설정  
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size = 3, stride = stride, padding = 1, bias = False) # in_planes 입력채널 수 / planes 출력채널 수 / kernel_size 3*3 필터(커널) 사용 / stride (커널로 훑을 때의 보폭) 기본값은 1 / padding 패딩의 크기 1 / bias = False 바이어스(출력값을 조절하기 위해 사용되는  값) 를 사용하지 않겠다. -&gt; 바로 다음 줄의 코드(배치정규화)에서 바이어스의 역할을 해주기 때문에 여기에선 사용하지 않는다.  
        #배치 정규화 설정  
        self.bn1 = nn.BatchNorm2d(planes) # planes 배치정규화를 적용할 채널의 수. 앞의 출력 채널의 수와 동일해야함(당연함)  
  
        #2D 컨볼루션 레이어 설정  
        self.conv2 = nn.Conv2d(planes, planes, kernel_size = 3, stride = 1, padding = 1, bias = False)  
        #배치 정규화 설정  
        self.bn2 = nn.BatchNorm2d(planes)  
                    
# shortcut 설정 -&gt; `H(x) = R(x) + x`에서의 x를 위한 작업  
        self.shortcut = nn.Sequential() # nn.Sequential : pytorch에서 여러 레이어들을 순서대로 쌓을 때 사용하는 도구 # x를 그대로 더할 수 있는 경우  
        if stride != 1: #stride의 값이 1인경우(입력과 출력의 채널 수가 다른 경우 = x를 그대로 더할 수 없는 경우)   
self.shortcut = nn.Sequential(  
                nn.Conv2d(in_planes, planes, kernel_size = 1, stride = stride, bias = False),  
                nn.BatchNorm2d(planes)  
            ) # nn.Sequential을 사용해서 Conv2d와 BatchNorm레이어들을 이어줬음  
                  
#순전파 함수 # __init__에서 설정해뒀던 거 실제로 사용하는 부분.  
    def forward(self,x):  
        out = F.relu(self.bn1(self.conv1(x))) #conv1 거치고, relu함수 거치기  
        out = self.bn2(self.conv2(out)) #그다음 conv2 거치기  
        out += self.shortcut(x) # resnet의 핵심인 skip connection : H(x) = R(x) + x
<br>#ResNet 클래스 정의  
class ResNet(nn.Module):  
    def __init__(self, block, num_blocks, num_classes = 10):  
        super(ResNet, self).__init__() #ResNet의 부모클래스인 nn.Module의 __init__함수를 먼저 호출해서 사용.  
        self.in_planes = 64 # 입력 채널 수 64        # 2D 컨볼루션레이어 설정  
        self.conv1 = nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1, bias = False) # 입력채널 수 3 / 출력채널 수 64 / kernel_size 3*3 필터(커널) 사용 / stride (커널로 훑을 때의 보폭) 1 / padding 패딩의 크기 1 / bias = False 바이어스(출력값을 조절하기 위해 사용되는  값) 를 사용하지 않겠다.  
        # 배치정규화 설정  
        self.bn1 = nn.BatchNorm2d(64) # 배치정규화를 위해 사용할 채널 수 = 이전 채널에서의 출력 채널 수 = 64        # 레이어블록 설정(각 블록은 앞서 정의한 BASIC BLOCK으로 구성될거임. 인자 block 자리에, BasicBlock이 들어갈거니까아아아~~)  
        # _make_layer() : (블록의 종류, 출력 채널 수, 쌓을 블럭의 수, 레이어의 첫 블럭에서 사용할 stride의 값)  
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride = 1) #  
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride = 2)  
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride = 2)  
        # self._make_layer()에서 self는 현재 클래스의 인스턴스를 가리킴.  
        # 클래스 예측값 계산  
        self.linear = nn.Linear(512, num_classes) # 입력 채널 수 512, 출력 채널 수 num_classes        # _make_layer 함수 설정  
    def _make_layer(self, block, planes, num_blocks, stride):  
        strides = [stride] + [1] * (num_blocks -1) # stride 값 설정 # 첫 번째 블록의 stride는 지정된 값을 사용하고 이후 블럭들은 stride = 1이 된다.  
        layers = [] # 블럭을 담을 빈 리스트 생성  
        for stride in strides:  
            layers.append(block(self.in_planes, planes, stride)) # 입력 채널 수 self.in_planes, 출력 채널 수 planes, 스트라이드 값 stride            self.in_planes = planes # 채널 수 변경해주기(다음 레이어를 위해)  
        return nn.Sequential(*layers) # 생성한 블록들을 하나의 레이어로 묶어서 반환.  
    # 순전파 함수 # __init__ 설정해뒀던거랑 _make_layer 함수 만든 거 실제로 사용하는 부분.  
    def forward(self, x):  
        out = F.relu(self.bn1(self.conv1(x)))  
        out = self.layer1(out)  
        out = self.layer2(out)  
        out = self.layer3(out)  
        out = self.layer4(out)  
        out = F.avg_pool2d(out, 4) # 풀링층  
        out = out.view(out.size(0),-1) # 텐서의 차원 변경  
        out = self.linear(out) #완전 연결층  
        return out
<br># ResNet 18 함수 정의  
def ResNet18():  
    return ResNet(BasicBlock, [2,2,2,2])
<br>데이터 불러오기<br>import torchvision
import torchvision.transforms as transforms


transform_train = transforms.Compose([
&nbsp; &nbsp; transforms.RandomCrop(32, padding=4),
&nbsp; &nbsp; transforms.RandomHorizontalFlip(),
&nbsp; &nbsp; transforms.ToTensor(),
])


transform_test = transforms.Compose([
&nbsp; &nbsp; transforms.ToTensor(),
])

  
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)


train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)

<br>학습시키기<br>device = 'cuda'
net = ResNet18()
net = net.to(device)
learning_rate = 0.1
file_name = 'resnet18_cifar10.pth'
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)


def train(epoch):
&nbsp; &nbsp; print('\n[ Train epoch: %d ]' % epoch)
&nbsp; &nbsp; net.train()
&nbsp; &nbsp; train_loss = 0
&nbsp; &nbsp; correct = 0
&nbsp; &nbsp; total = 0

&nbsp; &nbsp; for batch_idx, (inputs, targets) in enumerate(train_loader):
&nbsp; &nbsp; &nbsp; &nbsp; inputs, targets = inputs.to(device), targets.to(device)
&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()

&nbsp; &nbsp; &nbsp; &nbsp; outputs = net(inputs)
&nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(outputs, targets)
&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()

&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()
&nbsp; &nbsp; &nbsp; &nbsp; train_loss += loss.item()
&nbsp; &nbsp; &nbsp; &nbsp; _, predicted = outputs.max(1)

&nbsp; &nbsp; &nbsp; &nbsp; total += targets.size(0)
&nbsp; &nbsp; &nbsp; &nbsp;  current_correct = predicted.eq(targets).sum().item()
&nbsp; &nbsp; &nbsp; &nbsp; correct += current_correct
&nbsp; &nbsp; &nbsp; &nbsp; if batch_idx % 100 == 0:

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print('\nCurrent batch:', str(batch_idx))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print('Current batch average train accuracy:', current_correct / targets.size(0))
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print('Current batch average train loss:', loss.item() / targets.size(0))

&nbsp; &nbsp; print('\nTotal average train accuarcy:', correct / total)
&nbsp; &nbsp; print('Total average train loss:', train_loss / total)


def test(epoch):
&nbsp; &nbsp; print('\n[ Test epoch: %d ]' % epoch)
&nbsp; &nbsp; net.eval()
&nbsp; &nbsp; loss = 0
&nbsp; &nbsp; correct = 0
&nbsp; &nbsp; total = 0

  
&nbsp; &nbsp; for batch_idx, (inputs, targets) in enumerate(test_loader):
&nbsp; &nbsp; &nbsp; &nbsp; inputs, targets = inputs.to(device), targets.to(device)
&nbsp; &nbsp; &nbsp; &nbsp; total += targets.size(0)


&nbsp; &nbsp; &nbsp; &nbsp; outputs = net(inputs)
&nbsp; &nbsp; &nbsp; &nbsp; loss += criterion(outputs, targets).item()


&nbsp; &nbsp; &nbsp; &nbsp; _, predicted = outputs.max(1)
&nbsp; &nbsp; &nbsp; &nbsp; correct += predicted.eq(targets).sum().item()


&nbsp; &nbsp; print('\nTotal average test accuarcy:', correct / total)
&nbsp; &nbsp; print('Total average test loss:', loss / total)


&nbsp; &nbsp; state = {
&nbsp; &nbsp; &nbsp; &nbsp; 'net': net.state_dict()
&nbsp; &nbsp; }
&nbsp; &nbsp; if not os.path.isdir('checkpoint'):
&nbsp; &nbsp; &nbsp; &nbsp; os.mkdir('checkpoint')
&nbsp; &nbsp; torch.save(state, './checkpoint/' + file_name)
&nbsp; &nbsp; print('Model Saved!')



import time

def adjust_learning_rate(optimizer, epoch):
&nbsp; &nbsp; lr = learning_rate
&nbsp; &nbsp; if epoch &gt;= 50:
&nbsp; &nbsp; &nbsp; &nbsp; lr /= 10
&nbsp; &nbsp; if epoch &gt;= 100:
&nbsp; &nbsp; &nbsp; &nbsp; lr /= 10
&nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; for param_group in optimizer.param_groups:
&nbsp; &nbsp; &nbsp; &nbsp; param_group['lr'] = lr
  
start_time = time.time()

for epoch in range(0, 150):
&nbsp; &nbsp; adjust_learning_rate(optimizer, epoch)
&nbsp; &nbsp; train(epoch)
&nbsp; &nbsp; test(epoch)
&nbsp; &nbsp; print('\nTime elapsed:', time.time() - start_time)

<br>양자화 공부<br>
양자화 : 실수형 변수(floating-point type)를 정수형 변수(integer or fixed point)로 변환하는 과정<br>
양자화 하는 이유 : 인공지능 모델에 큰 비트수의 자료형을 사용 -&gt; 학습 과정에서 계산량과 필요한 메모리 크기 등이 커지게 됨. -&gt; 학습을 시키기 위해 많은 리소스가 필요해지고, 추론도 오래 걸리는 문제가 발생. 양자화를 통하여 효과적인 모델 최적화를 할 수 있는데, float 타입을 int형으로 줄이면서 용량을 줄일 수 있고 bit 수를 줄임으로써 계산 복잡도도 줄일 수 있음<br>
Pipeline<br>
-HuggingFace의 가장 기본 기능으로, 자연어 처리 작업, inference(추론)을 빠르게 할 수 있게 해준다.<br>
-(hugging face에 대한 내용은 처음 보낸 코랩 파일 가장 위에 있으니 더 알아보고싶으시면 참고하시면 됩니다!)<br>
-pretrained model(사전학습 모델)을 사용하는 가장 쉬운 방법.<br>
-사전학습모델이란 : 예를 들어 텍스트 유사도 예측 모델을 만들기 위해서, 감정 분석 문제를 학습했던 모델의 가중치를 활용하는 방법. 즉, 감정 분석 문제를 학습하면서 얻은 언어에 대한 이해를 텍스트 유사도 문제를 학습하는 데 활용하는 방식이다.<br>
pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, kwargs) 매개변수 설명**<br>
-task : 어떤 작업을 할것인가? -&gt; 여기에서는 'text-generation' 텍스트 생성 작업을 할거임. ( 그 외 question-answering, translation 등등이 있음 ) 이건 pipeline을 사용할 때 꼭 지정해주어야 함. 나머지것들은 기본으로 지정된 것들이 있기 때문에 따로 필요한 경우만 지정해주면 됨.<br>
-model : 어떤 모델을 사용할것인가? -&gt; 여기에서는 "meta-llama/Meta-Llama-3-8B-Instruct" 라는 hugging face에서 미리 가져온 모델을 사용.<br>
-device map : 모델이 어디서(GPU 또는 CPU) 실행되어야할까? -&gt; 여기에서는 "auto" 로, 현재 기기에서 사용가능한 장소를 자동으로 감지하고, GPU가 있다면 이를 우선적으로 사용<br>
-model_kwargs : 추가로 전달할 매개변수(예를 들어 특정 설정을 변경하는 경우 사용) -&gt; 여기에서는 {"quantization_config": quantization_config} 이라는 quantization(양자화) 에 대한 설정을 포함하구 있음.<br>#준비
!pip install bitsandbytes # 양자화 기법을 사용할 수 있게 해주는 파이썬 모듈 다운로드
!pip install -U bitsandbytes
from transformers import pipeline, BitsAndBytesConfig # BitsAndBytesConfig 허깅페이스에서 양자화를 위한 라이브러리

  

#허깅페이스 로그인("meta-llama/Meta-Llama-3-8B-Instruct"를 사용하기 위함)
from huggingface_hub import login
login("내 TOKEN")

  

#양자화 옵션 설정
#4bit로 되어있긴 하지만, 8bit도 가능.

quantization_config = BitsAndBytesConfig(load_in_4bit=True) &nbsp;# You can also try load_in_8bit
pipe = pipeline("text-generation", "meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", model_kwargs={"quantization_config": quantization_config})



#양자화 한 후 실행
chat = [
&nbsp; &nbsp; {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
&nbsp; &nbsp; {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
chat.append(
&nbsp; &nbsp; {"role": "user", "content": "Wait, what's so wild about soup cans?"}
)
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])

]]></description><link>https://ejkiwi.github.io/2024_여름_모각코/20240813-모각코-활동-7회차.html</link><guid isPermaLink="false">2024_여름_모각코/20240813 모각코 활동 7회차.md</guid><pubDate>Wed, 25 Sep 2024 07:11:10 GMT</pubDate></item></channel></rss>
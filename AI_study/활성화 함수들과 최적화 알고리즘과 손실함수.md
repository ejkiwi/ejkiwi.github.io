
# 활성화 함수
뉴런이 출력할 값을 변환 -> 비선형성 추가 -> 신경망은 복잡한 패턴 학습 가능
##### ==ReLU==
![ReLU1](https://ejkiwi.github.io/lib/media/ReLU1.png)
- 입력이 **0보다 크면 그대로 출력**, **0 이하면 0 출력**
- 비선형 함수로 딥러닝에서 가장 많이 사용됨
- Gradient Vanishing 문제 완화 → 역전파 시 기울기가 0이 되는 문제가 적음
- 계산량이 적음 → 곱셈 연산 없이 비교만 하면 됨!
- =================================
- 음수 입력은 **항상 0으로 출력**되기 때문에, 뉴런이 완전히 죽어버릴 수도 있음.
	- 이걸 해결하기 위해 변형된 ReLU들이 등장
		- Leaky ReLU : x<0일 때도 작은 기울기(α)를 유지.
			- 보통 α = 0.01 정도로 설정
			- 음수 영역을 조금이라도 학습할 수 있도록 함.
		- PReLU : α를 고정된 값이 아니라 학습 가능하게 만든 것

![ReLU2](https://ejkiwi.github.io/lib/media/ReLU2.png)

##### ==Sigmoid==
![sigmoid](https://ejkiwi.github.io/lib/media/sigmoid.png)
- **출력 범위: (0,1)**
    - 값이 0~1 사이로 정규화되어 확률처럼 해석할 수 있음
- 이진 분류(Binary Classification)에 사용 → YES/NO 같은 문제에서 확률을 예측할 때 유용
	- **값 하나를 입력받아서(x) 하나의 값만 출력** 
- ==================================
- Gradient Vanishing 문제
    - 시그모이드 함수는 미분하면 최대값이 0.25
    * **역전파(Backpropagation)애서 기울기를 전파하고, 이를 이용해서 가중치를 업데이트하는데, 기울기가 너무 작아지면 가중치가 거의 업데이트되지 않아서 학습이 잘 안됨.**
- 출력 값이 0이나 1에 가까워질수록 기울기가 0에 가까워짐
    - 네트워크가 깊어질수록 학습이 어려움 → 그래서 최근에는 거의 안 쓴다.



##### Softmax
![softmax](https://ejkiwi.github.io/lib/media/softmax.png)
- **시그모이드의 자식**
- **로지스틱 함수(시그모이드함수)의 다차원 일반화**
	- 다차원 일반화? : 여러개의 클래스에 적용 가능
	- 여러 **개의 입력값** (x1,x2,x3,...)을 받아서 **각 클래스별 확률을 출력**
	- 출력 값의 총합이 항상 1 → 확률 분포처럼 해석 가능
- **각 클래스에 대한 상대적인 확률 계산**
    - 값이 클수록 확률이 높게 나옴 (ex: 고양이, 개, 말 중에서 하나를 예측)
- 다중 클래스 분류(Multi-class Classification)에 사용
    - 여러 개의 클래스를 예측해야 할 때, 각각의 확률을 출력
- ==================================
- 한 클래스의 확률이 높아지면, 나머지 확률이 급격히 줄어듦
    - 큰 값이 들어오면 출력이 급격히 한쪽으로 치우칠 수 있음.
- Cross-Entropy Loss와 함께 사용해야 안정적인 학습 가능
    - 단순한 MSE 같은 손실 함수를 쓰면 학습이 어렵다.

![sigmoid_softmax](https://ejkiwi.github.io/lib/media/sigmoid_softmax.png)

##### Tanh
![Tanh1](https://ejkiwi.github.io/lib/media/Tanh1.png)
- **시그모이드의 자식**
- **로지스틱(시그모이드) 함수를 평행이동하고 상수 곱한것과 동일**
- **출력 범위: (-1,1)**
    - 시그모이드와 비슷하지만, 출력값이 **영점을 중심으로 대칭**
    - 평균이 0에 가까워서 Gradient Vanishing 문제 완화
- 은닉층에서 주로 사용됨
    - 시그모이드보다 더 나은 성능을 보임
- =================================
- 여전히 Gradient Vanishing 문제 존재
	- 값이 -1 또는 1에 가까워지면, 미분값이 0에 가까워짐
	- 깊은 네트워크에서는 사용하기 어려움 → 요즘은 ReLU 계열을 더 많이 쓴다.

![Tanh2](https://ejkiwi.github.io/lib/media/Tanh2.png)



# 최적화 알고리즘
**OPTIMIZER**
손실 최소화를 위해 가중치 업데이트를 하는 데 사용

##### ==SGD==
- 확률적 경사 하강법
- **가중치를 업데이트할 때, 전체 데이터가 아니라 일부만 사용**
    - 계산 속도를 빠르게 하기 위해 **배치(Batch) 단위로 업데이트**
- SGD는 랜덤 샘플을 사용하기 때문에 최적값 주변에서 흔들리는 경향이 있음
    - 이걸 해결하기 위해 Momentum, Nesterov Accelerated Gradient (NAG) 같은 기법 추가
- ⚠️
- 지역 최소값(Local Minima)에 빠질 수 있음
- 학습 속도가 느릴 수 있음
    - 기울기 방향이 항상 최선이 아니기 때문에 최적값에 도달하는데 오래 걸릴 수도 있음
- 진동(Loss값이 계속 크게 왔다 갔다 하는 현상) 발생

##### Momentum
- SGD 개선냥이
-  **속도(velocity) 개념을 도입**해서, 이전 기울기의 영향을 일부 유지
- β(보통 0.9) 값이 클수록 이전 방향을 더 강하게 유지
- 기울기가 갑자기 바뀌더라도 부드럽게 이동 가능 → 진동(oscillation) 감소
- ⚠️
- Overshooting 문제 : 너무 빠르게 이동가능해버려서 최적점을 지나칠수도있잖아! 라는 문제.
##### NAG
- Momentum 개선냥이
-  **미리 계산한 위치에서 기울기를 보고 더 정확한 방향으로 이동** ->  현 위치에서 앞으로의 위치도 미리 계산한 후 보정
- Overshooting 문제 개선
##### RMSprop
- SGD 개선냥이
- 각각의 파라미터에 대해 **적응적인 학습률(learning rate) 적용**
- 기울기가 크면 학습률을 줄이고, 기울기가 작으면 학습률을 증가
	- 약간의 브레이크와 엑셀 느낌임.
	- **학습률** ? : **가중치를 업데이트할 때 변화하는 크기**를 결정하는 **하이퍼파라미터**.
		- 모델이 얼마나 빠르게 또는 천천히 학습할지를 조절하는 중요한 요소
- Gradient Vanishing 문제를 해결하는 데 도움
##### ==Adam==
- **Momentum + RMSprop을 결합한 방식**
    - 모멘텀을 사용해서 빠르게 최적화하면서도, 적응적으로 학습률을 조정함
    - Momentum -> 빠르게 최적화 가능
    - RMSprop -> 적응적으로 학습률 조정
- **각각의 가중치에 대해 다른 학습률을 적용**
    - 변화가 큰 가중치는 학습률을 낮추고, 변화가 작은 가중치는 학습률을 높임
- SGD보다 빠르게 수렴할 가능성이 높음
    - 그래서 딥러닝에서 가장 많이 사용됨
- ⚠️
- 메모리를 많이 사용
    - 1차(moment)와 2차(moment) 모멘텀을 모두 저장해야 해서 SGD보다 메모리를 더 씀
- 일반화 성능이 떨어질 수 있다.
    - 너무 빠르게 수렴해서 오히려 최적해를 잘 못 찾을 수도 있음



# 손실함수
출력층에서 모델이 얼마나 잘못 예측했는지 평가하는 함수
모델의 예측값과 실제 정답(Label) 사이의 차이를 계산.
손실 함수의 값이 **클수록 예측이 많이 틀린 것**
손실 함수의 값이 **작을수록 예측이 잘 된 것

#### IN **회귀(Regression)**
##### MSE
- 평균 제곱 오차
- 예측값 y^와 실제값 y의 차이를 제곱한 후 평균을 구함.
- 오차가 클수록 더 큰 패널티를 줌 (제곱이기 때문)
- 미분이 부드럽고, 계산이 쉬움.
##### MAE
- 예측 값과 실제값 사이의 평균 절대 차이를 측정하는 방법
##### Huber Loss
- **mse + mae**
- 오차가 작을 때는 MSE처럼 동작하고,  
- 오차가 클 때는 MAE처럼 동작해서 이상치에 덜 민감
#### IN 분류(Classification)
##### Cross-Entropy Loss
##### Hinge Loss
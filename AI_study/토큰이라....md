Tokenization : 어디서 문장을 자를까? 정하기
- 모든 글자 하나 하나를 자르기
	- `run`, `running`, `runs` 같은 단어들은 `run`이라는 공통된 형태임을 안다
	- 새로운 단어 생겨도 알 수 잇잔냐
	- 원래보다 입력 길이가 길잖아
	- 단어를 한개한개 자르니까 단어 자체 의미를 바로 알기는 힘드러
	- 
- 단어 단위로 자르기
	- 단어 그 자체 의미가 보존되잖아
	- 입력 시퀀스도 짤밪ㄶ아 -> 계산량도 감소하구 속도도 빨라지구
	- 단어 개많은 거 어캐 처리할것인데?
	- 새로운 단어를 어캐 적응할건데?
		- -> ==UNK==
	- run runs running을 다 따로 해버리잖냐...

- 새 단어가 자주 등장하거나 다양한 입력을 다루려면 **Character** 단위가 좋고,  의미 단위를 명확히 다루고 싶으면 **Word** 단위가 좋은 것이다~~

UNK
- 한 번은 본 적 있는 단어라면 인덱스가 배정되어있을 것이야. 이녀석을 어떻게 숫자로 표현해야할지 알 수 있을 것이라구.. 근데????  본 적 없는 단어는 뭐 알 수가 없잖냐!!!!!! 모델에 입력할수가 없다구.. 모델에 얘를 뭐 어캐 숫자로 바꾸어서 넣어줄건데!!!
	- -> 이 때는 `<UNK>`로 처리하면 되는 것이야.
	- 자주 나오지 않는 특별 단어는 요래 처리를 하면 돼. 그래서 이 입력 자체를 버리지 않구 따로 처리할 수 있게 해주는 것이야. 그럼 모델이 돌다가 멈추지 않겟지??
	- 보통`<UNK>`는 0과 1같은 특별한 ID 숫자 번호를 준댕
	- ㄱ근데 이 UNK가 너무 많아져버리면??
		- 이것이 이 UNK의 한계인것이야.
		- 이게 너무 많으면 문장 의미 자체를 이해할 수 없게 되잖야.
		- 어캐 해결해? ->==Subword Tokenization - Byte Pair Encoding== 이 등장!

Subword Tokenization
알 고 리 즘
희귀 단어처리 위해서요 - 자주 등장하는 글자 쌍을 합쳐나가기
1. 처음에는 character단위로 시작
2. 가장 많이 등장하는 글자 쌍 찾기
3. 쌍 합쳐서 새 심볼 만들기
4. 점점 더 큰 단위 만들기

~ 자연어를 컴퓨터에 넣으려면 전처리를 해야 하고, 그중 하나가 **Tokenization**이다.  
~ 초기에 대표적으로 **Word 단위 Tokenization**을 사용했다.  
~ 그런데 Word Tokenization 방식은 **UNK(모르는 단어)** 문제를 해결할 방법이 없었다.  
~ 그래서 **Character-level Tokenization**도 시도했지만,  ➔ 글자 단위는 **의미를 포착하기 어렵고** 시퀀스가 너무 길어져서 힘들었다.
~ 이 문제를 궁극적으로 해결하려고,   **"Subword Tokenization"이라는 새로운 방식**이 등장했다!
~  그래서,  Subword Tokenization은 단순히 Word Tokenization에 딸려 있는 세부기법이 아니고, Word 방식, Character 방식과 나란히 독립적으로 존재하는 하나의 Tokenization 방법이다. 

근데 이거 잘 쓸라면 첫 base vocab에 초ㅣ대한 다양한 글자가 이성야 할 거 아님?
근데 모든 unicode 다 넣기에는 개심각. 십삼만팔천개임@@@ 도롸이...
그래서 지피티니 투 같은 경우에는 처음 시작을 256개부터 시작해서... 현재는  삼만이천개~ 육만사천개 정도의 base vocab 임...

자 근데 이것을.... 숫자로 바까야할것아님..
- 처음에는... 단어마다 고유 ID 배정하는 원 핫 벡터 방식을 사용함. ( 희소 표현 )
    - 30,000차원 벡터 중 3456번째만 1, 나머지는 0
        
    - 너무 비효율적! (메모리 낭비 + 의미 없음)
- 그래서 그것 대신에==Dense Vector(Word Embedding)==로 변환하자는 아이디어가 생겻잖냐
    - "banking" →` [0.12, -0.48, 0.23, ..., 0.04]` (예: 768차원)
	- 이 Vector가 바로 Word Vector(Word Embedding)
    - 밀집되어 있고 (Dense)
    - 의미를 담고 있다 (비슷한 의미면 가까운 벡터!)
    - word2vec
	    - word embedding 구현 방법 중 하나.
	    - banking이라는 단어가 있으면, 그 단어 혼자 의미를 가지는 게 아니라, **"항상 banking 근처에 어떤 단어들이 등장했는가"** 를 보고 그 패턴을 벡터에 새긴다라는 마인드를 가진 방법이다...!!
	    - ![word2vec](https://ejkiwi.github.io/lib/media/word2vec.png)
	    - 저 사진에서.**P(w_문맥어(주변단어) | w_중심어)** 간의 확률을==학습==하는 과정 안에서 단어의 의미 벡터가 만들어 진 것.
	    - 주변 문맥으로 학습한 결과 => 단어 의미하는 의미 벡터 ( 얘는 주변 문맥을 잘 예측하는 방향으로 조정되었음. )
	    - ==요즘에는 거의 Embedding을 학습하는 게 모델 학습 그 자체로서의 역할을 하도록 다른 모델에 포함시켜서 사용된다...!("**Word2Vec은 단어를 의미 있는 벡터로 만드는 임베딩 학습기법이고, 이 결과를 다른 NLP Task를 수행하는 데 이용한다! -> 지도학습! downstream task개념 기억나지?? downstream을 수행하기 위해서 다른 걸로 돌려버리잔아...!!!! **")==
	    - 


# 근데!!!!!!!!!!!! 단어의 순서는 어캐할것이야!!!
물론! rnn이면 시간 순서를 기억하니 ㄱㅊ 할지도.. 긏만???
self-attention과 transformer는 어캐할것이냐??
이녀석들은 모든것들을 한 번 에!!! 처리를한다고!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

그러면 뭐 apple이 love 앞ㅍ에나오ㅑㅅ는디뒤에나ㅗ왓는디저어캐아냐구

==그래서!! token의 위치 정보 또한 숫자로 표현해서 데이터로 가지고 잇어야해!!!==
필요한 것
1. 연속적으로 자연스럽게 변하는 값이 필요 ( 그냥 숫자 1 2 3 주면 부드럽게 순서 관계를 인식하기 어려움 )
2. 멀어질수록 값이 달라지고 가까울수록 값이 비슷하게 나와야 함
![positional_encoding1](https://ejkiwi.github.io/lib/media/positional_encoding1.png)
- 짝수 차원에는 sin 함수를, 홀수 차원에는 cos 함수 사용.
- 잘 보면 sin과cos에 들어가는 녀석들이 매우 복잡함
	- 각 차원(frequency)이 다 다르게 움직이고,
	- Sin, Cos도 교차로 쓰고
	- 주기도 다르게 걸어놨음.
	- 갱장히..갱장히...머...어찌저찌... 저찌저ㅉㅣ 해서 멀어질수록 값이 달라지고 가까울수록 값이 비슷하게 나오도록 함수를 설계하셧대요 그 이상은... **이해포기**

![positional_encoding2](https://ejkiwi.github.io/lib/media/positional_encoding2.png)

"단어가 문장에서 나오는 순서" 그대로 Y축에 1줄씩 차곡차곡 올라간 것...
그리고 y 축에는 단어 순서에 대한 의미를 가진 벡터가 들어감....
벡터 1번째 칸~ 128번째 칸 까지

1. 단어를 의미 임베딩(Embedding)한다 (ex: students → `[벡터]`
2. 단어 순서(Positional Encoding) 벡터를 만든다 (0번 단어용 벡터, 1번 단어용 벡터 등)
3. 두 벡터를 더한다 (Element-wise Sum) 의미 + 순서 둘 다 반영된 최종 벡터 완성
4. Transformer Encoder로 넣는다|(Self-Attention 계산 시작!)


# rnn기반 모델에서의 예시

## batch 1 -> 하나의 문장
batch 1에서의 문장 하나
```
[
[[나:임베딩값],[는:임베딩값],[빵:임베딩값],[이다:임베딩값]]-> batch0
]
```
이 때의 shape : ( 1, 4, 임베딩dim  )

## batch n -> n개의 문장
batch n에서의 문장 n개
```
[
[[나:임베딩값],[는:임베딩값],[빵:임베딩값],[이다:임베딩값]], -> batch0
[[빵:임베딩값],[좋아:임베딩값],[pad],[pad]], -> batch1
[[너:임베딩값],[밥:임베딩값],[이니:임베딩값],[?:임베딩값]], -> batch2
.
.
.
[[아니:임베딩값],[빵:임베딩값],[이다:임베딩값],[!:임베딩값]] -> batch n
]
```
이 때의 shape : ( n, max_token:4, 임베딩dim )

## batch 1 -> 여러개의 문장?
```
[
[[나:임베딩값],[는:임베딩값],[빵:임베딩값],[이다:임베딩값],[sep],
[빵:임베딩값],[좋아:임베딩값],[pad],[pad],[sep],
[너:임베딩값],[밥:임베딩값],[이니:임베딩값],[?:임베딩값],[sep],
[아니:임베딩값],[빵:임베딩값],[이다:임베딩값],[!:임베딩값]],
]
```
sep토큰을 쓸 때는 sep토큰을 처리하고, 다음 문장의 첫 번째 토큰을 입력받기 직전에 은닉 상태를 초기화해야 함(두 번째 문장이 마치 배치를 사용하는 상황에서의 다른 문장처럼 독립적인 상태에서 시작할 수 있도록 하기 위함)

--------------------------------------------------------
* 문장의 길이를 seq_length라고 한다.
* 모델에 들어가는 아키텍처 기준으로는 max_token만큼의 길이가 들어감
* [ [빵], [좋아], [pad], [pad] ] : seq_length2 max_token4 
* 임베딩값 = [ 값1 , 값2 , .... , 값512 ]
* 각 토큰을 값 512개로 표현 ->embedding dim이 512
---

## nn.Linear(512,30000)
내 생각대로, 당연하게도, linear layer는 마지막 임베딩값 기준으로 계산되는것이 맞음.

**각 배치마다 하나의 문장 넣은 경우**
입력이 : ( 3, 4, 512 ) # 3개의 문장 # 각 4개토큰 # 각 토큰 512차원 표현
```
[
[[나:임베딩값],[는:임베딩값],[빵:임베딩값],[이다:임베딩값]], -> batch0
[[빵:임베딩값],[좋아:임베딩값],[pad],[pad]], -> batch1
[[너:임베딩값],[밥:임베딩값],[이니:임베딩값],[?:임베딩값]], -> batch2
]
batch 0의 토큰 0: [512차원] -> Linear -> [30000차원]
batch 0의 토큰 1: [512차원] -> Linear -> [30000차원]
batch 0의 토큰 2: [512차원] -> Linear -> [30000차원]
batch 0의 토큰 3: [512차원] -> Linear -> [30000차원]
batch 1의 토큰 0: [512차원] -> Linear -> [30000차원]
.
.
batch 2의 토큰 3: [512차원] -> Linear -> [30000차원]
```
... (총 3×4=12개 토큰 각각 처리-> 물론 미니 배치를 쓰면 각 배치n개를 한번에 처리할 수 있음.) 

**sep으로 여러개의 문장을 묶은 경우**
입력이 : ( 1, 14, 512 ) # 3개의 문장 # 각 4개토큰 # 각 토큰 512차원 표현
```
[
[[나:임베딩값],[는:임베딩값],[빵:임베딩값],[이다:임베딩값],[sep],
[빵:임베딩값],[좋아:임베딩값],[pad],[pad],[sep],
[너:임베딩값],[밥:임베딩값],[이니:임베딩값],[?:임베딩값]
]
토큰 0: [512차원] -> Linear -> [30000차원]
토큰 1: [512차원] -> Linear -> [30000차원]
.
.
토큰 14: [512차원] -> Linear -> [30000차원]
```
(총 1x(4+1sep+4+1sep+4)=14개 토큰 각각 처리) 

둘 다 각 토큰의 표현을 512차원에서 30000차원으로 늘렸음.

그리고 이 계산은 시간흐름 t마다 각 배치들이 모두 병렬 처리 됨.
t=1 -> 배치0의 첫번째 + 배치1의 첫번째 + 배치2의 첫번째
t=2 -> 배치0의 두번째 + 배치1의 두번째 + 배치2의 두번째
t=3 -> 배치0의 세번째 + 배치1의 세번째 + 배치2의 세번째




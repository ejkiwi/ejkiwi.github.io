### latent space( embedding space )
1. 그냥 수학적으로 정의된 좌표계... 긍까 빈 도화지 같은 것.
    - 학습을 통해 **가중치 행렬**이 “점(벡터)을 찍는 규칙”을 배워서, 그 규칙에 따라 latent space에 점을 찍을 수 있게 되는데,,,, 그렇게 해서?? 그 공간에 **의미 있는 분포**가 형성된다.
    - 우리가 보는 PCA, t-SNE 그림은 **고차원 latent space에 찍힌 점들을 2D/3D로 투영한 그림자**일 뿐, 원본 공간을 직접 볼 수는 없음.
        
2. Latent vector = hidden state = embedding
    - 모두 같은 성격의 값: 입력이 들어왔을 때 모델이 계산해내는 **고차원 벡터 표현**. ( 압축한 값 )
    - 이 벡터가 latent space 위의 **한 점(point)이 됨.
    - forward pass 중에 계산되며, 특별히 저장되지는 않고 다음 연산으로 넘어가 버림.
    - 우리가 시각화할 때는 이 중간 값을 **추출해서 저장**한 뒤 차원 축소 기법(PCA 등)으로 찍는 거고!!!
        
3. **가중치 행렬과의 관계**
    - 가중치 행렬 자체가 공간은 아님.
    - 대신 ==**입력을 latent space 안의 점으로 어디에 찍을지에 대한 규칙**==을 정의
    - 학습이란, 이 가중치 행렬들을 조정해서 공간 위 점들의 패턴이 의미 있게 되도록 만드는 과정이다.
        
4. 가중치 vs 파라미터
    - 파라미터 = 학습 가능한 값 전체 
    - 가중치는 파라미터에 속함

---

## 사영?

- 잠재 벡터의 차원 -> 잠재 공간에서 쓰는 축의 개수
- 2D latent space를 시각화했을 때, scatter plot의 한 점 한 점들이 2D latent vector에 해당한다.
- 사영은왜하는것이지?
	-> 우리 눈은 4차원부터는 그 공간을 시각화하여 표현하지 못한다.
	->  4, 5, 6 ,,,, 차원의 잠재 벡터로 표현되어있는 벡터 공간을 시각화해서 보고싶으면 어떡해~
	-> 그 때는 차원을 줄이면 되지! 고차원을 저차원(1,2,3차원)으로 내리 꽃는 그 무드임

---

## 탐색 공간
- 변수의 ==종류에== 따라 탐색 공간 정의 가능
- 변수의 ==개수에== 따라 탐색 공간 차원이 정해짐
- **이산 변수 (예: binary)**
    - 변수 하나 → 차원 1의 공간 예: {0, 1}
    - 변수 두 개 → 차원 2의 공간 예: {0, 1}² = {(0,0), (0,1), (1,0), (1,1)}
    - 변수 n개 → 차원 n의 이산 공간 예: {0, 1}ⁿ
        
- **연속 변수 (예: 실수)**
    - 변수 하나 → 차원 1의 공간 예: ℝ (실수직선)
    - 변수 두 개 → 차원 2의 공간 예: ℝ² (평면)
    - 변수 n개 → 차원 n의 연속 공간 예: ℝⁿ


---

## 단어 임베딩

- 단어를 숫자로 특징 표현.
- vector_size를 지정해 단어를 특정 차원의 의미 벡터로 표현.
- 예: `vector_size = 5`,  `"이"` → `[0.1, -0.3, 0.5, 0.0, 0.2]`

---

## 단어 예측(Language Modeling)의 구조

- n개의 단어 시퀀스를 입력으로 주고, 그 ==다음 단어를 예측하는 분류 문제==로 처리함.
  - 입력 예: `"이", "것", "은"`  ( 이 때 "이","것","은" 은 각각 5차원 벡터 -> 총` 3*5 `matrix 가 입력으로 들어감)
  - 출력 예: `"첫"`
- 출력은 **단어장 크기**만큼의 **확률 벡터** (예: softmax로 17차원 출력)
- 정답(GT)은 예측 단어가 단어장 내에서 위치하고 있는 인덱스 (예: `"첫"`은 7번 인덱스)
	- ==출력 시퀀스의 인덱스가 아님!!==
	- 정답 단어는 보통 정답 단어는 보통 **one-hot 벡터**로 표현되며, 실제 학습에서는 one-hot 벡터에서 1이 적혀있는 인덱스 번호만으로도 정답을 표현 함
- 예측 결과에 대한 loss 함수는 CrossEntropyLoss 사용 (softmax + log loss)

---

## 시작 위치 처리 (START 토큰)

- 문장 앞부분처럼 앞에 단어가 부족할 때는 `[START]` 같은 특수 토큰으로 채움
- 예: 시퀀스 길이 = 3이면  
  `[START], [START], "이"` → 예측: `"것"`

---

## 임베딩도 학습되는 파라미터!!

- 각 단어의 벡터는 모델이 학습 과정에서 계속 업데이트됨
- 즉, 임베딩은 단순히 고정된 변환이 아니라 의미를 학습하는 파라미터(weight matrix)임

---

## 벡터의 차원 수가 의미하는 바

- 차원이 클수록 더 미묘하고 복잡한 의미를 표현할 수 있지만,
- 연산량 증가, 메모리 부담, 학습 시간 증가 등의 trade-off가 있음
- 일반적으로:
  - Word2Vec: 100~300차원
  - BERT: 768차원
  - GPT-3: 최대 12,288차원까지도 사용

---

## 포지셔널 인코딩(Positional Encoding)

- Transformer는 단어 순서를 알 수 없기 때문에 위치 정보(Position)를 따로 넣어줘야 함
- 이 위치 정보는 단어 ==임베딩 벡터와 같은 차원==으로 만들어져 element-wise ==덧셈== 방식으로 추가함

예:  
"이"의 임베딩 = `[0.1, -0.3, 0.5, 0.0, 0.2]`  
위치 인코딩(Pos 0) = `[0.01, 0.02, 0.03, 0.04, 0.05]`  
→ 최종 입력 벡터 = `[0.11, -0.28, 0.53, 0.04, 0.25]`

- 벡터 차원 수는 그대로 유지되며, 단어 의미 + 위치 정보가 하나의 벡터에 담김

---

## 임베딩 벡터의 각 차원의 의미는 해석 불가

- 벡터 전체는 의미를 잘 반영하지만,
- 각 인덱스가 “성별”이나 “감정” 같은 특정 의미를 갖는 건 보장되지 않음
- 이 차원별 의미를 해석하려는 것이 현재 활발한 연구 주제 중 하나임  
  - 예: probing task, interpretable embeddings, disentangled representations 등

---
## GPT 와 BERT
### TRANSFORMER

|구성 요소|하는 일|
|---|---|
|**Encoder**|전체 입력을 한 번에 다 보고, 이해하는 역할|
|**Decoder**|입력 내용을 바탕으로, **출력을 한 단어씩 생성**하는 역할|

### GPT
**Decoder만 사용**
- Encoder는 전체 입력을 한꺼번에 다 보고 처리 -> gpt는 아니다~
- 앞 단어들만 보면서 오른쪽 단어를 예측하는! 순서대로 단어 예측하는 녀석

**gpt의 출력은 ==다음 단어를 예측하는 Vocab size만큼의 확률 분포**==
- (예: '것' 다음에 나올 단어가 '은'일 확률 등등)
- "나는 그것"까지 입력 -> 그러면 GPT는 "**그 다음 단어**가 뭐일지" 예측해야 함.
	- 이 때, GPT는 자기가 알고 있는 **모든 단어(=어휘집, Vocabulary)** 중에서~~ "것", "은", "이다", "합니다", "좋아해요" 같은 **모든 후보 단어에 대해 확률**을 계산해.
**출력을 Linear를 통해 원하는 차원으로 맞추어 줌**
- Transformer는 결국 숫자 벡터(예: 768차원)를 출력해줌 : 복잡한 의미를 담은 벡터를 뽑아줄 수는 있지만?
- 그러나? 우리가 원하는 건...
	- (예)**Classification**: 단일 문장 분류
	- (예)**Entailment**: 전제 → 가설 관계 추론
	- (예)**Similarity**: 두 문장의 유사도
	- (예)**Multiple Choice**: 지문과 보기들 중 정답 고르기
	- => 요런것들이기 때문에, 그 복잡한 벡터를 우리가 원하는 형태, 즉 정답 클래스 수로 가공해줘야한다!
		- => Linear Layer 선형층
특수토큰
- 기계 번역![GPT1](https://ejkiwi.github.io/lib/media/GPT1.png)
- Decoder만 쓰는 GPT가 어캐 번역을 하지.. 라고 생각할 수 있겟으나!! 입력 안에 번역해줘~ 라는 토큰이 들어온다면 가능할 수 잇다아~
- Auto-regressive generation
	- how are you `<to-fr>` : 입력 문장 (마지막 토큰`<to-fr>`이 작업 명령임)
	- Transformer-Decoder : 예측을 계산하는 모델 본체
	1. Time step #1
		- 지금까지의 입력을 보고, 첫 번째 단어의 위치인 Position 4 (즉, `<to-fr>` 바로 다음 위치)에 해당하는 단어를 예측.
		- "Comment"라는 단어 출력
	2. Time step #2
		- 현재 time step #1 에 해당하는 "Comment"까지 기억하고 있음.
		- 그 다음 "allez-vous" 단어 예측, 출력

- 요약![GPT2](https://ejkiwi.github.io/lib/media/GPT2.png)
- 입력 : 긴 문서 내용 (예: 위키피디아 문장들) + **`<summarize>`** 토큰
- 출력 : 그 문서에 대한 요약문
- `<summarize>` = "요약해줘"라는 신호

### BERT
**Encoder만 사용**
- 지피티니핑처럼 왼쪽 -> 오른쪽 이렇게 한 방향만 바라보는 것이 아님.
- 양방향을 바라봄.
- **보통 한 단어를 768차원 벡터로 표현**

**pretraining**
- Masked language model : 토큰의 15%를 masking처리, 그 자리에 들어갈 적절한 단어를 예측하며 훈련받음
	- (예) `Let’s stick to [MASK] in this skit → [MASK] = improvisation`
- Next Sentence Prediction : 두 문장을 보고, 두 번째로 온 문장이 첫 번째 문장의 다음 문장이 진짜 맞을지!! 를 맞추며 훈련받음


**FFNN + Softmax**
- FFNN : 완전 연결층 feed forward neural network
- 벡터 → 의미 있는 값(확률)
- `[MASK]`위치를 예측한 출력 벡터 `V`가 있다구 치며는
	- `V`는 문맥을 이해한 벡터이지만, 이게 그래서 먼 단어인지는 알 수 없음.
	- 이 때, FFNN을 통과 : `FFNN: 768차원 → vocab 크기 (예: 30,000개)`
		- 모든 단어들(vocab) 중에서 어떤 게 가장 어울리는지 점수 매김
	- FFNN을 통과한 V를 Softmax 함수에 넣어서 확률로 바꿈


**fine-tuning과 cls 토큰**
- `[CLS]`토큰 : 맨 앞 `[CLS]`의 출력 벡터는 전체 문장의 의미 요약 벡터가 됨.
	- `[CLS]` 벡터를 기반으로 결과를 내는 **출력층만 새로 추가**
	- 그 전체를 **새로운 태스크에 맞게 재학습** => **fine-tuning**
- 분류 작업에서의 fine-tuning![BERT1](https://ejkiwi.github.io/lib/media/BERT1.png)
- Sentence Pair Classification![BERT2](https://ejkiwi.github.io/lib/media/BERT2.png)
	- 핑크벡터들 : 텍스트 토큰 단위들
		- 입력 텍스트의 토큰화 결과 ( 아직 임베딩 안 됨 )
		- 아직 숫자로 바뀌기 전 순수한 단어/기호 상태
	- 노랑벡터들 : 입력 임베딩 벡터들
		- BERT 인코더에 들어가기 직전 입력 준비 완료!
		- Eᵢ : 단어에 대한 임베딩
			- Token Embedding + Segment Embedding + Position Embedding
		- `E[CLS]`: `[CLS]` 토큰에 대한 입력 임베딩
		- `E[SEP]`: 문장 구분자에 대한 임베딩 
			- Sentence 1 + `[SEP]` + Sentence 2  : `[SEP]`을 통해 두 문장을 하나로 연결
	- 초록벡터들 : 출력 벡터들
		- BERT인코더를 거친 후 나온 최종 출력 벡터들
		- `C` : BERT 출력 중 `[CLS]`에 해당하는 벡터 (문장쌍 전체 의미 요약)
		- `T₁`, `T₂`, ..., `T'ₘ`: 각 토큰의 출력 벡터 → 문맥을 고려한 의미 벡터
			- 노랑 벡터에서처럼 단어의 고립된 의미가 아니라, 주변 문맥을 보고 의미가 조정된 결과

	- 🔤 **핑크색** = "텍스트" 단어 
	- 🔢 **노란색** = "벡터"로 바뀐 단어 (Embedding Layer 출력) 
	- 🧠 **초록색** = 문맥 고려한 최종 의미 벡터 (Encoder 출력)
- Question Answering![BERT3](https://ejkiwi.github.io/lib/media/BERT3.png)
	- 입력 : `[CLS] 질문 토큰들 [SEP] 지문(답) 토큰들 [SEP]` 
	- Start/End Span : 입력이 들어왔을 때,  어디부터 어디까지가 답일까?를 고르는 것.
		- 각 토큰별로..
			- 시작 위치일 확률 계산 하나
			- 끝 위치일 확률 계산 하나
			- 두 개의 linear layer 사용
		- softmax를 통해 확률 분포 만들기 -> 가장 확률이 높은 시작/끝 고름

## 연합학습
*어디에서 학습할 것인가?*
사용자의 데이터를 중앙 서버로 모아서 학습하지 않고, **개별 디바이스에서 모델을 학습**하는 방식.
학습된 결과( 모델의 가중치나 업데이트 정보 )만 중앙 서버로 보내게 되는데, 이 때 중앙서버에서는 데이터를 취합 후 더 개선된 공통 모델을 생성하는 과정을 거침. 그리고 그 개선된 모델은 다시 사용자들에게 배포됨!!
- 관련 개념 ( 어디에서 학습할 것인가? )
	- 중앙 집중 학습 : 모든 데이터를 중앙 서버로 집합
	- 분산 학습 : 모델 학습을 여러 서버에서 수행 후 결과를 병합
	- 연합 전이 학습 : 연합학습 + 전이학습 -> 서로 다른 기관  간 데이터를 공유하지 않고 협력 학습...?
	- 스완슨 학습 : 블록체인 사용하여 중앙서버 없이 학습

## 표현학습 ( = 특징학습 = representation learning )
*어떻게 학습할 것인가?*
**특징**을 자동으로 추출할 수 있도록 학습하는 과정을 뜻함. ==SELF FEATURE EXTRACT==
데이터만 제공하면 그 데이터로부터 핵심 정보를 추출해낼 수 있고 기계 스스로 배워내는 학습 과정을 뜻함.
*지도학습이될수도있구 비지도학습이될수도있구 자기지도학습이될수도있어 뭐 다.. 섞이는거지 어떻게 학습할 것인지에 대한 개념들이니까!!!!*
- 관련 개념 ( 어떻게 학습할 것인가? )
	- 지도 학습 : 입력 데이터와 정답이 있는 상태 ( 사람이 직접 라벨을 떠먹여 줌. ) ==YES LABEL==
	- 비지도 학습 : 데이터의 라벨 없이 "패턴" 학습 ( 클러스터링 느낌 ) ==NO LABEL==
	- 자기지도 학습 : 데이터에서 일부 정보 숨기고 이를 예측하도록 학습 ( 데이터 자체에서 스스로 라벨을 "만들도록" 함 ) ==NO LABEL, GENERATE LABEL==
	- 전이 학습 : 미리 학습된 모델을 새로운 문제에 적용. ==RECYCLE==

##### 도메인? domain
*어떤 데이터를 다루는가?*
- domain : 특정 데이터 소스 또는 모델이 작동하는 환경
	- 이미지 vs 음성
	- domain shifts : 훈련은 A사용자의 이메일로 스팸 필터링을 했다가, 실제 사용은 B사용자의 이메일로 스팸 필터링을 하는 것.
- task : 모델이 해결하거나 달성해야하는 문제 또는 목표 -> 이 데이터를 가지고 무엇을 할 것인가?
**domain adaptation?**
diffusion model?
encoder-decoder -> 입출력 크기를 같게.

---

## batch

가중치를 한 번 업데이트 할 때 동시에 사용하는 독립적인 데이터 샘플(llm에서는 문장의 개수)

|**구분**|**전체 배치 (Full-Batch)**|**미니 배치 (Mini-Batch)**|**확률적 경사 하강 (SGD)**|
|---|---|---|---|
|**Batch Size (M)**|10 (전체 데이터셋)|4 (임의 지정)|1 (샘플 1개)|
|**입력 데이터**|문장 10개 전체|문장 4개씩 묶음|문장 1개씩|
|**입력 Shape**|(10, 5, 512)|(4, 5, 512)|(1, 5, 512)|
|**경사 계산**|10개 문장의 손실 평균|4개 문장의 손실 평균|1개 문장의 손실|
|**가중치 업데이트 (1 Epoch 당)**|**1회**|$10 \div 4 \approx$ **3회** (4개, 4개, 2개)|**10회**|
|**경사 특징**|가장 안정적이고 정확함|적당한 노이즈와 안정성을 가짐|노이즈가 가장 심함|
## 배치 정규화
```
배치 1 [ 값1, 값2, 값3 ]
배치 2 [ 값1, 값2, 값3 ]
배치 3 [ 값1, 값2, 값3 ]
		 ↓    ↓    ↓   각 열마다 정규화
		 
		 
# Batch Norm 없을 때:
배치1(나는): [0.1, 100.0, 0.2]
배치2(너는): [0.2, 120.0, 0.3]
배치3(내가): [0.15, 110.0, 0.25]
# 문제: 차원2(100.0, 120.0, 110.0)가 너무 큰 값 → 이 차원에 "치중"됨
→ 이 차원의 "비중(가중치)"이 너무 큼
→ 다른 차원들의 영향력이 무시됨

# Batch Norm 후:
배치1(나는): [-1.0, -1.0, -1.0]
배치2(너는): [ 1.0, 1.0, 1.0]
배치3(내가): [ 0.0, 0.0, 0.0]
# 해결: 모든 차원이 비슷한 범위
→ 어떤 차원에도 "치중"되지 않음
→ 모든 차원의 "비중(가중치)"이 균형있게 됨
→ 모든 차원이 공평하게 기여함
```
배치1의 값1, 배치2의 값1, 배치3의 값
-> 평균 0
-> 분산1로 
맞춰주는 것.

## + 레이어 정규화
```
배치 1 [ 값1, 값2, 값3 ]  ← 이 3개를 정규화
배치 2 [ 값1, 값2, 값3 ]  ← 이 3개를 정규화
배치 3 [ 값1, 값2, 값3 ]  ← 이 3개를 정규화
	   ←─────────────→ 각 행마다 정규화
```

---
## 깨달음의공간!

- Word Embedding은 단순 숫자가 아니라 ==의미 표현 공간==이라는 것!
- Language Model이 ==분류 문제== 구조로 작동한다는 것!
- 위치 인코딩 벡터는 단어 임베딩 벡터와 ==같은 차원==으로 만들어져 element-wise ==덧셈== 방식으로 추가된다는 것!
	- 임베딩 = "토큰을 숫자로 표현한 벡터"  (discrete → dense vector // 고차원 -> 저차원) 
	- 인코딩 = "어떤 정보(단어, 문장 등)를 벡터로 표현한 결과" (정보 구조 → 표현 벡터)
	- "위치"는 그 둘의 경계에 있는 예외 케이스라서 둘 다 쓰임... ( 위치 인코딩/임베딩 )
- 단어 임베딩 벡터는 단순히 고정된 변환이 아니라 ==의미를 학습하는 파라미터==라는 것!
- layer norm
	- batch norm은 텐서가 있다고 하면 세로로 정규화 하고
	- layer norm은 텐서가 있다고 하면 가로로 정규화 하는 느낌
	(n, m) 텐서가 있다면...!
	- Batch Norm: 각 m 특성마다 n 샘플의 평균/분산으로 정규화
	- Layer Norm: 각 n개의 샘플마다 m 특성의 평균/분산으로 정규화


---
LLM의 임베딩 공간 => LATENT SPACE









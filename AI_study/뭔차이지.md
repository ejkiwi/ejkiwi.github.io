# 공부하며 이게먼차이야?! 싶었던것들 정리

## 입력 데이터의 중간 상태들 
층과 층 사이를 흐르는 텐서/벡터
==보통 동그라미로 표현하는 그것임.==
==입력 데이터에서부터 입력 데이터의 변형된 상태...?들을 모두 칭하는 말..? 이라고해야하나...==

| 용어                    | 사용 맥락                 | 먼느낌?                           |
| --------------------- | --------------------- | ------------------------------ |
| **Hidden State**      | RNN, LSTM, GRU        | 시간에 따라 변화하는 내부 상태              |
| **Feature Map**       | CNN                   | 2D 구조를 유지한 특징 표현               |
| **Feature Vector**    | CNN (flatten 후)       | 1D로 변환된 특징                     |
| ==**Latent Vector**== | VAE, GAN, Autoencoder | 압축된 숨겨진 표현                     |
| **Embedding**         | NLP, Vision           | 의미 공간의 벡터 표현                   |
| **Activation**        | 일반적                   | 뉴런의 활성화 값 ( 보통 동그라미로 표현하는 그것 ) |
| **Representation**    | 학술 논문                 | 데이터의 내부 표현                     |


---

## 학습 가능한 파라미터
학습을 통해 업데이트되는 값들
진짜중요한녀석들! 핵심인거임. 심장인거임.

| 용어            | 의미                | 예시              |
| ------------- | ----------------- | --------------- |
| **Weight**    | 연결 강도             | `W @ x`         |
| **Parameter** | 모든 학습 가능한 값       | Weight + Bias   |
| **Kernel**    | CNN 필터            | 3x3 conv kernel |
| **Filter**    | CNN 필터 (= Kernel) | 같은 의미           |
| **Bias**      | 편향값               | `Wx + b`의 b     |

---

## 층/연산

입력을 변환하는 함수

|용어|프레임워크/맥락|의미|
|---|---|---|
|**Linear Layer**|PyTorch|`y = Wx + b`|
|**Dense Layer**|TensorFlow/Keras|`y = Wx + b`|
|**Fully Connected (FC)**|일반적/논문|`y = Wx + b`|
|**MLP**|Multi-Layer Perceptron|여러 Linear 층|
|**Feed Forward**|일반적|순방향 전파|

---

## 학습 과정 (The Training Process)

|용어|의미|맥락|
|---|---|---|
|**Forward Pass**|순전파|입력 → 출력|
|**Backward Pass**|역전파|그래디언트 계산|
|**Backpropagation**|역전파 알고리즘|체인룰로 그래디언트 전파|
|**Gradient Descent**|경사하강법|파라미터 업데이트 방법|
|**Loss Function**|손실 함수|예측과 정답의 차이|
|**Cost Function**|비용 함수|= Loss Function|
|**Objective Function**|목적 함수|= Loss Function|

---

## 데이터 표현 (The Data Formats)

| 용어         | 형태       | 설명                |
| ---------- | -------- | ----------------- |
| **Tensor** | N차원 배열   | 일반적인 데이터 구조       |
| **Matrix** | 2차원 배열   | 2D Tensor         |
| **Vector** | 1차원 배열   | 1D Tensor         |
| **Scalar** | 0차원      | 단일 값              |
| **Batch**  | 여러 샘플 묶음 | [batch_size, ...] |

--- 

## 날 헷갈리게해 막
자주헷갈렸던 부분들... 이젠 혼동하지말고 잘 이해해보자!

| 이녀석들...                | 찐 관계               |
| ---------------------- | ------------------ |
| Dense vs Linear        | 완전히 같음             |
| Feature vs Activation  | 거의 같음 (뉘앙스만 다름)    |
| Kernel vs Filter       | 완전히 같음             |
| Loss vs Cost           | 완전히 같음             |
| Hidden State vs Latent | 맥락만 다름             |
| Parameter vs Weight    | Weight ⊂ Parameter |

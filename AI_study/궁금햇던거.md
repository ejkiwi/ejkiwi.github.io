## 사영?

- 잠재 벡터의 차원 -> 잠재 공간에서 쓰는 축의 개수
- 2D latent space를 시각화했을 때, scatter plot의 한 점 한 점들이 2D latent vector에 해당한다.
- 사영은왜하는것이지?
	-> 우리 눈은 4차원부터는 그 공간을 시각화하여 표현하지 못한다.
	->  4, 5, 6 ,,,, 차원의 잠재 벡터로 표현되어있는 벡터 공간을 시각화해서 보고싶으면 어떡해~
	-> 그 때는 차원을 줄이면 되지! 고차원을 저차원(1,2,3차원)으로 내리 꽃는 그 무드임

---

## 탐색 공간
- 변수의 ==종류에== 따라 탐색 공간 정의 가능
- 변수의 ==개수에== 따라 탐색 공간 차원이 정해짐
- **이산 변수 (예: binary)**
    
    - 변수 하나 → 차원 1의 공간 예: {0, 1}
        
    - 변수 두 개 → 차원 2의 공간 예: {0, 1}² = {(0,0), (0,1), (1,0), (1,1)}
        
    - 변수 n개 → 차원 n의 이산 공간 예: {0, 1}ⁿ
        
- **연속 변수 (예: 실수)**
    
    - 변수 하나 → 차원 1의 공간 예: ℝ (실수직선)
        
    - 변수 두 개 → 차원 2의 공간 예: ℝ² (평면)
        
    - 변수 n개 → 차원 n의 연속 공간 예: ℝⁿ


---

## 단어 임베딩

- 단어를 숫자로 특징 표현.
- vector_size를 지정해 단어를 특정 차원의 의미 벡터로 표현.
- 예: `vector_size = 5`,  `"이"` → `[0.1, -0.3, 0.5, 0.0, 0.2]`

---

## 단어 예측(Language Modeling)의 구조

- n개의 단어 시퀀스를 입력으로 주고, 그 ==다음 단어를 예측하는 분류 문제==로 처리함.
  - 입력 예: `"이", "것", "은"`  ( 이 때 "이","것","은" 은 각각 5차원 벡터 -> 총` 3*5 `matrix 가 입력으로 들어감)
  - 출력 예: `"첫"`
- 출력은 **단어장 크기**만큼의 **확률 벡터** (예: softmax로 17차원 출력)
- 정답(GT)은 예측 단어가 단어장 내에서 위치하고 있는 인덱스 (예: `"첫"`은 7번 인덱스)
	- ==출력 시퀀스의 인덱스가 아님!!==
	- 정답 단어는 보통 정답 단어는 보통 **one-hot 벡터**로 표현되며, 실제 학습에서는 one-hot 벡터에서 1이 적혀있는 인덱스 번호만으로도 정답을 표현 함
- 예측 결과에 대한 loss 함수는 CrossEntropyLoss 사용 (softmax + log loss)

---

## 시작 위치 처리 (START 토큰)

- 문장 앞부분처럼 앞에 단어가 부족할 때는 `[START]` 같은 특수 토큰으로 채움
- 예: 시퀀스 길이 = 3이면  
  `[START], [START], "이"` → 예측: `"것"`

---

## 임베딩도 학습되는 파라미터!!

- 각 단어의 벡터는 모델이 학습 과정에서 계속 업데이트됨
- 즉, 임베딩은 단순히 고정된 변환이 아니라 의미를 학습하는 파라미터(weight matrix)임

---

## 벡터의 차원 수가 의미하는 바

- 차원이 클수록 더 미묘하고 복잡한 의미를 표현할 수 있지만,
- 연산량 증가, 메모리 부담, 학습 시간 증가 등의 trade-off가 있음
- 일반적으로:
  - Word2Vec: 100~300차원
  - BERT: 768차원
  - GPT-3: 최대 12,288차원까지도 사용

---

## 포지셔널 인코딩(Positional Encoding)

- Transformer는 단어 순서를 알 수 없기 때문에 위치 정보(Position)를 따로 넣어줘야 함
- 이 위치 정보는 단어 ==임베딩 벡터와 같은 차원==으로 만들어져 element-wise ==덧셈== 방식으로 추가함

예:  
"이"의 임베딩 = `[0.1, -0.3, 0.5, 0.0, 0.2]`  
위치 인코딩(Pos 0) = `[0.01, 0.02, 0.03, 0.04, 0.05]`  
→ 최종 입력 벡터 = `[0.11, -0.28, 0.53, 0.04, 0.25]`

- 벡터 차원 수는 그대로 유지되며, 단어 의미 + 위치 정보가 하나의 벡터에 담김

---

## 임베딩 벡터의 각 차원의 의미는 해석 불가

- 벡터 전체는 의미를 잘 반영하지만,
- 각 인덱스가 “성별”이나 “감정” 같은 특정 의미를 갖는 건 보장되지 않음
- 이 차원별 의미를 해석하려는 것이 현재 활발한 연구 주제 중 하나임  
  - 예: probing task, interpretable embeddings, disentangled representations 등

---
## GPT 와 BERT
### TRANSFORMER

|구성 요소|하는 일|
|---|---|
|**Encoder**|전체 입력을 한 번에 다 보고, 이해하는 역할|
|**Decoder**|입력 내용을 바탕으로, **출력을 한 단어씩 생성**하는 역할|

### GPT
**Decoder만 사용**
- Encoder는 전체 입력을 한꺼번에 다 보고 처리 -> gpt는 아니다~
- 앞 단어들만 보면서 오른쪽 단어를 예측하는! 순서대로 단어 예측하는 녀석

**gpt의 출력은 ==다음 단어를 예측하는 Vocab size만큼의 확률 분포**==
- (예: '것' 다음에 나올 단어가 '은'일 확률 등등)
- "나는 그것"까지 입력 -> 그러면 GPT는 "**그 다음 단어**가 뭐일지" 예측해야 함.
	- 이 때, GPT는 자기가 알고 있는 **모든 단어(=어휘집, Vocabulary)** 중에서~~ "것", "은", "이다", "합니다", "좋아해요" 같은 **모든 후보 단어에 대해 확률**을 계산해.
**출력을 Linear를 통해 원하는 차원으로 맞추어 줌**
- Transformer는 결국 숫자 벡터(예: 768차원)를 출력해줌 : 복잡한 의미를 담은 벡터를 뽑아줄 수는 있지만?
- 그러나? 우리가 원하는 건...
	- (예)**Classification**: 단일 문장 분류
	- (예)**Entailment**: 전제 → 가설 관계 추론
	- (예)**Similarity**: 두 문장의 유사도
	- (예)**Multiple Choice**: 지문과 보기들 중 정답 고르기
	- => 요런것들이기 때문에, 그 복잡한 벡터를 우리가 원하는 형태, 즉 정답 클래스 수로 가공해줘야한다!
		- => Linear Layer 선형층
특수토큰
- 기계 번역![GPT1](https://ejkiwi.github.io/lib/media/GPT1.png)
- Decoder만 쓰는 GPT가 어캐 번역을 하지.. 라고 생각할 수 있겟으나!! 입력 안에 번역해줘~ 라는 토큰이 들어온다면 가능할 수 잇다아~
- Auto-regressive generation
	- how are you `<to-fr>` : 입력 문장 (마지막 토큰`<to-fr>`이 작업 명령임)
	- Transformer-Decoder : 예측을 계산하는 모델 본체
	1. Time step #1
		- 지금까지의 입력을 보고, 첫 번째 단어의 위치인 Position 4 (즉, `<to-fr>` 바로 다음 위치)에 해당하는 단어를 예측.
		- "Comment"라는 단어 출력
	2. Time step #2
		- 현재 time step #1 에 해당하는 "Comment"까지 기억하고 있음.
		- 그 다음 "allez-vous" 단어 예측, 출력

- 요약![GPT2](https://ejkiwi.github.io/lib/media/GPT2.png)
- 입력 : 긴 문서 내용 (예: 위키피디아 문장들) + **`<summarize>`** 토큰
- 출력 : 그 문서에 대한 요약문
- `<summarize>` = "요약해줘"라는 신호

### BERT
**Encoder만 사용**
- 지피티니핑처럼 왼쪽 -> 오른쪽 이렇게 한 방향만 바라보는 것이 아님.
- 양방향을 바라봄.
- **보통 한 단어를 768차원 벡터로 표현**

**pretraining**
- Masked language model : 토큰의 15%를 masking처리, 그 자리에 들어갈 적절한 단어를 예측하며 훈련받음
	- (예) `Let’s stick to [MASK] in this skit → [MASK] = improvisation`
- Next Sentence Prediction : 두 문장을 보고, 두 번째로 온 문장이 첫 번째 문장의 다음 문장이 진짜 맞을지!! 를 맞추며 훈련받음


**FFNN + Softmax**
- FFNN : 완전 연결층 feed forward neural network
- 벡터 → 의미 있는 값(확률)
- `[MASK]`위치를 예측한 출력 벡터 `V`가 있다구 치며는
	- `V`는 문맥을 이해한 벡터이지만, 이게 그래서 먼 단어인지는 알 수 없음.
	- 이 때, FFNN을 통과 : `FFNN: 768차원 → vocab 크기 (예: 30,000개)`
		- 모든 단어들(vocab) 중에서 어떤 게 가장 어울리는지 점수 매김
	- FFNN을 통과한 V를 Softmax 함수에 넣어서 확률로 바꿈


**fine-tuning과 cls 토큰**
- `[CLS]`토큰 : 맨 앞 `[CLS]`의 출력 벡터는 전체 문장의 의미 요약 벡터가 됨.
	- `[CLS]` 벡터를 기반으로 결과를 내는 **출력층만 새로 추가**
	- 그 전체를 **새로운 태스크에 맞게 재학습** => **fine-tuning**
- 분류 작업에서의 fine-tuning![BERT1](https://ejkiwi.github.io/lib/media/BERT1.png)
- Sentence Pair Classification![BERT2](https://ejkiwi.github.io/lib/media/BERT2.png)
	- 핑크벡터들 : 텍스트 토큰 단위들
		- 입력 텍스트의 토큰화 결과 ( 아직 임베딩 안 됨 )
		- 아직 숫자로 바뀌기 전 순수한 단어/기호 상태
	- 노랑벡터들 : 입력 임베딩 벡터들
		- BERT 인코더에 들어가기 직전 입력 준비 완료!
		- Eᵢ : 단어에 대한 임베딩
			- Token Embedding + Segment Embedding + Position Embedding
		- `E[CLS]`: `[CLS]` 토큰에 대한 입력 임베딩
		- `E[SEP]`: 문장 구분자에 대한 임베딩 
			- Sentence 1 + `[SEP]` + Sentence 2  : `[SEP]`을 통해 두 문장을 하나로 연결
	- 초록벡터들 : 출력 벡터들
		- BERT인코더를 거친 후 나온 최종 출력 벡터들
		- `C` : BERT 출력 중 `[CLS]`에 해당하는 벡터 (문장쌍 전체 의미 요약)
		- `T₁`, `T₂`, ..., `T'ₘ`: 각 토큰의 출력 벡터 → 문맥을 고려한 의미 벡터
			- 노랑 벡터에서처럼 단어의 고립된 의미가 아니라, 주변 문맥을 보고 의미가 조정된 결과

	- 🔤 **핑크색** = "텍스트" 단어 
	- 🔢 **노란색** = "벡터"로 바뀐 단어 (Embedding Layer 출력) 
	- 🧠 **초록색** = 문맥 고려한 최종 의미 벡터 (Encoder 출력)
- Question Answering![BERT3](https://ejkiwi.github.io/lib/media/BERT3.png)
	- 입력 : `[CLS] 질문 토큰들 [SEP] 지문(답) 토큰들 [SEP]` 
	- Start/End Span : 입력이 들어왔을 때,  어디부터 어디까지가 답일까?를 고르는 것.
		- 각 토큰별로..
			- 시작 위치일 확률 계산 하나
			- 끝 위치일 확률 계산 하나
			- 두 개의 linear layer 사용
		- softmax를 통해 확률 분포 만들기 -> 가장 확률이 높은 시작/끝 고름
- 

---
## 깨달음의공간이다

- Word Embedding은 단순 숫자가 아니라 ==의미 표현 공간==이라는 것!
- Language Model이 ==분류 문제== 구조로 작동한다는 것!
- 위치 인코딩 벡터는 단어 임베딩 벡터와 ==같은 차원==으로 만들어져 element-wise ==덧셈== 방식으로 추가된다는 것!
	- 임베딩 = "토큰을 숫자로 표현한 벡터"  (discrete → dense vector // 고차원 -> 저차원) 
	- 인코딩 = "어떤 정보(단어, 문장 등)를 벡터로 표현한 결과" (정보 구조 → 표현 벡터)
	- "위치"는 그 둘의 경계에 있는 예외 케이스라서 둘 다 쓰임... ( 위치 인코딩/임베딩 )
- 단어 임베딩 벡터는 단순히 고정된 변환이 아니라 ==의미를 학습하는 파라미터==라는 것!